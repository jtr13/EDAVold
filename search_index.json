[
["index.html", "edav.info/ Welcome 0.1 Everything you need for EDAV 0.2 Contact 0.3 License 0.4 Colophon", " edav.info/ Zach Bogart, Joyce Robbins 2019-01-18 Welcome 0.1 Everything you need for EDAV This resource has everything you need and more to be successful with R, this EDAV course, and beyond. Let’s get started! With this resource, we try to give you a curated collection of tools and references that will make it easier to learn how to work with data in R. In addition, we include sections on basic chart types/tools so you can learn by doing. There are also several walkthroughs where we work with data and discuss problems as well as some tips/tricks that will help you. We hope this resource serves you well! This resource is specifically tailored to the GR5702 Exploratory Data Analysis and Visualization course offered at Columbia University. However, anyone interested in working with data in R will benefit from perusing these pages. Happy coding! 0.2 Contact Zach Bogart: Website / Twitter / GitHub Joyce Robbins: Columbia Profile / Website / Twitter / GitHub 0.3 License This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License. 0.4 Colophon The EDAV Logo, the url/404 banners, and associated chapter icon designs are designed by Zach Bogart and published with permission. The url and 404 banners have been adapted into a typeface called Koji. Selected chapter icons can be accessed/purchased at The Noun Project. Please attribute the creator if using them for external purposes (see their icon attribution guidelines for more information). "],
["intro.html", "1 Introduction 1.1 Overview 1.2 How this resource is structured 1.3 Help improve edav.info/ 1.4 Fun stuff 1.5 Acknowledgments", " 1 Introduction 1.1 Overview This chapter introduces how this resource is organized, explains how you can add to this resource, and includes some general acknowledgments. 1.2 How this resource is structured This resource is split into four color-coded sections, each of which provides different kinds of assistance. Below is an explanation of each section: 1.2.1 Section I: Information (Blue) Pages in the blue section contain basic information. Examples of blue pages include this introduction page and the basics page, which explains how to setup R/RStudio as well as ways to get help if you need it. Blue pages are the help desk of this resource: look to them if you are lost and need to find your way. 1.2.2 Section II: Walkthroughs (Red) Pages in the red section contain more extensive walkthroughs. An example of a red page is the iris walkthrough, where a well-known dataset is presented as a pretty scatterplot and steps are shown from start to finish. This page type is the most thorough: it tries to provide full documentation, explanations of design choices, and advice on best practices. It’s like going to office hours and having a great clarifying chat with a course assistant…in article form. If you would like to see a fully-worked-through example of something with a lot of guidance along the way, check out the red pages. 1.2.3 Section III: Documentation (Green) Pages in the green section contain more compact documentation. An example of a green page is the histogram page, which includes simple examples of how to create histograms, when to use them, and things to be aware of/watch out for. The green pages hold your hand much less than the red pages: they explain how to use a chart/tool using examples and simple terms. If you have an idea in mind and are just wondering how to execute it, the green pages will help fill in those gaps. 1.2.4 Section IV: References (Yellow) Pages in the yellow section contain simple collections of references. An example of a yellow page is the external resources page, which is a list of materials that you can look through and learn from. Yellow pages have the least amount of hand-holding: they are collections of resources and bare-boned tutorials that will help you learn about new things. 1.3 Help improve edav.info/ This resource is an ongoing creation made by students, for students. We welcome you to help make it better. Not finding what you are looking for? Think a section could be made clearer? Consider helping improve edav.info/ by submitting a pull request to the github page. Don’t understand that last sentence? We have a page on how you can contribute to edav.info/. 1.4 Fun stuff 1.4.1 T-shirts Zach Bogart has made a few t-shirts available on Teespring so you can show your love for EDAV and R. Hope you enjoy! 1.5 Acknowledgments 1.5.1 Our Contributors Thank you so much to everyone who has contributed. You make edav.info/ possible. Aashna Kanuga (@aashnakanuga), @Akanksha1Raj, Akhil Punia (@AkhilPunia), Akshata Patel (@akshatapatel), Angela Li (@angela-li), @anipin, @AshwinJay101, Eric Boxer (@Ecboxer), @excited-student, @hao871563506, Harin Sanghirun (@harin), @jw2531, @kiransaini, @leahparreztnik, Louis Massera (@louismassera), @naotominakawa, Neha Saraf (@nehasaraf1994), Oleh Dubno (@odubno), Ramy Jaber (@ramyij), Rod Bogart (@rodbogart), @Somendratripathi, Tim Kartawijaya (@TimKartawijaya), @ujjwal95, Zhida Zhang (@ZhangZhida) "],
["basics.html", "2 R Basics 2.1 Essentials checklist 2.2 Getting started 2.3 Packages and imports 2.4 Communicating Results 2.5 Getting help", " 2 R Basics So…there is soooo much to the world of R. Textbooks, cheatsheets, exercises, and other buzzwords full of resources you could go through. There are over 13600 packages on CRAN, the network through which R code and packages are distributed. It can be overwhelming. However, bear in mind that R is being used for a lot of different things, not all of which are relevant to EDAV. To help you navigate the landscape, here we provide a collection of resources that you should be familiar with in the context of this course. This is not to say that any of these resources are prerequisites, but they will come up in the course and we want to give you places to learn about them. Since people come with a variety of backgrounds, we will try to provide the essentials as well as some resources for more advanced users. Do not feel you have to go through all of these resources, but know that they are here if/when you need them. 2.1 Essentials checklist In an effort to get everyone on the same page, here is a checklist of essentials so you can get up and running with this course. It will echo/reference a lot of info said below, but we want to make sure everything mentioned is clear and understood. Okay, then. Here are the essentials, in checklist form: 2.1.1 Learn R Download R and RStudio: This is the biggest thing to do by far. Make sure to download both R and RStudio, as mentioned in Setting up R and RStudio. Learn your way around RStudio: RStudio is powerful…if you know how to use it. Take the time to look through the DataCamp sections on the RStudio IDE so you feel comfortable (see Use RStudio like a pro section). Try something!: Getting comfortable with an IDE is all about practice. So while the DataCamp vids are great, don’t solely rely on them. Try things out for yourself! Here are some things to play around with: Create an R Script file, paste in print(&quot;Hello, World!&quot;), and run it Create an R Markdown file and have it generate an HTML page Download some packages like tidyverse or MASS Do some math in the console Study R: See Learning about R below. Learn how to get help: Make sure you are comfortable searching for answers when you get stuck. See the section below on getting help for some…help. 2.1.2 Prepare for class Get the Textbook: This course uses Graphical Data Analysis with R as its textbook. Here is an Amazon link for a physical copy and a link to the book’s website. Setup DataCamp Account: A lot of the references and support materials discussed in edav.info/ are from DataCamp, an online collection of courses/articles on data science. Some of the sections are free, but most are behind a paywall. However, DataCamp currently provides full access to the site for students with .edu email addresses. If you are enrolled in this course, during the first week of class you will receive an invitation to create an account using your columbia.edu email address, which will grant you full access. 2.2 Getting started 2.2.1 Setting up R and RStudio It is super important to get up and running with R and RStudio as soon as you can. This video from DataCamp pretty much covers it. Know that you will be downloading two separate things: R, which is a programming language; and RStudio, which is an IDE (integrated development environment…fancy tool for working with R) that will make working with R a lot more enjoyable. 2.2.2 Use RStudio like a pro Great! RStudio is up and running on your computer! Now make sure you get comfy with what it can do. Don’t know your way around the RStudio IDE? I highly recommend this DataCamp course. Sections from Part 1 (Orientation, Programming, and Projects) are the most relevant for this course. They include videos about all the regions in RStudio, how to program efficiently/effectively in the IDE (gotta love those keyboard shortcuts), and the benefits of setting up R projects. A little hazy on that last sentence? The course will help. Just want a quick reference to brush up with? Take a look at the RStudio Cheatsheets page. Another option is this RStudio webinar. Want to make the RStudio IDE your own? Look into modifying the preferences. You can customize the look of the IDE like default colors and typefaces, tweak default behaviors like clearing the environment on load, and integrate a session with a git repository. If something about the IDE bugs you, chances are you can make it more to your liking. 2.2.3 Learning about R R is just like any language, programming or otherwise: you need to use it to get used to it. Just starting out in R? Check out this free DataCamp course for a quick introduction. For this course, you can skim/mostly ignore matrices and lists (Parts 3 &amp; 6). Next cover the material in the first section of R for Data Science It is based on the tidyverse, a group of packages which, among other things, make it easier to code in R: Data Visualization Workflow: basics Data transformation Workflow: scripts Exploratory Data Analysis Workflow: projects General advice: don’t get caught up in the details. Keep a list of questions and move on. 2.3 Packages and imports 2.3.1 Installing packages A lot of the cool stuff comes from installing packages into R. How do you install packages? The main function we use is install.packages(&quot;&lt;package_name&gt;&quot;), which installs from CRAN, a well-known place where packages are stored. Then, once installed, you can use packages by calling them within library(). Still confused? This DataCamp video should help explain the process. Also be sure to try the accompanying exercise to make sure you have a feel for loading a package. Want more info? Check out this DataCamp article on everything about installing packages in R. As well as covering the basics, this article shows you how to install packages that are not located on CRAN using devtools, as well as ways to monitor the status/health of your installed packages. 2.3.2 Tidyverse Don’t know what the tidyverse is? It’s great and we use it throughout this course. Specifically, ggplot2 and dplyr, two packages within the Tidyverse. What’s ggplot? Check out this DataCamp course. This course is split up into three parts and it is quite long, but it does go over pretty much everything ggplot has to offer. If you are starting out, stick with Part 1. What’s dplyr? Make friends with this DataCamp course. It goes through the main dplyr verbs: select, mutate, filter, arrange, summarise; as well as the lovely pipe operator. Want case studies to go through? Try this one or this one. 2.3.3 Importing data We often will need to pull data into RStudio to work with it. “Pull data”? I’m already confused. But wait! Here’s a DataCamp course on importing data using dplyr. Note: This course explains how to import every kind of data format under the sun…all you need to be familiar with for this course (mostly) is pulling in CSV files using read_csv. So, if you are overwhelmed, just stick to the read_csv stuff. Importing every data format under the sun you say? I want to know how to do that. Here’s Part 1, as well as Part 2, which focuses on databases and HTTP requests. Go nuts. 2.4 Communicating Results 2.4.1 R Markdown &amp; Knitr R Markdown is how you will be writing assignments for this course and Knitr is how you will generate an output file for submission. In general, they’re a great way to communicate your findings to others (for the python-lovers among you, this is the Jupyter Notebook of the R world). Want to jump right in? Open a new R Markdown file (File &gt; New File &gt; R Markdown…), and set its Default Output Format to HTML. You will get a R Markdown template you can tinker with. Try knitting the document to see what everything does. For more info on what is happening behind the scenes, checkout this R Markdown Quick Tour. Want a simple description of R Markdown? Checkout this RStudio article for a description on how to combine text, source code, and output into one document. Prefer videos? DataCamp course to the rescue! There is also an RStudio webinar about it. Don’t know about Knitr? Here’s the specific section on Knitr from the DataCamp course cited above. With this package, you can embed code directly into your R Markdown files and generate output documents. Make sure to go through the later exercises to learn about code chunks and chunk options so you can fine-tune your final output document with ease. Wondering what chunk options are? Have you ever wanted to align graphs in your output PDF differently? Or re-size a plot in your output document? Or suppress an annoying message a package raises? Chunk options address this. We have made an R Markdown file showing off different chunk options that you can download from our github repo and play around with. Also make sure to checkout the documentation on chunk and package options for a full list of what’s possible. The R Markdown page from RStudio has lessons with extensive info. Also, more cheatsheets. 2.4.2 Submitting Assignments Here’s a quick run-down of how to submit your assignments using R Markdown and Knitr. Create R Markdown file with PDF output format: We will often provide you with a template, and feel free to add on to it directly, but make sure its output format is set to pdf_document. Write out your explanations and insert code chunks to answer the questions provided. If you want to make a new file, go to File &gt; New File &gt; R Markdown… and set the Default Output Format to PDF. Either way, the header of the .Rmd file should look something like this: Add PDF Dependencies: As stated when you create a new R Markdown file, the PDF output format requires TeX: Make sure you download TeX for your machine. Here are some Medium articles on the process of creating PDF reports (the articles cover starting from scratch with no installs at all, but you can skip over to installing TeX only): Mac OS Windows This can be a little complicated, but it will make that Knit button near the top of the IDE magically generate a PDF for you. If you are in a rush and want a shortcut, you can instead set the Default Output Format to HTML. When you open the file in your browser, you can save it as a PDF. It will not be as nicely formatted, but it will still work. 2.5 Getting help via https://dev.to/rly First off…breeeeeeathe. We can fix this. There are a bunch of resources out there that can help you. 2.5.1 Things to try Remember: Always try to help yourself! This article has a great list of tools to help you learn about anything you may be confused by. This includes learning about functions and packages as well as searching for info about a function/package/problem/etc. This is the perfect place to learn how to get the info you need. The RStudio Help menu (in the top toolbar) is a fantastic place to go for understanding/fixing any problems. There are links to documentation and manuals as well as cheatsheets and a lovely collection of keyboard shortcuts. Vignettes are a great way to learn about packages and how they work. Vignettes are like stylized manuals that can do a better job at explaining a package’s contents. For example, ggplot2 has a vignette on aesthetics called ggplot2-specs that talks about different ways you can map data to different formats. Typing browseVignettes() in the console will show you all the vignettes for all of the packages you have installed. You can also see vignettes by package by typing vignette(package = &quot;&lt;package_name&gt;&quot;) into the console. To run a specific vignette, use vignette(&quot;&lt;vignette_name&gt;&quot;). If the vignette can’t be resolved, include the package name as well: vignette(&quot;&lt;vignette_name&quot;, package = &quot;&lt;package_name&gt;&quot;) Don’t ignore errors. They are telling you so much! If you give up because red text showed up in your console, take the time to see what that red text is saying. Learn how to read errors and what they are telling you. They usually include where the problem happened and what R thinks the problem stems from. More Advanced: Learn to love debugger mode. Debugging can have a steep learning curve, but huge payoffs. Take a look at these videos about debugging with R. Topics include running the debugger, setting breakpoints, customizing preferences, and more. Note: R Markdown files have some limitations for debugging, as discussed in this article. You could also consider working out your code in a .R file before including it in your R Markdown homework submission. 2.5.2 Help me, R community! Relax. There are a bunch of people using the same tools you are. Your fellow classmates are a good place to start! Post questions to Piazza to see how they could help. There is a lot of great documentation on R and its functions/packages/etc. Get comfy with R Documentation and it will help you immensely. There is a vibrant RStudio Community page. Also, R likes twitter. Check out #rstats or maybe let Hadley Wickham know about a wonky error message. "],
["project.html", "3 Final Project Assignment 3.1 Overview 3.2 General info 3.3 Outline 3.4 FAQ 3.5 Executive summary notes 3.6 Resources", " 3 Final Project Assignment 3.1 Overview This section goes over what’s expected for the final project. General Note: Please note that this sheet cannot possibly cover all the “do’s and don’ts” of data analysis and visualization. You are expected to follow all of the best practices discussed in class throughout the semester. 3.2 General info 3.2.1 Goal The goal of this project is to perform an exploratory data analysis / create visualizations with data of your choosing in order to gain preliminary insights on questions of interest to you. 3.2.2 Teams You must work in teams of 2-4 people. (If you have specific interests you should try to find partners on Piazza first as we will not be able to match on specific criteria – we will simply assign groups in the order in which responses come in.) 3.2.3 Signup Please create a group by clicking on the People tab in CourseWorks, and then Student Groups. Next add the members of your group by dragging the names into the group. Please use the following naming convention for your group name: Columbia UNIs in alphabetical order, with underscores separating names, for example: jtr13_rar3010_sw2934. Remember that final project groups may have between 2 and 4 members. Once you do so, we will create a copy of your group as a Final Project Group, which appears as a separate tab in the People section of CourseWorks. (If you’re wondering, this is necessary since Student Groups cannot be used for graded assignments due to the way CourseWorks is designed.) If you don’t sign up by the November 1 deadline, you will be assigned to a group randomly on November 2. That is not a bad option, and more realistic in terms of preparing yourself for a work environment. Once the groups are set up, we will ask for a short description of your project ideas, so start planning! 3.2.4 Topics The topic you choose is open-ended… choose something that you are intereted in and genuinely curious about! Think of some questions that you don’t know the answer to. Next look for data that might help you answer those questions. 3.2.5 Data The data can be pulled from multiple sources; it does not need to be a single dataset. Be sure to get data from the original source. For example, if you wish to work with data collected and distributed by the Centers for Disease Control, that is where you should go to access the data, not a third party that has posted the data. Avoid overused datasets (think Titanic) as well as those used in Kaggle (or similar) competitions. A few examples are: NYC Open Data US Bureau of Labor Statistics 3.2.6 Code All of your code should be stored on GitHub. In your report, include a link to the repo, as well as links to specific files as relevant. The static visualizations should be done in R, but other pieces, such as data importation and cleaning do not. 3.2.7 Analysis You have a lot of freedom to choose what to do, as long as you restrict yourselves to exploratory techniques (rather than modeling / prediction approaches). In addition, your analysis must be clearly documented and reproducible. 3.2.8 Feedback At any point, you may ask the TAs (Bridget and Zach) or the instructor (Joyce) for advice. Our primary role in this regard will be to provide general guidance on your choice of data / topic / direction. As always, you are encouraged to post specific questions to Piazza, particularly coding questions and issues. You may also volunteer to discuss your project with the class in order to get feedback–if you’d like to do this, email the instructor to schedule a date. 3.2.9 Peer review After final projects are turned in, you will be asked write peer reviews of other projects. Each individual will be assigned two project groups to review, and instructions will be provided. Note: part of the grade you receive for the class is based on the quality of review that you write, not on the feedback that your project receives. Your grade for the project (as for all other assignments for the class) will be determined solely by the instructor and TAs. 3.2.10 Report format With the exception of the interactive part, your project should be submitted to CourseWorks as an .Rmd and .html or pdf file, with graphs / output rendered. If it’s not feasible to include certain portions of your material in the report, make those parts available online (for example, GitHub), and provide links to them in the report. You will lose points if we have trouble reading your file, need to ask you to resubmit with graphs visible, if links are broken, or if we have other difficulties accessing your materials. It’s ok if code is in different files and different places, just make sure there are working links in your report to these locations. Note: Using Markdown + code chunks is supposed to make combining code, text and graphs easier. If it is making it more difficult, you are probably trying to do something that isn’t well suited to the tool set. Focus on the text and graphs, not the formatting. If you’re not sure if something is important to focus on or not, please ask. Advice: don’t wait to start writing. Your overall project will undoubtedly be better if you give up trying to get that last graph perfect or the last bit of analysis done and get to the writing! 3.2.11 A note on style You are encouraged to be as intellectually honest as possible. That means pointing out flaws in your work, detailing obstacles, disagreements, decision points, etc. – the kinds of “behind-the-scene” things that are important but often left out of reports. You may use the first person (“I”/“We”) or specific team members’ names, as relevant. 3.3 Outline Your report should include the following sections, with subtitles (“Introduction”, etc.) as indicated: 3.3.1 Introduction Explain why you chose this topic, and the questions you are interested in studying. List team members and a description of how each contributed to the project. 3.3.2 Description of data Describe how the data was collected, how you accessed it, and any other noteworthy features. 3.3.3 Analysis of data quality Provide a detailed, well-organized description of data quality, including textual description, graphs, and code. 3.3.4 Main analysis (Exploratory Data Analysis) Provide a detailed, well-organized description of your findings, including textual description, graphs, and code. Your focus should be on both the results and the process. Include, as reasonable and relevant, approaches that didn’t work, challenges, the data cleaning process, etc. 3.3.5 Executive summary (Presentation-style) Provide a short nontechnical summary of the most revealing findings of your analysis written for a nontechnical audience. The length should be approximately two pages (if we were using pages…) Take extra care to clean up your graphs, ensuring that best practices for presentation are followed. Note: “Presentation” here refers to the style of graph, that is, graphs that are cleaned up for presentation, as opposed to the rough ones we often use for exploratory data analysis. You do not have to present your work to the class! However, you may choose to present your work as your community contribution, in which case you need to email me to set a date before the community contribution due date. (The presentation itself may be later.) 3.3.6 Interactive component Select one (or more) of your key findings to present in an interactive format. Be selective in the choices that you present to the user; the idea is that in 5-10 minutes, users should have a good sense of the question(s) that you are interested in and the trends you’ve identified in the data. In other words, they should understand the value of the analysis, be it business value, scientific value, general knowledge, etc. Interactive graphs must follow all of the best practices as with static graphs in terms of perception, labeling, accuracy, etc. You may choose the tool (D3, Shiny, or other) The complexity of your tool will be taken into account: we expect more complexity from a higher-level tool like Shiny than a lower-level tool like D3, which requires you to build a lot from scratch. Make sure that the user is clear on what the tool does and how to use it. Publish your graph somewhere on the web and provide a link in your report in the interactive section. The obvious choices are blockbuilder.org to create a block for D3, and shinyapps.io for Shiny apps but other options are fine. You are encouraged to share experiences on Piazza to help classmates with the publishing process. As applicable, all of the following will be considered in the grading process: Choice of data and plot types to present Clear relevance to question(s), project in general Design of interactive component(s) Clarity of presentation, including instructions Technical execution (include a description of what you would work on in the future, what you’ve attempted, etc. so we know it’s on your radar) 3.3.7 Conclusion Discuss limitations and future directions, lessons learned. 3.4 FAQ How long should the project be? It should take the reader approximately 15-20 minutes to read the report. We cannot provide a specific number of graphs or pages since there are so many variables. Use your judgment to cover all of the important material without being repetitive. You can report on what you’ve done without including all of the graphs; for example, if you looked at maps of each of the fifty states you can include 1 or 2 as examples. Do we have to present the project to the class? No. Presenting your project as your community contribution is optional. Someone has already used the same data, is that ok? Yes. As long as you get the data from the original source, not a site like Kaggle, you’re fine. You can check with the professor if you want to be sure. I spent 30 minutes looking at my data, and then 1000 hours building this super cool interactive app so users can analyze the data themselves. Can’t you count the interactive part for 95% of my grade? No. While skill sets overlap in the real world, and it’s important to know something about building things, the assumption is that you are doing the work of the data scientist: actually analyzing the data rather than building tools for someone else to do it. The former (the data!) has been the main focus of this class and therefore is the primary focus of the final project. 3.5 Executive summary notes The executive summary should be a well-formatted, presentable final product of your results. Here are some notes to consider when putting it together: Title, axis labels, tick mark labels, and legends should be comprehensible (easy to understand) and legible (easy to read / decipher). Tick marks should not be labeled in scientific notation or with long strings of zeros, such as 3000000000. Instead, convert to smaller numbers and change the units: 3000000000 becomes “3” and the axis label “billions of views”. Units should be intuitive (An axis labeled in month/day/year format is intuitive; one labeled in seconds since January 1, 1970 is not.) The font size should be large enough to read clearly. The default in ggplot2 is generally too small. You can easily change it by passing the base font size to the theme, such as + theme_grey(16) (The default base font size is 11). The order of items on the axes and legends should be logical. (Alphabetical is usually not the best option.) Colors should be color-vision-deficiency-friendly. If categorical variable levels are long, set up the graph so the categorical variable is on the y-axis and the names are horizontal. A better option, if possible, is to shorten the names of the levels. Not all EDA graphs lend themselves to presentation, either because the graph form is hard to understand without practice or it’s not well labeled. The labeling problem can be solved by adding text in an image editor. The downside is that it is not reproducible. If you want to go this route, for the Mac, Keynote and Paintbrush are good, free options. Err on the side of simplicity. Don’t, for example, overuse color when it’s not necessary. Ask yourself: does color make this graph any clearer? If it doesn’t, leave it out. Test your graphs on nontechnical friends and family and ask for feedback. Above all, have fun with it 3.6 Resources “Tidy Tuesday Screencast: analyzing college major &amp; income data in R” David Robinson explores a dataset in R live, without looking at the data in advance. "],
["contribute.html", "4 Contribute to this Resource 4.1 Overview 4.2 Why contribute? 4.3 Ways you can contribute 4.4 Resources", " 4 Contribute to this Resource 4.1 Overview This page explains how to contribute to edav.info/. 4.2 Why contribute? We don’t want edav.info/ to be just another resource. Rather, we want it to be your resource. If there are things that trip you up or cause you frustration, chances are you’re not alone. Everyone comes to this course with different backgrounds and expertise. Being able to collect all that knowledge in one place is this resource’s mission and you can help move that mission forward. 4.3 Ways you can contribute There are three main ways you can contribute: For simple changes contribute directly (we got a full walkthrough on how to do this) For bigger/more abstract suggestions submit an issue (very simple, much appreciated) For adventurous/social GitHub users solve an open issue (more advanced/open-ended, also much appreciated) Below you’ll find more detail on each option. Happy coding! 4.3.1 Contribute directly One way to contribute to edav.info/ is to contribute directly by editing a chapter. At the top of every page of this resource, you will see an icon that looks like this: . Clicking it will open a new tab where you can edit the markdown for that page on our GitHub repo and submit your change as a pull request. Essentially, you will create a copy of our repo, make your desired changes, and suggest to us that we include them. If we approve of your changes, they will be rendered and published to the site. Contributing directly works best if the change you are proposing is something relatively small, such as: A typo/grammatical error An unclear phrasing/explanation A quick code fix 4.3.1.1 Direct contribution walkthrough This is a full walkthough on proposing a change to edav.info/. It follows a hypothetical student that spots a typo and uses a pull request to fix it. It’s a little long, but don’t get scared; it’s a great way to learn about GitHub and it’s almost entirely hitting big green buttons! Let’s find something to change. I’m pretty sure they meant to write “repository” here. Oops. Let’s fix it for them! That’s not how you spell “repository”! Let’s fix it. To make the fix, we click on the edit icon, , at the top of the page. This will take us to their GitHub repo, where all the code for this resource is stored. Note: You need to have a valid GitHub account to contribute. In this example, we are using a dummy account called excited-student so if you see it in a screenshot, know that it would be replaced by your own username. Hit this icon to go to GitHub. We haven’t forked the repo yet, so GitHub shows us a page like the one below. No worries! We just hit the big green button labeled Fork this repository and propose changes and we’ll be good to go (as you will see, big green buttons are our friends). For more info on forking repos, the GitHub Guide on Forking Projects is very informative. Note: you will not have to fork the repo every time. If you propose another change in the future, the edit icon, , will jump you directly to this point of the walkthrough. Just remember to keep your fork up to date. Haven’t forked the repo before? No worries; the big green button will solve everything. Now that we have successfully forked the repo, we can see the code for the page we want to edit. Note: That little blue blurb at the top is spelling out what is happening/going to happen: we have made a copy of a repo because we don’t have write access to it . So, after we make our change on this page, we will inform the owners of the repo about our edits by using a pull request. GitHub can be super overwhelming, but it will try its darndest to inform you what will happen along the way. Ready to edit the code. The blue blurb is worth reading. Let’s fix that embarrassing typo! We update the code right in this editor, include an explanation for what we changed/why we made the change, and then hit the big green button labeled Propose file change. Gotta love those big green buttons! Make your edits, include a quick explanation, and hit the big green button. Now GitHub is once again helping out by letting us review the changes we made. On this page we can review our proposed changes by scrolling down and looking at the diffs. Our fix is very simple so there isn’t much to see. Once again, we are going to push the big green button, this time labeled Create pull request. This will start the process of letting the edav.info/ people know that we would like them to include our changes (in git-speak, we are requesting that the edav.info/ people do a git merge to update their files with our proposed changes.) Chance to review your changes. Once satisfied, hit the big green button to start a pull request. Here we are at the pull request page. Notice the green checkmark that says “Able to merge” (a good sign that everything is going smoothly). Now we explain our pull request with some comments and, once again, hit the big green button labeled Create pull request. Note: You may be asking, “Why do I have to type this explanation in again?”. This is because the explanation we wrote in Step 5 (where we edited the file) is a commit. We could have had multiple commits at once that we wanted to bundle into one pull request. This step is a way to explain the pull request as a whole. It is redundant for us because our change is so small and only has one commit. Still totally lost? This GitHub Guide on Understanding the GitHub Flow is an incredibly helpful read and our GitHub Resources page also has a lot of helpful links. Explain your pull request and hit the big green button. Congratulations are in order! We have successfully opened a pull request on a GitHub repo! Now one of the repo owners (like the guy writing this tutorial, for example ) has to decide if they want to include your pull request or not. In this case they’ll certainly approve it, but know that they may decide against adding your changes. For more info, read the section of the Open Source Guides on what happens after you submit a contribution. Note: Be aware that the icon shown below may initially be yellow to signal that some tests are being performed to check the conflicts of your proposal with the original repo. It should turn green if everything passes. We did it! Now the maintainers will review our changes and get back to us… And now we wait… via GIPHY What’s this!? We have received an email from one of the repo owners, Zach Bogart. And it says that they merged the change! Huzzah! We click on the number to take us back to the pull request we opened. We got an email! And it says they merged! Click that number to see the updated pull request. Here we are at the updated pull request page. Notice that everything has turned purple. Purple is the best color to see on GitHub; it’s the color of victory. It signals that our pull request was merged with the repo, meaning our change has become part of the repo! Also, notice the button that says Delete branch. Since all the work on our branch was merged with the repo, it has served its purpose and can be deleted safely. Everything is purple! Woot! Can safely delete our branch Now if we go back to the main page of the repo, we can see our merge was the most recent addition. And, if we scroll down, we will see that github_resources.Rmd, the file we edited, has been updated recently and it shows our commit message “fix typo”. We did it! Let’s check out the site to see our change published for the whole internet to see! Look! There’s our merged pull request added to the repo! And the edits we made to github_resources.Rmd! There it is! We go back to the page we edited and now our typo fix has been included!Note: The changes will take several minutes to appear on the site after notification of a successful merge. This is because we use Travis CI on the backend of our repo and it takes a little time for it to re-render the site pages. If you want to learn more about how you can use Travis CI to auto-magically generate your work, checkout our section on Hooking Up Travis to a GitHub bookdown book in the Publishing Resources page. Look at that! It’s published! So many exclamation points!!! We contributed to a GitHub repo! Hooray! Time to celebrate! via GIPHY If what you want to improve is a little more substantial (too difficult to contribute directly), read on. 4.3.2 Submit an issue If your proposed change is more complex, consider letting us know by submitting an issue. Maybe you have a great idea for a brand new chapter, something we have not covered but would like to see here in this resource (a new chart page, say; or a walkthrough using a specific tool/package). It may be a little too complicated to contribute directly. What to do? Submit an issue, of course! Issues are tasks you can post to a GitHub repo that people can then take on and fix. They can be small (“this link is broken” / “add this resource”) or complex (“I would love to have a chapter on…” / “reformat this code chunk in this way”). Once posted, issues can be taken on by anyone. You do not have to know how to code up your issue; from fixing a bug to proposing a resource we should link to, we appreciate any feedback you have and will take it all into consideration. How to submit issues: Go to our GitHub repo and click on the Issues Tab Click on “New Issue” Propose your Issue and click “Submit new issue” That’s it! We appreciate your input and will take your issue into account in improving edav.info/ Notes about submitting issues: Make sure your changes are not already an open issue (so as not to have redundant issues) Please thoroughly explain your proposed change when posting a new issue Consider using labels to specify the kind of issue, such as “bug”, “enhancement”, “help wanted”, “question”, or create your own. For more info, please consider reading the Open Source Guide on how to contribute. 4.3.3 Solve an open issue If you see an open issue that you think you can solve, by all means go for it! Simply fork our repo, add to the code base, and submit your work as a pull request. Checkout our open issues to see what needs doing. We appreciate any input you may have. Note: before getting too far into changing something, let us know in the github issue that you are working on solving it. This makes sure we are all on the same page. Confused how to actually do what was mentioned above? We have a thorough walkthrough example that should help and make sure to checkout our GitHub references page for links to learn about GitHub. For more info, please consider reading the GitHub Guide on Forking Projects and the Open Source Guide on how to contribute. 4.4 Resources Our GitHub repo: Link to the GitHub repository for edav.info/ Open Source Guide: Fantastic guide on how to contribute to projects like this one Our Page of GitHub Resources: Confused about the GitHub basics? Checkout our page of resources (once you learn more about git, you’ll realize that was a joke). "],
["iris.html", "5 Walkthrough: Iris Scatterplot 5.1 Overview 5.2 Quick note on doing it the lazy way 5.3 Viewing data 5.4 Plotting data 5.5 Markdown etiquette 5.6 Overlapping data 5.7 Formatting for presentation 5.8 Alter appearance 5.9 Consider themes 5.10 Going deeper 5.11 Helpful links", " 5 Walkthrough: Iris Scatterplot 5.1 Overview This example goes through some work with the iris dataset to get to a finished scatterplot that is ready to present. 5.1.1 tl;dr Here’s what we end up with: library(ggplot2) base_plot &lt;- ggplot(data = iris, mapping = aes(x = Sepal.Length, y = Sepal.Width)) + geom_point(aes(color = Species), size = 3, alpha = 0.5, position = &quot;jitter&quot;) + xlab(&quot;Sepal Length (cm)&quot;) + ylab(&quot;Sepal Width (cm)&quot;) + ggtitle(&quot;Sepal Dimensions in Different Species of Iris Flowers&quot;) base_plot + theme_minimal() Wondering how we got there? Read on. 5.1.2 Packages ggplot2 dplyr stats Base datasets (gridExtra) 5.1.3 Techniques Keyboard Shortcuts Viewing Data Structure/Dimensions/etc. Accessing Documentation Plotting with ggplot2 Layered Nature of ggplot2/Grammar of Graphics Mapping aesthetics in ggplot2 Overlapping Data: alpha and jitter Presenting Graphics Themes 5.2 Quick note on doing it the lazy way Shortcuts are your best friend to get work done faster. And they are easy to find. In the toolbar: Tools &gt; Keyboard Shortcuts Help OR ⌥⇧K Some good ones: Insert assignment operator (&lt;-): Alt/Option+- Insert pipe (%&gt;%): Ctrl/Cmd+Shift+M Comment Code: Ctrl/Cmd+Shift+C Run current line/selection: Ctrl/Cmd+Enter Re-run previous region: Ctrl/Cmd+Shift+P Be on the lookout for things you do often and try to see if there is a faster way to do them. Additionally, the RStudio IDE can be a little daunting, but it is full of useful tools that you can read about in this cheatsheet or go through with this DataCamp course: Part 1, Part 2. Okay, now let’s get to it… 5.3 Viewing data Let’s start with loading the package so we can get the data as a dataframe. library(datasets) class(iris) ## [1] &quot;data.frame&quot; This is not a huge dataset, but it is helpful to get into the habit of treating datasets as large no matter what. Because of this, make sure you inspect the size and structure of your dataset before going and printing it to the console. Here we can see that we have 150 observations across 5 different variables. dim(iris) ## [1] 150 5 There are a bunch of ways to get information on your dataset. Here are a few: str(iris) ## &#39;data.frame&#39;: 150 obs. of 5 variables: ## $ Sepal.Length: num 5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ... ## $ Sepal.Width : num 3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ... ## $ Petal.Length: num 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ... ## $ Petal.Width : num 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ... ## $ Species : Factor w/ 3 levels &quot;setosa&quot;,&quot;versicolor&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... summary(iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## Min. :4.300 Min. :2.000 Min. :1.000 Min. :0.100 ## 1st Qu.:5.100 1st Qu.:2.800 1st Qu.:1.600 1st Qu.:0.300 ## Median :5.800 Median :3.000 Median :4.350 Median :1.300 ## Mean :5.843 Mean :3.057 Mean :3.758 Mean :1.199 ## 3rd Qu.:6.400 3rd Qu.:3.300 3rd Qu.:5.100 3rd Qu.:1.800 ## Max. :7.900 Max. :4.400 Max. :6.900 Max. :2.500 ## Species ## setosa :50 ## versicolor:50 ## virginica :50 ## ## ## # This one requires dplyr, but it&#39;s worth it :) library(dplyr) glimpse(iris) ## Observations: 150 ## Variables: 5 ## $ Sepal.Length &lt;dbl&gt; 5.1, 4.9, 4.7, 4.6, 5.0, 5.4, 4.6, 5.0, 4.4, 4.9, 5… ## $ Sepal.Width &lt;dbl&gt; 3.5, 3.0, 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 2.9, 3.1, 3… ## $ Petal.Length &lt;dbl&gt; 1.4, 1.4, 1.3, 1.5, 1.4, 1.7, 1.4, 1.5, 1.4, 1.5, 1… ## $ Petal.Width &lt;dbl&gt; 0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.2, 0.1, 0… ## $ Species &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, set… Plotting the data by calling iris to the console will print the whole thing. Go ahead and try it in this case, but this is not recommended for larger datasets. Instead, use head() in the console or View(). If you want to learn more about these commands, or anything for that matter, just type ?&lt;command&gt; into the console. ?head, for example, will reveal that there is an additional argument to head called n for the number of lines printed, which defaults to 6. Also, you may notice there is something called tail. I wonder what that does? 5.4 Plotting data Let’s plot something! # Something&#39;s missing library(ggplot2) ggplot(iris) Where is it? Maybe if we add some aesthetics. I remember that was an important word that came up somewhere: # Still not working... ggplot(data = iris, mapping = aes(x = Sepal.Length, y = Sepal.Width)) Still nothing. Remember, you have to add a geom for something to show up. # There we go! ggplot(data = iris, mapping = aes(x = Sepal.Length, y = Sepal.Width)) + geom_point() Yay! Something showed up! Notice where we put the data, inside of ggplot(). ggplot is built on layers. Here we put it in the main call to ggplot. The data argument is also available in geom_point(), but in that case it would only apply to that layer. Here, we are saying, for all layers, unless specified, make the data be iris. Now let’s add a color mapping by Species: ggplot(data = iris, mapping = aes(x = Sepal.Length, y = Sepal.Width)) + geom_point(aes(color = Species)) Usually it is helpful to store the main portion of the plot in a variable and add on the layers. The code below achieves the same output as above: sepal_plot &lt;- ggplot(data = iris, mapping = aes(x = Sepal.Length, y = Sepal.Width)) sepal_plot + geom_point(aes(color = Species)) 5.5 Markdown etiquette I’m seeing that my R Markdown file is getting a little messy. Working with markdown and chunks can get out of hand, but there are some helpful tricks. First, consider naming your chunks as you go. If you combine this with headers, your work will be much more organized. Specifically, the little line at the bottom of the editor becomes much more useful. From this: To this: Just add a name to the start of each chunk: {r &lt;cool-code-chunk-name&gt;, &lt;chunk_option&gt; = TRUE} Now you can see what the chunks were about as well as get a sense of where you are in the document. Just don’t forget, it is a space after the r and commas for the other chunk options you may have like eval or echo. For more info, see our section on communicating results. 5.6 Overlapping data Eagle-eyed viewers may notice that we seem to be a few points short. We should be seeing 150 points, but we only see 117 (yes, I counted). Where are those 33 missing points? They are actually hiding behind other points. This dataset rounds to the nearest tenth of a centimeter, which is what is giving us those regular placings of the points. How did I know the data was in centimeters? Running ?iris in the console of course! Ah, you ask a silly question, you get a silly answer. # This plot hides some of the points ggplot(data = iris, mapping = aes(x = Sepal.Length, y = Sepal.Width)) + geom_point(aes(color = Species)) What’s the culprit? The color aesthetic. The color by default is opaque and will hide any points that are behind it. As a rule, it is always beneficial to reduce the opacity a little no matter what to avoid this problem. To do this, change the alpha value to something other than it’s default 1, like 0.5. ggplot(data = iris, mapping = aes(x = Sepal.Length, y = Sepal.Width)) + geom_point(aes(color = Species, alpha = 0.5)) Okay…a couple things with this. 5.6.1 First: the legend First, did you notice the new addition to the legend? That looks silly! Why did that show up? Well, when we added the alpha into aes(), we got a new legend. Let’s look at what we are doing with geom_point(). Specifically, this is saying how we should map the color and alpha: geom_point(mapping = aes(color = Species, alpha = 0.5)) So, we are mapping these given aesthetics, color and alpha, to certain values. ggplot knows that usually the aesthetic mapping will vary since you are probably passing in data that varies, so it will create a legend for each mapping. However, we don’t need a legend for the alpha: we explicitly set it to be 0.5. To fix this, we can pull alpha out of aes and instead treat it like an attribute: ggplot(data = iris, mapping = aes(x = Sepal.Length, y = Sepal.Width)) + geom_point(aes(color = Species), alpha = 0.5) No more legend. So, in ggplot, there is a difference between where an aesthetic is placed. It is also called MAPPING an aesthetic (making it vary with data inside aes) or SETTING an aesthetic (make it a constant attribute across all datapoints outside of aes). 5.6.2 Second: jittering Secondly, did this alpha trick really help us? Are we able to see anything in the plot in an easier way? Not really. Since the points perfectly overlap, the opacity difference doesn’t help us much. Usually, opacity will work, but here the data is so regular that we don’t gain anything in the perception department. We can fix this by introducing some jitter to the datapoints. Jitter adds a little random noise and moves the datapoints so that they don’t fully overlap: ggplot(data = iris, mapping = aes(x = Sepal.Length, y = Sepal.Width)) + geom_point(aes(color = Species), alpha = 0.5, position = &quot;jitter&quot;) Consider your motives when using jittering. You are by definition altering the data, but it may be beneficial in some situations. 5.6.3 Aside: example where alpha blending works We are dealing with a case where jittering works best to see the data, while changing the alpha doesn’t help us much. Here’s a quick example where opacity using alpha might be more directly helpful. # lib for arranging plots side by side library(gridExtra) # make some normally distributed data x_points &lt;- rnorm(n = 10000, mean = 0, sd = 2) y_points &lt;- rnorm(n = 10000, mean = 6, sd = 2) df &lt;- data.frame(x_points, y_points) # plot with/without changed alpha plt1 &lt;- ggplot(df, aes(x_points, y_points)) + geom_point() + ggtitle(&quot;Before (alpha = 1)&quot;) plt2 &lt;- ggplot(df, aes(x_points, y_points)) + geom_point(alpha = 0.1) + ggtitle(&quot;After (alpha = 0.1)&quot;) # arrange plots gridExtra::grid.arrange(plt1, plt2, ncol = 2, nrow = 1) Here it is much easier to see where the dataset is concentrated. 5.7 Formatting for presentation Let’s say we have finished this plot and we are ready to present it to other people: We should clean it up a bit so it can stand on its own. 5.8 Alter appearance First, let’s make the x/y labels a little cleaner and more descriptive: ggplot(data = iris, mapping = aes(x = Sepal.Length, y = Sepal.Width)) + geom_point(aes(color = Species), alpha = 0.5, position = &quot;jitter&quot;) + xlab(&quot;Sepal Length (cm)&quot;) + ylab(&quot;Sepal Width (cm)&quot;) Next, add a title that encapsulates the plot: ggplot(data = iris, mapping = aes(x = Sepal.Length, y = Sepal.Width)) + geom_point(aes(color = Species), alpha = 0.5, position = &quot;jitter&quot;) + xlab(&quot;Sepal Length (cm)&quot;) + ylab(&quot;Sepal Width (cm)&quot;) + ggtitle(&quot;Sepal Dimensions in Different Species of Iris Flowers&quot;) And make the points a little bigger: ggplot(data = iris, mapping = aes(x = Sepal.Length, y = Sepal.Width)) + geom_point(aes(color = Species), size = 3, alpha = 0.5, position = &quot;jitter&quot;) + xlab(&quot;Sepal Length (cm)&quot;) + ylab(&quot;Sepal Width (cm)&quot;) + ggtitle(&quot;Sepal Dimensions in Different Species of Iris Flowers&quot;) Now it’s looking presentable. 5.9 Consider themes It may be better for your situation to change the theme of the plot (the background, axes, etc.; the “accessories” of the plot). Explore what different themes can offer and pick one that is right for you. base_plot &lt;- ggplot(data = iris, mapping = aes(x = Sepal.Length, y = Sepal.Width)) + geom_point(aes(color = Species), size = 3, alpha = 0.5, position = &quot;jitter&quot;) + xlab(&quot;Sepal Length (cm)&quot;) + ylab(&quot;Sepal Width (cm)&quot;) + ggtitle(&quot;Sepal Dimensions in Different Species of Iris Flowers&quot;) base_plot base_plot + theme_light() base_plot + theme_minimal() base_plot + theme_classic() base_plot + theme_void() I’m going to go with theme_minimal() this time. So here we are! We got a lovely scatterplot ready to show the world! 5.10 Going deeper We have just touched the surface of ggplot and dipped our toes into grammar of graphics. If you want to go deeper, I highly recommend the DataCamp courses on Data Visualization with ggplot2 with Rick Scavetta. There are three parts and they are quite dense, but the first part is definitely worth checking out. 5.11 Helpful links RStudio ggplot2 Cheat Sheet DataCamp: Mapping aesthetics to things in ggplot R Markdown Reference Guide R for Data Science "],
["tidy.html", "6 Walkthrough: Tidy Data &amp; dplyr 6.1 Overview 6.2 Installing packages 6.3 Viewing the data 6.4 What is Tidy data? 6.5 Tibbles 6.6 Test for missing values 6.7 Recode the missing values 6.8 Data wrangling verbs 6.9 Rename 6.10 Select 6.11 Mutate 6.12 Filter 6.13 Arrange 6.14 Summarize &amp; Group By 6.15 Pipe Operator 6.16 Tidying the transformed data 6.17 Helpful Links", " 6 Walkthrough: Tidy Data &amp; dplyr This chapter originated as a community contribution created by akshatapatel This page is a work in progress. We appreciate any input you may have. If you would like to help improve this page, consider contributing to our repo. 6.1 Overview This example goes through some work with the biopsy dataset using dplyr functions to get to a tidy dataset. 6.1.1 Packages dplyr MASS tidyr 6.2 Installing packages Write the following statements in the console: install.packages('dplyr') install.packages('ggplot2') install.packages('tidyr') install.packages('MASS') Note: The first three packages are a part of the tidyverse, a collection of helpful packages in R, which can all be installed using install.packages('tidyverse'). dplyr is used for data wrangling and data transformation in data frames. The “d” in “dplyr” stands for “data frames” which is the most-used data type for storing datasets in R. 6.3 Viewing the data Let’s start with loading the package so we can get the data as a dataframe: #loading the dplyr library library(dplyr) #loading data from MASS:biopsy library(MASS) class(biopsy) ## [1] &quot;data.frame&quot; #glimpse is a part of the dplyr package glimpse(biopsy) ## Observations: 699 ## Variables: 11 ## $ ID &lt;chr&gt; &quot;1000025&quot;, &quot;1002945&quot;, &quot;1015425&quot;, &quot;1016277&quot;, &quot;1017023&quot;, &quot;10… ## $ V1 &lt;int&gt; 5, 5, 3, 6, 4, 8, 1, 2, 2, 4, 1, 2, 5, 1, 8, 7, 4, 4, 10, … ## $ V2 &lt;int&gt; 1, 4, 1, 8, 1, 10, 1, 1, 1, 2, 1, 1, 3, 1, 7, 4, 1, 1, 7, … ## $ V3 &lt;int&gt; 1, 4, 1, 8, 1, 10, 1, 2, 1, 1, 1, 1, 3, 1, 5, 6, 1, 1, 7, … ## $ V4 &lt;int&gt; 1, 5, 1, 1, 3, 8, 1, 1, 1, 1, 1, 1, 3, 1, 10, 4, 1, 1, 6, … ## $ V5 &lt;int&gt; 2, 7, 2, 3, 2, 7, 2, 2, 2, 2, 1, 2, 2, 2, 7, 6, 2, 2, 4, 2… ## $ V6 &lt;int&gt; 1, 10, 2, 4, 1, 10, 10, 1, 1, 1, 1, 1, 3, 3, 9, 1, 1, 1, 1… ## $ V7 &lt;int&gt; 3, 3, 3, 3, 3, 9, 3, 3, 1, 2, 3, 2, 4, 3, 5, 4, 2, 3, 4, 3… ## $ V8 &lt;int&gt; 1, 2, 1, 7, 1, 7, 1, 1, 1, 1, 1, 1, 4, 1, 5, 3, 1, 1, 1, 1… ## $ V9 &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 5, 1, 1, 1, 1, 1, 4, 1, 1, 1, 2, 1… ## $ class &lt;fct&gt; benign, benign, benign, benign, benign, malignant, benign,… head(biopsy) ## ID V1 V2 V3 V4 V5 V6 V7 V8 V9 class ## 1 1000025 5 1 1 1 2 1 3 1 1 benign ## 2 1002945 5 4 4 5 7 10 3 2 1 benign ## 3 1015425 3 1 1 1 2 2 3 1 1 benign ## 4 1016277 6 8 8 1 3 4 3 7 1 benign ## 5 1017023 4 1 1 3 2 1 3 1 1 benign ## 6 1017122 8 10 10 8 7 10 9 7 1 malignant 6.4 What is Tidy data? What does it mean for your data to be tidy? Tidy data has a standardized format and it is a consistent way to organize your data in R. Here’s the definition of Tidy Data given by Hadley Wickham: A dataset is messy or tidy depending on how rows, columns and tables are matched up with observations, variables and types. In tidy data: Each variable forms a column. Each observation forms a row. Each observational unit forms a value in the table. See r4ds on tidy data for more info. What are the advantages of tidy data? Uniformity : It is easier to learn the tools that work with the data because they have a consistent way of storing data. Most built-in R functions work with vectors of values. Thus, having variables as columns/vectors allows R’s vectorized nature to shine. Can you observe and tell why this data is messy? The names of the columns such as V1, V2 are not intuitive in what they contain; good sign it is untidy. They are not different variables, but are values of a common variable. Now, we will see the how to transform our data using dplyr functions and then look at how to tidy our transformed data. 6.5 Tibbles A tibble is a modern re-imagining of the data frame. It is particularly useful for large datasets because it only prints the first few rows. It helps you confront problems early, leading to cleaner code. # Converting a df to a tibble biopsy &lt;- tbl_df(biopsy) biopsy ## # A tibble: 699 x 11 ## ID V1 V2 V3 V4 V5 V6 V7 V8 V9 class ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt; ## 1 1000025 5 1 1 1 2 1 3 1 1 benign ## 2 1002945 5 4 4 5 7 10 3 2 1 benign ## 3 1015425 3 1 1 1 2 2 3 1 1 benign ## 4 1016277 6 8 8 1 3 4 3 7 1 benign ## 5 1017023 4 1 1 3 2 1 3 1 1 benign ## 6 1017122 8 10 10 8 7 10 9 7 1 malignant ## 7 1018099 1 1 1 1 2 10 3 1 1 benign ## 8 1018561 2 1 2 1 2 1 3 1 1 benign ## 9 1033078 2 1 1 1 2 1 1 1 5 benign ## 10 1033078 4 2 1 1 2 1 2 1 1 benign ## # … with 689 more rows 6.6 Test for missing values # Number of missing values in each column in the data frame colSums(is.na(biopsy)) ## ID V1 V2 V3 V4 V5 V6 V7 V8 V9 class ## 0 0 0 0 0 0 16 0 0 0 0 The dataset contains missing values which need to be addressed. 6.7 Recode the missing values One way to deal with missing values is to recode them with the average of all the other values in that column: #change all the NAs to mean of the column biopsy$V6[is.na(biopsy$V6)] &lt;- mean(biopsy$V6, na.rm = TRUE) colSums(is.na(biopsy)) ## ID V1 V2 V3 V4 V5 V6 V7 V8 V9 class ## 0 0 0 0 0 0 0 0 0 0 0 See our chapter on time series with missing data for more info about dealing with missing data. 6.8 Data wrangling verbs Here are the most commonly used functions that help wrangle and summarize data: Rename Select Mutate Filter Arrange Summarize Group_by Select and mutate functions manipulate the variable (the columns of the data frame). Filter and arrange functions manipulate the observations (the rows of the data) ,whereas the summarize function manipulates groups of observations. All the dplyr functions work on a copy of the data and return a modified copy. They do not change the original data frame. If we want to access the results afterwards, we need to save the modified copy. 6.9 Rename The names of the columns in our biopsy data are very vague and do not give us the meaning of the values in that column. We need to change the names of the column so that the viewer gets a sense of the values they’re referring to. rename(biopsy, thickness = V1,cell_size = V2, cell_shape = V3, marg_adhesion = V4, epithelial_cell_size = V5, bare_nuclei = V6, chromatin = V7, norm_nucleoli = V8, mitoses = V9) ## # A tibble: 699 x 11 ## ID thickness cell_size cell_shape marg_adhesion epithelial_cell… ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 1000… 5 1 1 1 2 ## 2 1002… 5 4 4 5 7 ## 3 1015… 3 1 1 1 2 ## 4 1016… 6 8 8 1 3 ## 5 1017… 4 1 1 3 2 ## 6 1017… 8 10 10 8 7 ## 7 1018… 1 1 1 1 2 ## 8 1018… 2 1 2 1 2 ## 9 1033… 2 1 1 1 2 ## 10 1033… 4 2 1 1 2 ## # … with 689 more rows, and 5 more variables: bare_nuclei &lt;dbl&gt;, ## # chromatin &lt;int&gt;, norm_nucleoli &lt;int&gt;, mitoses &lt;int&gt;, class &lt;fct&gt; The tibble shown above is not saved and cannot be used further. To use it afterwards we save it as a new tibble: #saving the rename function output biopsy_new&lt;-rename(biopsy, thickness = V1,cell_size = V2, cell_shape = V3, marg_adhesion = V4, epithelial_cell_size = V5, bare_nuclei = V6, chromatin = V7, norm_nucleoli = V8, mitoses = V9) head(biopsy_new,5) ## # A tibble: 5 x 11 ## ID thickness cell_size cell_shape marg_adhesion epithelial_cell… ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 1000… 5 1 1 1 2 ## 2 1002… 5 4 4 5 7 ## 3 1015… 3 1 1 1 2 ## 4 1016… 6 8 8 1 3 ## 5 1017… 4 1 1 3 2 ## # … with 5 more variables: bare_nuclei &lt;dbl&gt;, chromatin &lt;int&gt;, ## # norm_nucleoli &lt;int&gt;, mitoses &lt;int&gt;, class &lt;fct&gt; The biopsy_new data frame can now be used for further manipulation. 6.10 Select Select returns a subset of the data. Specifically, only the columns that are specified are included. In the biopsy data, we do not require the variables “chromatin” and “mitoses”. So, let’s drop them using a minus sign: #selecting all except the columns chromatin and mitoses biopsy_new&lt;-select(biopsy_new,-chromatin,-mitoses) head(biopsy_new,5) ## # A tibble: 5 x 9 ## ID thickness cell_size cell_shape marg_adhesion epithelial_cell… ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 1000… 5 1 1 1 2 ## 2 1002… 5 4 4 5 7 ## 3 1015… 3 1 1 1 2 ## 4 1016… 6 8 8 1 3 ## 5 1017… 4 1 1 3 2 ## # … with 3 more variables: bare_nuclei &lt;dbl&gt;, norm_nucleoli &lt;int&gt;, ## # class &lt;fct&gt; 6.11 Mutate The mutate function computes new variables from the already existing variables and adds them to the dataset. It gives information that the data already contained but was never displayed. The “V6” variable contains the values of the bare nucleus from 1.00 to 10.00. If we wish to normalize the variable, we can use the mutate function: #normalize the bare nuclei values maximum_bare_nuclei&lt;-max(biopsy_new$bare_nuclei,na.rm=TRUE) biopsy_new&lt;-mutate(biopsy_new,bare_nuclei=bare_nuclei/maximum_bare_nuclei) head(biopsy_new,5) ## # A tibble: 5 x 9 ## ID thickness cell_size cell_shape marg_adhesion epithelial_cell… ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 1000… 5 1 1 1 2 ## 2 1002… 5 4 4 5 7 ## 3 1015… 3 1 1 1 2 ## 4 1016… 6 8 8 1 3 ## 5 1017… 4 1 1 3 2 ## # … with 3 more variables: bare_nuclei &lt;dbl&gt;, norm_nucleoli &lt;int&gt;, ## # class &lt;fct&gt; 6.12 Filter Filter is the row-equivalent function of select; it returns a modified copy that contains only certain rows. This function filters rows based on the content and the conditions supplied in its argument. The filter function takes the data frame as the first argument. The next argument contains one or more logical tests. The rows/observations that pass these logical tests are returned in the result of the filter function. For our example, we only want the data of those tumor cells that have clump thickness greater than six as most of the malign tumors have this thickness looking at a plot of clump thickness vs tumor cell size grouped by class: library(ggplot2) ggplot(biopsy_new)+ geom_point(aes(x=thickness,y=cell_size,color=class))+ ggtitle(&quot;Plot of Clump Thickness Vs Tumor Cell Size&quot;) #normalize the bare nuclei values biopsy_new&lt;-filter(biopsy_new,thickness&gt;5.5) head(biopsy_new,5) ## # A tibble: 5 x 9 ## ID thickness cell_size cell_shape marg_adhesion epithelial_cell… ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 1016… 6 8 8 1 3 ## 2 1017… 8 10 10 8 7 ## 3 1044… 8 7 5 10 7 ## 4 1047… 7 4 6 4 6 ## 5 1050… 10 7 7 6 4 ## # … with 3 more variables: bare_nuclei &lt;dbl&gt;, norm_nucleoli &lt;int&gt;, ## # class &lt;fct&gt; 6.13 Arrange Arrange reorders the rows of the data based on their contents in the ascending order by default. The doctors would want to view the data in the order of the cell size of the tumor. #arrange in the order of V2:cell size arrange(biopsy_new,cell_size) ## # A tibble: 186 x 9 ## ID thickness cell_size cell_shape marg_adhesion epithelial_cell… ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 1050… 6 1 1 1 2 ## 2 1204… 6 1 1 1 2 ## 3 1223… 6 1 3 1 2 ## 4 5435… 6 1 3 1 4 ## 5 63375 9 1 2 6 4 ## 6 7529… 10 1 1 1 2 ## 7 1276… 6 1 1 3 2 ## 8 1238… 6 1 1 3 2 ## 9 1257… 6 1 1 1 1 ## 10 1224… 6 1 1 1 2 ## # … with 176 more rows, and 3 more variables: bare_nuclei &lt;dbl&gt;, ## # norm_nucleoli &lt;int&gt;, class &lt;fct&gt; This shows the data in increasing order of the cell size. To arrange the rows in decreasing order of V2, we add the desc() function to the variable before passing it to arrange. #arrange in the order of V2:cell size in decreasing order arrange(biopsy_new,desc(cell_size)) ## # A tibble: 186 x 9 ## ID thickness cell_size cell_shape marg_adhesion epithelial_cell… ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 1017… 8 10 10 8 7 ## 2 1080… 10 10 10 8 6 ## 3 1100… 6 10 10 2 8 ## 4 1103… 10 10 10 4 8 ## 5 1112… 8 10 10 1 3 ## 6 1116… 9 10 10 1 10 ## 7 1123… 6 10 2 8 10 ## 8 1168… 10 10 10 10 10 ## 9 1170… 10 10 10 8 2 ## 10 1173… 10 10 10 3 10 ## # … with 176 more rows, and 3 more variables: bare_nuclei &lt;dbl&gt;, ## # norm_nucleoli &lt;int&gt;, class &lt;fct&gt; As you can see, there are a number of rows with the same value of V2:cell_size. To break the tie, you can add another variable to be used for ordering when the first variable has the same value. Here, we use the tie breaker as the order of variable V3: by cell shape and by ID: #arrange in the order of V2:cell size biopsy_new&lt;-arrange(biopsy_new,desc(cell_size),desc(cell_shape),ID) head(biopsy_new,5) ## # A tibble: 5 x 9 ## ID thickness cell_size cell_shape marg_adhesion epithelial_cell… ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 1017… 8 10 10 8 7 ## 2 1073… 10 10 10 10 6 ## 3 1080… 10 10 10 8 6 ## 4 1100… 6 10 10 2 8 ## 5 1100… 6 10 10 2 8 ## # … with 3 more variables: bare_nuclei &lt;dbl&gt;, norm_nucleoli &lt;int&gt;, ## # class &lt;fct&gt; 6.14 Summarize &amp; Group By Summarize uses the data to create a new data frame with the summary statistics such as minimum, maximum, average, and so on. These statistical functions must be aggregate functions which take a vector of values as input and output a single value. The group_by function groups the data by the values of the variables. This, along with summarize, makes observations about groups of rows of the dataset. The doctors would want to see the maximum cell size and the thickness for each of the classes: benign and malignant. This can be done by grouping the data by class and finding the maximum of the required variables: biopsy_grouped &lt;- group_by(biopsy_new,class) summarize(biopsy_grouped, max(thickness), mean(cell_size), var(norm_nucleoli)) ## # A tibble: 2 x 4 ## class `max(thickness)` `mean(cell_size)` `var(norm_nucleoli)` ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 benign 8 2.67 5.93 ## 2 malignant 10 6.73 11.3 6.15 Pipe Operator What if we want to use the various data wrangling verbs together? This could be done by saving the result of each wrangling function in a new variable and using it for the next function as we did above. However, this is not recommended as: It requires extra typing and longer code. Unnecessary space is used up to save the various variables. If the data is large, this method slows down the analysis. The pipe operator can be used instead for the same purpose. The operator is placed between and object and the function. The pipe takes the object on its left and passes it as the first argument to the function to its right. The pipe operator is a part of the magrittr package. However, this package need not be loaded as the dplyr package makes life simpler and imports the pipe operator for us: biopsy_grouped &lt;- biopsy_new %&gt;% group_by(class) %&gt;% summarize(max(thickness),mean(cell_size),var(norm_nucleoli)) head(biopsy_grouped) ## # A tibble: 2 x 4 ## class `max(thickness)` `mean(cell_size)` `var(norm_nucleoli)` ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 benign 8 2.67 5.93 ## 2 malignant 10 6.73 11.3 6.16 Tidying the transformed data Have a look again at the messy data: # Messy Data head(biopsy_new,5) ## # A tibble: 5 x 9 ## ID thickness cell_size cell_shape marg_adhesion epithelial_cell… ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 1017… 8 10 10 8 7 ## 2 1073… 10 10 10 10 6 ## 3 1080… 10 10 10 8 6 ## 4 1100… 6 10 10 2 8 ## 5 1100… 6 10 10 2 8 ## # … with 3 more variables: bare_nuclei &lt;dbl&gt;, norm_nucleoli &lt;int&gt;, ## # class &lt;fct&gt; Planning is required to decide which columns we need to keep unchanged, which ones to change, and what names are to be given to the new columns. The columns to keep are the ones that are already tidy. The ones to change are the ones that aren’t true variables but in fact levels of another variable. So, the ID and class columns are already tidy. These are kept as is. The columns V1:thickness, V2:cell_size, V3:cell_shape, V4:marg_adhesion, V5:epithelial_cell_size, V6:bare_nuclei, and V8:norm_nucleoli are not true variables but values of the variable Tumor_attributes. We can fix this with tidyr::gather(), which is used to convert data from messy to tidy. The gather function takes the data frame which we want to tidy as input. The next two parameters are the names of the key and the value columns in the tidy dataset. In our example, key=‘Tumor_Atrributes’ and value=‘Score’. You can also specify the columns that you do not want to be tidied, i.e. ID and class: #Tidy Data library(tidyr) tidy_df &lt;- biopsy_new %&gt;% gather(key = &quot;Tumor_Attributes&quot;, value = &quot;Score&quot;, -ID, -class) tidy_df ## # A tibble: 1,302 x 4 ## ID class Tumor_Attributes Score ## &lt;chr&gt; &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1017122 malignant thickness 8 ## 2 1073960 malignant thickness 10 ## 3 1080185 malignant thickness 10 ## 4 1100524 malignant thickness 6 ## 5 1100524 malignant thickness 6 ## 6 1103608 malignant thickness 10 ## 7 1112209 malignant thickness 8 ## 8 1116116 malignant thickness 9 ## 9 1116116 malignant thickness 9 ## 10 1168736 malignant thickness 10 ## # … with 1,292 more rows 6.17 Helpful Links r4ds on tidy data: It is always best to learn from the source, so a textbook written by Hadley Wickham is perfect. DataCamp dplyr course: This course covers the different fucntions in dplyr and how they manipulate data. "],
["missingTS.html", "7 Time Series with Missing Data 7.1 Overview 7.2 Motivation 7.3 Visualizing missing values 7.4 Imputing missing values 7.5 Further reading", " 7 Time Series with Missing Data This chapter originated as a community contribution created by sdt2134 This page is a work in progress. We appreciate any input you may have. If you would like to help improve this page, consider contributing to our repo. 7.1 Overview For data scientists, data preparation could easily be the second most frustrating task, right after explaining their models to other departments. Missing value treatment is a key part of data preparation and knowing how to handle it well can reduce the excruciating pain one feels after seeing a poor RMSE. The chapter on missing data talks about some of the ways you could handle missing data in your dataset; this walkthrough takes it further focusing specifically on timeseries data. 7.2 Motivation Why should you care when you have mean, median &amp; mode? You should care because time series problems are not that straight-forward. Most often time series are accompanied by forecasting tasks and most algorithms won’t allow missing data. Imputation using mean, median &amp; mode might hide trends or seasonal patterns whereas removing missing data points altogether might reduce information contained in other features for those cases. ImputeTS in R provides a bunch of functions to visualize and approximate missing values with high precision. But first you must ask yourself two questions: Is there any identifiable reason for it? Are they missing at random? Is there any identifiable reason for it? Let’s take the example of a bakery. You are forecasting daily cake sales and find missing data is linked to holidays. How to impute? One way could be to fill with zeroes and create a flag that could help your forecasting algorithm understand this pattern. If sales are missing if your cakes were out of stock or the baker was on leave such flags won’t work as you cannot predict these events in the future. Imputation is ideal for treating such cases. What if they are missing at random? You have only one option - imputation. Let’s go through a time series exercise where we have to decompose it when missing values are present. Dataset : a10 “Monthly anti-diabetic drug sales in Australia from 1992 to 2008” For this walkthrough we will use two copies of the a10 dataset. Here’s the original dataset from the fpp package complete_a10 &lt;- fpp::a10 Let’s create missing values and check the performance of various imputation functions on hidden values. set.seed(134) missatrand_a10 &lt;- complete_a10 missatrand_a10[sample(length(complete_a10),0.2*length(complete_a10))] &lt;- NA 7.2.0.1 Let’s analyse this timeseries graphically plot(missatrand_a10) Looks like the series has multiplicative seasonality. Let’s transform it into additive to see if seasonality appears more clearly. plot(log(missatrand_a10)) Wow, it shows a clear seasonal pattern with an increasing trend. 7.2.0.2 What if we decompose it into into its components plot(decompose(missatrand_a10, type = &quot;multiplicative&quot;)) ## Error in na.omit.ts(x): time series contains internal NAs Not so fast. As mentioned above, we must treat the missing values before we do this. Enough said let’s impute this missing piece into our toolkit! 7.3 Visualizing missing values library(imputeTS) plotNA.distribution(missatrand_a10) Let’s get a summary of the missing values. statsNA(missatrand_a10) ## [1] &quot;Length of time series:&quot; ## [1] 204 ## [1] &quot;-------------------------&quot; ## [1] &quot;Number of Missing Values:&quot; ## [1] 40 ## [1] &quot;-------------------------&quot; ## [1] &quot;Percentage of Missing Values:&quot; ## [1] &quot;19.6%&quot; ## [1] &quot;-------------------------&quot; ## [1] &quot;Stats for Bins&quot; ## [1] &quot; Bin 1 (51 values from 1 to 51) : 13 NAs (25.5%)&quot; ## [1] &quot; Bin 2 (51 values from 52 to 102) : 8 NAs (15.7%)&quot; ## [1] &quot; Bin 3 (51 values from 103 to 153) : 10 NAs (19.6%)&quot; ## [1] &quot; Bin 4 (51 values from 154 to 204) : 9 NAs (17.6%)&quot; ## [1] &quot;-------------------------&quot; ## [1] &quot;Longest NA gap (series of consecutive NAs)&quot; ## [1] &quot;3 in a row&quot; ## [1] &quot;-------------------------&quot; ## [1] &quot;Most frequent gap size (series of consecutive NA series)&quot; ## [1] &quot;1 NA in a row (occuring 29 times)&quot; ## [1] &quot;-------------------------&quot; ## [1] &quot;Gap size accounting for most NAs&quot; ## [1] &quot;1 NA in a row (occuring 29 times, making up for overall 29 NAs)&quot; ## [1] &quot;-------------------------&quot; ## [1] &quot;Overview NA series&quot; ## [1] &quot; 1 NA in a row: 29 times&quot; ## [1] &quot; 2 NA in a row: 4 times&quot; ## [1] &quot; 3 NA in a row: 1 times&quot; Are there any patterns in missingness, how many consecutive NA’s are there (gapsize), and how many missing values are accounted by a specific gapsize? plotNA.gapsize(missatrand_a10) 7.4 Imputing missing values 7.4.1 Generic methods : independent of time series’ temporal properties 7.4.1.1 Mean, median and mode This is a straight forward and simple method for interpolation. But given we are imputing the missing value by calculating the mean, we end up losing information around the trend and seasonality, leading to huge errors in imputation. Because of this reason, this method is best suited for data that is stationary. This function also allows us to use other measures like median and mode. 7.4.1.1.0.1 Method 1 imp &lt;- na.mean(missatrand_a10) p1&lt;-as.tibble(cbind(Date = as.yearmon(time(missatrand_a10)), missatrand = missatrand_a10,complete_a10 = complete_a10,imputed_val = imp))%&gt;% mutate(imputed_val = ifelse(is.na(missatrand),imputed_val,NA), complete_a10 = ifelse(is.na(missatrand),complete_a10,NA) )%&gt;% ggplot(aes(x=Date)) + #geom_line(aes(y=missatrand),color=&quot;red&quot;)+ geom_line(aes(y=missatrand),color = &quot;black&quot;)+ geom_point(aes(y=imputed_val),color = &quot;blue&quot;)+ geom_point(aes(y=complete_a10),color = &quot;red&quot;)+ ylab(&quot;Anti-diabetic drug sales&quot;)+ ggtitle(&quot;Imputed values (Blue); Real values (Red) &quot;)+ theme(plot.title = element_text(size = 10)) p2 &lt;- ggplot() + geom_line(aes(y = complete_a10-imp, x = as.yearmon(time(missatrand_a10)) )) + ylim(-10,10) + ylab(&quot;Errors&quot;)+ xlab(&quot;Date&quot;)+ ggtitle(paste(&quot;Errors in imputation&quot;))+ theme(plot.title = element_text(size = 10)) grid.arrange(p1, p2, ncol=2,top=&quot;Interpolation using mean&quot;) 7.4.1.2 Moving Averages As this function calculates moving averages based on the last n observations, it will generally be performing better than the last method. Moving averages work well when data has a linear trend. This function also allows us to use linear-weighted and exponentially-weighted moving averages. 7.4.1.2.0.1 Method 2 imp &lt;- na.ma(missatrand_a10) p1&lt;-as.tibble(cbind(Date = as.yearmon(time(missatrand_a10)), missatrand = missatrand_a10,complete_a10 = complete_a10,imputed_val = imp))%&gt;% mutate(imputed_val = ifelse(is.na(missatrand),imputed_val,NA), complete_a10 = ifelse(is.na(missatrand),complete_a10,NA) )%&gt;% ggplot(aes(x=Date)) + #geom_line(aes(y=missatrand),color=&quot;red&quot;)+ geom_line(aes(y=missatrand),color = &quot;black&quot;)+ geom_point(aes(y=imputed_val),color = &quot;blue&quot;)+ geom_point(aes(y=complete_a10),color = &quot;red&quot;)+ ylab(&quot;Anti-diabetic drug sales&quot;)+ ggtitle(&quot;Imputed values (Blue); Real values (Red) &quot;)+ theme(plot.title = element_text(size = 10)) p2 &lt;- ggplot() + geom_line(aes(y = complete_a10-imp, x = as.yearmon(time(missatrand_a10)) )) + ylim(-10,10) + ylab(&quot;Errors&quot;)+ xlab(&quot;Date&quot;)+ ggtitle(paste(&quot;Errors in imputation&quot;))+ theme(plot.title = element_text(size = 10)) grid.arrange(p1, p2, ncol=2,top=&quot;Interpolation using n-Moving Averages&quot;) 7.4.2 Time series specific methods 7.4.2.1 Kalman smoothing with auto.arima This is a more advanced function for interpolation and requires higher computation time. It uses Kalman Smoothing on ARIMA model to impute the missing values. Structural time series models are another type of model that can be used to impute missing values using this function . 7.4.2.1.0.1 Method 3 imp &lt;- na.kalman(missatrand_a10,model = &quot;auto.arima&quot;) p1&lt;-as.tibble(cbind(Date = as.yearmon(time(missatrand_a10)), missatrand = missatrand_a10,complete_a10 = complete_a10,imputed_val = imp))%&gt;% mutate(imputed_val = ifelse(is.na(missatrand),imputed_val,NA), complete_a10 = ifelse(is.na(missatrand),complete_a10,NA) )%&gt;% ggplot(aes(x=Date)) + #geom_line(aes(y=missatrand),color=&quot;red&quot;)+ geom_line(aes(y=missatrand),color = &quot;black&quot;)+ geom_point(aes(y=imputed_val),color = &quot;blue&quot;)+ geom_point(aes(y=complete_a10),color = &quot;red&quot;)+ ylab(&quot;Anti-diabetic drug sales&quot;)+ ggtitle(&quot;Imputed values (Blue); Real values (Red) &quot;)+ theme(plot.title = element_text(size = 10)) p2 &lt;- ggplot() + geom_line(aes(y = complete_a10-imp, x = as.yearmon(time(missatrand_a10)) )) + ylim(-10,10) + ylab(&quot;Errors&quot;)+ xlab(&quot;Date&quot;)+ ggtitle(paste(&quot;Errors in imputation&quot;))+ theme(plot.title = element_text(size = 10)) grid.arrange(p1, p2, ncol=2,top=&quot;Interpolation using Auto.arima&quot;) 7.4.2.2 Seasonal splitting This function interpolates missing values by splitting the data by seasons and performing imputation on the resulting datasets. Another similar function that works based on the seasonality component in this package is na.seadec() which deseasonalizes the data, performs imputation, and adds the seasonal component back again. 7.4.2.2.0.1 Method 4 imp &lt;- na.seasplit(missatrand_a10) p1&lt;-as.tibble(cbind(Date = as.yearmon(time(missatrand_a10)), missatrand = missatrand_a10,complete_a10 = complete_a10,imputed_val = imp))%&gt;% mutate(imputed_val = ifelse(is.na(missatrand),imputed_val,NA), complete_a10 = ifelse(is.na(missatrand),complete_a10,NA) )%&gt;% ggplot(aes(x=Date)) + #geom_line(aes(y=missatrand),color=&quot;red&quot;)+ geom_line(aes(y=missatrand),color = &quot;black&quot;)+ geom_point(aes(y=imputed_val),color = &quot;blue&quot;)+ geom_point(aes(y=complete_a10),color = &quot;red&quot;)+ ylab(&quot;Anti-diabetic drug sales&quot;)+ ggtitle(&quot;Imputed values (Blue); Real values (Red) &quot;)+ theme(plot.title = element_text(size = 10)) p2 &lt;- ggplot() + geom_line(aes(y = complete_a10-imp, x = as.yearmon(time(missatrand_a10)) )) + ylim(-10,10) + ylab(&quot;Errors&quot;)+ xlab(&quot;Date&quot;)+ ggtitle(paste(&quot;Errors in imputation&quot;))+ theme(plot.title = element_text(size = 10)) grid.arrange(p1, p2, ncol=2,top=&quot;Seasonally splitted interpolation&quot;) 7.5 Further reading Details about the imputeTS package can be found here. imputeTS To learn more about time series, decomposition, forecasting and more check Rob J. Hyndman’s notes from otext "],
["bar.html", "8 Chart: Bar Chart 8.1 Overview 8.2 tl;dr 8.3 Simple examples 8.4 Theory 8.5 When to use 8.6 Considerations 8.7 Modifications 8.8 External resources", " 8 Chart: Bar Chart 8.1 Overview This section covers how to make bar charts 8.2 tl;dr I want a nice example. Not tomorrow, not after breakfast. NOW! Here’s a bar chart showing the survival rates of passengers aboard the RMS Titanic: And here’s the code: library(datasets) # data library(ggplot2) # plotting library(dplyr) # manipulation # Combine Children and Adult stats together ship_grouped &lt;- as.data.frame(Titanic) %&gt;% group_by(Class, Sex, Survived) %&gt;% summarise(Total = sum(Freq)) ggplot(ship_grouped, aes(x = Survived, y = Total, fill = Sex)) + geom_bar(position = &quot;dodge&quot;, stat = &quot;identity&quot;) + geom_text(aes(label = Total), position = position_dodge(width = 0.9), vjust = -0.4, color = &quot;grey68&quot;) + facet_wrap(~Class) + # formatting ylim(0, 750) + ggtitle(&quot;Don&#39;t Be A Crew Member On The Titanic&quot;, subtitle = &quot;Survival Rates of Titanic Passengers by Class and Gender&quot;) + scale_fill_manual(values = c(&quot;#b2df8a&quot;, &quot;#a6cee3&quot;)) + labs(y = &quot;Passenger Count&quot;, caption = &quot;Source: titanic::titanic_train&quot;) + theme(plot.title = element_text(face = &quot;bold&quot;)) + theme(plot.subtitle = element_text(face = &quot;bold&quot;, color = &quot;grey35&quot;)) + theme(plot.caption = element_text(color = &quot;grey68&quot;)) For more info on this dataset, type ?datasets::Titanic into the console. 8.3 Simple examples My eyes were bigger than my stomach. Much simpler please! Let’s use the HairEyeColor dataset. To start, we will just look at the different categories of hair color among females: colors &lt;- as.data.frame(HairEyeColor) # just female hair color, using dplyr colors_female_hair &lt;- colors %&gt;% filter(Sex == &quot;Female&quot;) %&gt;% group_by(Hair) %&gt;% summarise(Total = sum(Freq)) # take a look at data head(colors_female_hair) ## # A tibble: 4 x 2 ## Hair Total ## &lt;fct&gt; &lt;dbl&gt; ## 1 Black 52 ## 2 Brown 143 ## 3 Red 37 ## 4 Blond 81 Now let’s make some graphs with this data. 8.3.1 Bar graph using base R barplot(colors_female_hair[[&quot;Total&quot;]], names.arg = colors_female_hair[[&quot;Hair&quot;]], main = &quot;Bar Graph Using Base R&quot;) We recommend using Base R only for simple bar graphs for yourself. Like all of Base R, it is simple to setup. Note: Base R expects a vector or matrix, hence the double brackets in the barplot call (gets columns as lists). 8.3.2 Bar graph using ggplot2 library(ggplot2) # plotting ggplot(colors_female_hair, aes(x = Hair, y = Total)) + geom_bar(stat = &quot;identity&quot;) + ggtitle(&quot;Bar Graph Using ggplot2&quot;) Bar plots are very easy in ggplot2. You pass in a dataframe and let it know which parts you want to map to different aesthetics. Note: In this case, we have a table of values and want to plot them as explicit bar heights. Because of this, we specify the y aesthetic as the Total column, but we also have to specify stat = &quot;identity&quot; in geom_bar() so it knows to plot them correctly. Often you will have datasets where each row is one observation and you want to group them into bars. In that case, the y aesthetic and stat = &quot;identity&quot; do not have to be specified. 8.4 Theory For more info about plotting categorical data, check out Chapter 4 of the textbook. 8.5 When to use Bar Charts are best for categorical data. Often you will have a collection of factors that you want to split into different groups. 8.6 Considerations 8.6.1 Not for continuous data If you are finding that your bar graphs aren’t looking right, make sure your data is categorical and not continuous. If you want to plot continuous data using bars, that is what histograms are for! 8.7 Modifications These modifications assume you are using ggplot2. 8.7.1 Flip Bars To flip the orientation, just tack on coord_flip(): ggplot(colors_female_hair, aes(x = Hair, y = Total)) + geom_bar(stat = &quot;identity&quot;) + ggtitle(&quot;Bar Graph Using ggplot2&quot;) + coord_flip() 8.7.2 Reorder the bars With both base R and ggplot2 bars are drawn in alphabetical order for character data and in the order of factor levels for factor data. However, since the default order of levels for factor data is alphabetical, the bars will be alphabetical in both cases. Please see this tutorial for a detailed explanation on how bars should be ordered in a bar chart, and how the forcats package can help you accomplish the reordering. 8.7.3 Facet Wrap You can split the graph into small multiples using facet_wrap() (don’t forget the tilde, ~): ggplot(colors, aes(x = Sex, y = Freq)) + geom_bar(stat = &quot;identity&quot;) + facet_wrap(~Hair) 8.8 External resources Cookbook for R: Discussion on reordering the levels of a factor. DataCamp Exercise: Simple exercise on making bar graphs with ggplot2. ggplot2 cheatsheet: Always good to have close by. "],
["box.html", "9 Chart: Boxplot 9.1 Overview 9.2 tl;dr 9.3 Simple examples 9.4 Theory 9.5 When to use 9.6 Considerations 9.7 External resources", " 9 Chart: Boxplot 9.1 Overview This section covers how to make boxplots. 9.2 tl;dr I want a nice example and I want it NOW! Here’s a look at the weights of newborn chicks split by the feed supplement they received: And here’s the code: library(datasets) # data library(ggplot2) # plotting # reorder supplements supps &lt;- c(&quot;horsebean&quot;, &quot;linseed&quot;, &quot;soybean&quot;, &quot;meatmeal&quot;, &quot;sunflower&quot;, &quot;casein&quot;) # boxplot by feed supplement with jitter layer ggplot(chickwts, aes(x = factor(feed, levels = supps), y = weight)) + # plotting geom_boxplot(fill = &quot;#cc9a38&quot;, color = &quot;#473e2c&quot;) + geom_jitter(alpha = 0.2, width = 0.1, color = &quot;#926d25&quot;) + # formatting ggtitle(&quot;Casein Makes You Fat?!&quot;, subtitle = &quot;Boxplots of Chick Weights by Feed Supplement&quot;) + labs(x = &quot;Feed Supplement&quot;, y = &quot;Chick Weight (g)&quot;, caption = &quot;Source: datasets::chickwts&quot;) + theme(plot.title = element_text(face = &quot;bold&quot;)) + theme(plot.subtitle = element_text(face = &quot;bold&quot;, color = &quot;grey35&quot;)) + theme(plot.caption = element_text(color = &quot;grey68&quot;)) For more info on this dataset, type ?datasets::chickwts into the console. 9.3 Simple examples Okay…much simpler please. Let’s use the airquality dataset from the datasets package: library(datasets) head(airquality, n = 5) ## Ozone Solar.R Wind Temp Month Day ## 1 41 190 7.4 67 5 1 ## 2 36 118 8.0 72 5 2 ## 3 12 149 12.6 74 5 3 ## 4 18 313 11.5 62 5 4 ## 5 NA NA 14.3 56 5 5 9.3.1 Boxplot using base R # plot data boxplot(airquality, col = &#39;lightBlue&#39;, main = &quot;Base R Boxplots of airquality&quot;) Boxplots with Base R are super easy. Like histograms, boxplots only need the data. In this case, we passed a dataframe with six variables, so it made separate boxplots for each variable. You may not want to create boxplots for every variable, in which case you could specify the variables individually or use filter from the dplyr package. 9.3.2 Boxplot using ggplot2 # import ggplot library(ggplot2) # plot data g1 &lt;- ggplot(stack(airquality), aes(x = ind, y = values)) + geom_boxplot(fill = &quot;lightBlue&quot;) + # extra formatting labs(x = &quot;&quot;) + ggtitle(&quot;ggplot2 Boxplots of airquality&quot;) g1 ## Warning: Removed 44 rows containing non-finite values (stat_boxplot). ggplot2 requires data to be mapped to the x and y aesthetics. Here we use the stack function to combine each column of the airquality dataframe. Reading the documentation for the stack function (?utils::stack), we see the new stacked dataframe has two columns: values and ind, which we use to create the boxplots. Notice: ggplot2 warns us that it is ignoring “non-finite values”, which are the NA’s in the dataset. 9.4 Theory Here’s a quote by Hadley Wickham that sums up boxplots nicely: The boxplot is a compact distributional summary, displaying less detail than a histogram or kernel density, but also taking up less space. Boxplots use robust summary statistics that are always located at actual data points, are quickly computable (originally by hand), and have no tuning parameters. They are particularly useful for comparing distributions across groups. - Hadley Wickham Another important use of the boxplot is in showing outliers. A boxplot shows how much of an outlier a data point is with quartiles and fences. Use the boxplot when you have data with outliers so that they can be exposed. What it lacks in specificity it makes up with its ability to clearly summarize large data sets. For more info about boxplots and continuous variables, check out Chapter 3 of the textbook. 9.5 When to use Boxplots should be used to display continuous variables. They are particularly useful for identifying outliers and comparing different groups. Aside: Boxplots may even help you convince someone you are their outlier (If you like it when people over-explain jokes, here is why that comic is funny.). 9.6 Considerations 9.6.1 Flipping orientation Often you want boxplots to be horizontal. Super easy to do: just tack on coord_flip(): # g1 plot from above (5.3.2) g1 + coord_flip() ## Warning: Removed 44 rows containing non-finite values (stat_boxplot). 9.6.2 NOT for categorical data Boxplots are great, but they do NOT work with categorical data. Make sure your variable is continuous before using boxplots. Here’s an example of what not to do: library(likert) # data library(dplyr) # data manipulation # load/format data data(pisaitems) pisa &lt;- pisaitems[1:100, 2:7] %&gt;% dplyr::mutate_all(as.integer) %&gt;% dplyr::filter(complete.cases(.)) # create theme theme &lt;- theme(plot.title = element_text(face = &quot;bold&quot;)) + theme(plot.subtitle = element_text(face = &quot;bold&quot;, color = &quot;grey35&quot;)) + theme(plot.caption = element_text(color = &quot;grey68&quot;)) # create plot plot &lt;- ggplot(stack(pisa), aes(x = ind, y = values)) + geom_boxplot(fill = &quot;#9B3535&quot;) + ggtitle(&quot;Don&#39;t Plot Boxplots of Categorical Variables Like This&quot;, subtitle = &quot;...seriously don&#39;t. Here, I&#39;ll make it red so it looks scary:&quot;) + labs(x = &quot;Assessment Code&quot;, y = &quot;Values&quot;, caption = &quot;Source: likert::pisaitems&quot;) # bad boxplot plot + theme 9.7 External resources Tukey, John W. 1977. Exploratory Data Analysis. Addison-Wesley. (Chapter 2): the primary source in which boxplots are first presented. DataCamp: Quick Exercise on Boxplots: a simple example of making boxplots from a dataset. Article on boxplots with ggplot2: An excellent collection of code examples on how to make boxplots with ggplot2. Covers layering, working with legends, faceting, formatting, and more. If you want a boxplot to look a certain way, this article will help. Boxplots with plotly package: boxplot examples using the plotly package. These allow for a little interactivity on hover, which might better explain the underlying statistics of your plot. ggplot2 Boxplot: Quick Start Guide: Article from STHDA on making boxplots using ggplot2. Excellent starting point for getting immediate results and custom formatting. ggplot2 cheatsheet: Always good to have close by. Hadley Wickhan and Lisa Stryjewski on boxplots: good for understanding basics of more complex boxplots and some of the history behind them. "],
["heatmap.html", "10 Chart: Heatmap 10.1 Overview 10.2 tl;dr 10.3 Simple examples 10.4 Theory 10.5 External resources", " 10 Chart: Heatmap 10.1 Overview This section covers how to make heatmaps. 10.2 tl;dr Enough with these simple examples! I want a complicated one! Here’s a heatmap of occupational categories of sons and fathers in the US, UK, and Japan: And here’s the code: library(vcdExtra) # dataset library(dplyr) # manipulation library(ggplot2) # plotting library(viridis) # color palette # format data orderedclasses &lt;- c(&quot;Farm&quot;, &quot;LoM&quot;, &quot;UpM&quot;, &quot;LoNM&quot;, &quot;UpNM&quot;) mydata &lt;- Yamaguchi87 mydata$Son &lt;- factor(mydata$Son, levels = orderedclasses) mydata$Father &lt;- factor(mydata$Father, levels = orderedclasses) japan &lt;- mydata %&gt;% filter(Country == &quot;Japan&quot;) uk &lt;- mydata %&gt;% filter(Country == &quot;UK&quot;) us &lt;- mydata %&gt;% filter(Country == &quot;US&quot;) # convert to % of country and class total mydata_new &lt;- mydata %&gt;% group_by(Country, Father) %&gt;% mutate(Total = sum(Freq)) %&gt;% ungroup() # make custom theme theme_heat &lt;- theme_classic() + theme(axis.line = element_blank(), axis.ticks = element_blank()) # basic plot plot &lt;- ggplot(mydata_new, aes(x = Father, y = Son)) + geom_tile(aes(fill = Freq/Total), color = &quot;white&quot;) + coord_fixed() + facet_wrap(~Country) + theme_heat # plot with text overlay and viridis color palette plot + geom_text(aes(label = round(Freq/Total, 1)), color = &quot;white&quot;) + scale_fill_viridis() + # formatting ggtitle(&quot;Like Father, Like Son&quot;, subtitle = &quot;Heatmaps of occupational categories for fathers and sons, by country&quot;) + labs(caption = &quot;Source: vcdExtra::Yamaguchi87&quot;) + theme(plot.title = element_text(face = &quot;bold&quot;)) + theme(plot.subtitle = element_text(face = &quot;bold&quot;, color = &quot;grey35&quot;)) + theme(plot.caption = element_text(color = &quot;grey68&quot;)) For more info on this dataset, type ?vcdExtra::Yamaguchi87 into the console. 10.3 Simple examples Too complicated! Simplify, man! 10.3.1 Heatmap of two-dimensional bin counts For this heatmap, we will use the SpeedSki dataset. Only two variables, x and y are needed for two-dimensional bin count heatmaps. The third variable–i.e., the color–represents the bin count of points in the region it covers. Think of it as a two-dimensional histogram. To create a heatmap, simply substitute geom_point() with geom_bin2d(): library(ggplot2) # plotting library(GDAdata) # data (SpeedSki) ggplot(SpeedSki, aes(Year, Speed)) + geom_bin2d() 10.3.2 Heat map of dataframe To get a visual sense of the dataframe, you can use a heatmap. You can also look into scaling the columns to get a sense of your data on a common scale. In this example, we use geom_tile to graph all cells in the dataframe and color them by their value: library(pgmm) # data library(tidyverse) # processing/graphing library(viridis) # color palette data(wine) # convert to column, value wine_new &lt;- wine %&gt;% rownames_to_column() %&gt;% gather(colname, value, -rowname) ggplot(wine_new, aes(x = rowname, y = colname, fill = value)) + geom_tile() + scale_fill_viridis() + ggtitle(&quot;Italian Wine Dataframe&quot;) # only difference from above is scaling wine_scaled &lt;- data.frame(scale(wine)) %&gt;% rownames_to_column() %&gt;% gather(colname, value, -rowname) ggplot(wine_scaled, aes(x = rowname, y = colname, fill = value)) + geom_tile() + scale_fill_viridis() + ggtitle(&quot;Italian Wine Dataframe, Scaled&quot;) 10.3.3 Modifications You can change the color palette by specifying it explicitly in your chain of ggplot function calls. The bin width can be added inside the geom_bin2d() function call: library(viridis) # viridis color palette # create plot g1 &lt;- ggplot(SpeedSki, aes(Year, Speed)) + scale_fill_viridis() # modify color # show plot g1 + geom_bin2d(binwidth = c(5, 5)) # modify bin width Here are some other examples: # larger bin width g1 + geom_bin2d(binwidth = c(10, 10)) # hexagonal bins g1 + geom_hex(binwidth = c(5, 5)) # hexagonal bins + scatterplot layer g1 + geom_hex(binwidth = c(5, 5), alpha = .4) + geom_point(size = 2, alpha = 0.8) # hexagonal bins with custom color gradient/bin count ggplot(SpeedSki, aes(Year, Speed)) + scale_fill_gradient(low = &quot;#cccccc&quot;, high = &quot;#09005F&quot;) + # color geom_hex(bins = 10) # number of bins horizontally/vertically 10.4 Theory Heat maps are like a combination of scatterplots and histograms: they allow you to compare different parameters while also seeing their relative distributions. While heatmaps are visually striking, there are often better choices to get your point across. For more info, checkout this DataCamp section on heatmaps and alternatives. 10.5 External resources R Graph Gallery: Heatmaps: Has examples of creating heatmaps with the heatmap() function. How to make a simple heatmap in ggplot2: Create a heatmap with geom_tile(). "],
["histo.html", "11 Chart: Histogram 11.1 Overview 11.2 tl;dr 11.3 Simple examples 11.4 Theory 11.5 Types of histrograms 11.6 Parameters 11.7 External resources", " 11 Chart: Histogram 11.1 Overview This section covers how to make histograms. 11.2 tl;dr Gimme a full-fledged example! Here’s an application of histograms that looks at how the beaks of Galapagos finches changed due to external factors: And here’s the code: library(Sleuth3) # data library(ggplot2) # plotting # load data finches &lt;- Sleuth3::case0201 # finch histograms by year with overlayed density curves ggplot(finches, aes(x = Depth, y = ..density..)) + # plotting geom_histogram(bins = 20, colour = &quot;#80593D&quot;, fill = &quot;#9FC29F&quot;, boundary = 0) + geom_density(color = &quot;#3D6480&quot;) + facet_wrap(~Year) + # formatting ggtitle(&quot;Severe Drought Led to Finches with Bigger Chompers&quot;, subtitle = &quot;Beak Depth Density of Galapagos Finches by Year&quot;) + labs(x = &quot;Beak Depth (mm)&quot;, caption = &quot;Source: Sleuth3::case0201&quot;) + theme(plot.title = element_text(face = &quot;bold&quot;)) + theme(plot.subtitle = element_text(face = &quot;bold&quot;, color = &quot;grey35&quot;)) + theme(plot.caption = element_text(color = &quot;grey68&quot;)) For more info on this dataset, type ?Sleuth3::case0201 into the console. 11.3 Simple examples Whoa whoa whoa! Much simpler please! Let’s use a very simple dataset: # store data x &lt;- c(50, 51, 53, 55, 56, 60, 65, 65, 68) 11.3.1 Histogram using base R # plot data hist(x, col = &quot;lightblue&quot;, main = &quot;Base R Histogram of x&quot;) For the Base R histogram, it’s advantages are in it’s ease to setup. In truth, all you need to plot the data x in question is hist(x), but we included a little color and a title to make it more presentable. Full documentation on hist() can be found here 11.3.2 Histogram using ggplot2 # import ggplot library(ggplot2) # must store data as dataframe df &lt;- data.frame(x) # plot data ggplot(df, aes(x)) + geom_histogram(color = &quot;grey&quot;, fill = &quot;lightBlue&quot;, binwidth = 5, center = 52.5) + ggtitle(&quot;ggplot2 histogram of x&quot;) The ggplot version is a little more complicated on the surface, but you get more power and control as a result. Note: as shown above, ggplot expects a dataframe, so if you are getting an error where “R doesn’t know what to do” like this: ggplot dataframe error make sure you are using a dataframe. 11.4 Theory Generally speaking, the histogram is one of many options for displaying continuous data. The histogram is clear and quick to make. Histograms are relatively self-explanatory: they show your data’s empirical distribution within a set of intervals. Histograms can be employed on raw data to quickly show the distribution without much manipulation. Use a histogram to get a basic sense of the distribution with minimal processing necessary. For more info about histograms and continuous variables, check out Chapter 3 of the textbook. 11.5 Types of histrograms Use a histogram to show the distribution of one continuous variable. The y-scale can be represented in a variety of ways to express different results: 11.5.1 Frequency or count y = number of values that fall in each bin 11.5.2 Relative frequency historgram y = number of values that fall in each bin / total number of values 11.5.3 Cumulative frequency histogram y = total number of values &lt;= (or &lt;) right boundary of bin 11.5.4 Density y = relative frequency / binwidth 11.6 Parameters 11.6.1 Bin boundaries Be mindful of the boundaries of the bins and whether a point will fall into the left or right bin if it is on a boundary. # format layout op &lt;- par(mfrow = c(1, 2), las = 1) # right closed hist(x, col = &quot;lightblue&quot;, ylim = c(0, 4), xlab = &quot;right closed ex. (55, 60]&quot;, font.lab = 2) # right open hist(x, col = &quot;lightblue&quot;, right = FALSE, ylim = c(0, 4), xlab = &quot;right open ex. [55, 60)&quot;, font.lab = 2) 11.6.2 Bin number The default bin number of 30 in ggplot2 is not always ideal, so consider altering it if things are looking strange. You can specify the width explicitly with binwidth or provide the desired number of bins with bins. # default...note the pop-up about default bin number ggplot(finches, aes(x = Depth)) + geom_histogram() + ggtitle(&quot;Default with pop-up about bin number&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Here are examples of changing the bins using the two ways described above: # using binwidth p1 &lt;- ggplot(finches, aes(x = Depth)) + geom_histogram(binwidth = 0.5, boundary = 6) + ggtitle(&quot;Changed binwidth value&quot;) # using bins p2 &lt;- ggplot(finches, aes(x = Depth)) + geom_histogram(bins = 48, boundary = 6) + ggtitle(&quot;Changed bins value&quot;) # format plot layout library(gridExtra) grid.arrange(p1, p2, ncol = 2) 11.6.3 Bin alignment Make sure the axes reflect the true boundaries of the histogram. You can use boundary to specify the endpoint of any bin or center to specify the center of any bin. ggplot2 will be able to calculate where to place the rest of the bins (Also, notice that when the boundary was changed, the number of bins got smaller by one. This is because by default the bins are centered and go over/under the range of the data.) df &lt;- data.frame(x) # default alignment ggplot(df, aes(x)) + geom_histogram(binwidth = 5, fill = &quot;lightBlue&quot;, col = &quot;black&quot;) + ggtitle(&quot;Default Bin Alignment&quot;) # specify alignment with boundary p3 &lt;- ggplot(df, aes(x)) + geom_histogram(binwidth = 5, boundary = 60, fill = &quot;lightBlue&quot;, col = &quot;black&quot;) + ggtitle(&quot;Bin Alignment Using boundary&quot;) # specify alignment with center p4 &lt;- ggplot(df, aes(x)) + geom_histogram(binwidth = 5, center = 67.5, fill = &quot;lightBlue&quot;, col = &quot;black&quot;) + ggtitle(&quot;Bin Alignment Using center&quot;) # format layout library(gridExtra) grid.arrange(p3, p4, ncol = 2) Note: Don’t use both boundary and center for bin alignment. Just pick one. 11.7 External resources DataCamp ggplot2 Histograms Exercise: Simple interactive example of histograms with ggplot2 DataCamp Histogram with Basic R: “Tutorial for new R users whom need an accessible and easy-to-understand resource on how to create their own histogram with basic R.” ’Nuff said. DataCamp Histogram with ggplot2: Great article on making histograms with ggplot2. hist documentation: base R histogram documentation page. ggplot2 cheatsheet: Always good to have close by. "],
["mosaic.html", "12 Chart: Mosaic 12.1 Overview 12.2 tl;dr 12.3 Simple Example Walkthrough 12.4 Mosaic using base R 12.5 Mosaic using ggplot 12.6 Theory 12.7 When to use 12.8 Considerations 12.9 External resources", " 12 Chart: Mosaic This chapter originated as a community contribution created by harin This page is a work in progress. We appreciate any input you may have. If you would like to help improve this page, consider contributing to our repo. 12.1 Overview This section covers how to make Mosaic plots 12.2 tl;dr library(vcd) mosaic(Favorite ~ Age + Music, labeling = labeling_border( abbreviate_labs = c(3, 10, 6), rot_labels=c(0,0,-45) ), direction=c(&#39;v&#39;,&#39;v&#39;,&#39;h&#39;), # Age = Vertical, Music = Vertical, Favoriate = Horizonal (a.k.a DoubleDecker) gp = gpar(fill=c(&#39;lightblue&#39;, &#39;gray&#39;)), df) 12.3 Simple Example Walkthrough 12.3.1 Order of splits It is best to draw mosaic plots incrementally: start with splitting on one variable and then add additional variables one at a time. The full mosaic plot will have one split per variable. Important: if your data is in a data frame (see above), the count column must be called Freq. Split on Age only: library(vcd) mosaic(~Age, df) Split on Age, then Music: mosaic(Music ~ Age, df) Note that the first split is between “young” and “old”, while the second set of splits divides each age group into “classical” and “rock”. Split on Age, then Music, then Favorite: mosaic(Favorite ~ Age + Music, df) 12.3.2 Direction of splits Note that in the previous example, the direction of the splits is as follows: Age – horizontal split Music – vertical split Favorite – horizontal split This is the default direction pattern: alternating directions beginning with horizontal. Therefore we get the same plot with the following: mosaic(Favorite ~ Age + Music, direction = c(&quot;h&quot;, &quot;v&quot;, &quot;h&quot;), df) The directions can be altered as desired. For example, to create a doubledecker plot, make all splits vertical except the last one: mosaic(Favorite ~ Age + Music, direction = c(&quot;v&quot;, &quot;v&quot;, &quot;h&quot;), df) Note that the direction vector is in order of splits (Age, Music, Favorite), not in the order in which the variables appear in the formula, where the last variable to be split is listed first, before the “~”. 12.3.3 Options 12.3.3.1 Fill color: library(grid) # needed for gpar mosaic(Favorite ~ Age + Music, gp = gpar(fill = c(&quot;lightblue&quot;, &quot;blue&quot;)), df) 12.3.3.2 Rotate labels: mosaic(Favorite ~ Age + Music, labeling = labeling_border(rot_labels = c(45, -45, 0, 0)), df) The rot_labels = vector sets the rotation in degrees on the four sides of the plot in this order: top, right, bottom, left. (Different from the typical base graphics order!) The default is rot_labels = c(0, 90, 0, 90). 12.3.3.3 Abbreviate labels: mosaic(Favorite ~ Age + Music, labeling = labeling_border(abbreviate_labs = c(3, 1, 6)), df) Labels are abbreviated in the order of the splits (as for direction =). The abbreviation algorithm appears to return the specified number of characters after vowels are eliminated (if necessary). For more formatting options, see &gt;?vcd::labeling_border. 12.3.3.4 Remove spacing between cells mosaic(Favorite ~ Age + Music, spacing = spacing_equal(sp = unit(0, &quot;lines&quot;)), df) For more details, see &gt;?vcd::spacings 12.3.3.5 Change border color (must also set fill(?)) mosaic(Favorite ~ Age + Music, gp = gpar(fill = c(&quot;lightblue&quot;, &quot;blue&quot;), col = &quot;white&quot;), spacing = spacing_equal(sp = unit(0, &quot;lines&quot;)), df) 12.4 Mosaic using base R library(vcdExtra) mosaicplot(xtabs(count ~ lake + sex, data=Alligator), main=&quot;&quot;) mosaicplot(xtabs(Freq ~ Favorite + Age + Music, data=df), main=&quot;&quot;, dir=c(&#39;h&#39;, &#39;v&#39;, &#39;v&#39;)) 12.4.1 Mosaic using vcd::doubledecker data(Arthritis) vcd::doubledecker(Improved ~ Treatment + Sex, data=Arthritis) vcd::doubledecker(Music ~ Favorite + Age, xtabs(Freq ~ Age + Music + Favorite, df)) 12.5 Mosaic using ggplot For a comprehensive overview of mosaic plot in ggplot check out the link below. https://cran.r-project.org/web/packages/ggmosaic/vignettes/ggmosaic.html library(ggmosaic) # equivalent to doing Favorite ~ Age + Music in vcd::mosaic with doubledecker style cut ggplot(df) + geom_mosaic( aes(x=product(Favorite, Age, Music), # cut from right to left weight=Freq, fill=Favorite ), divider=c(&quot;vspine&quot; , &quot;hspine&quot;, &quot;hspine&quot;) # equivalent to divider=ddecker() ) 12.6 Theory 12.7 When to use When you want to see the relationships in Multivariate Categorical Data 12.8 Considerations 12.8.1 Labels Legibility of the labels is problematic in mosaic plot especially when there are a lot of dimensions. This can be alleviated by - Abbreviate names - Rotating the labels 12.8.2 Aspect Ratio lengths are easier to judge than area, so try to use rectangles with same width or height Taller thinner rectangles are better (we are better at distinguishing length than area) 12.8.3 Gaps between rectangles No gap = most efficient However, a gap can help improve legibility, so try out different combinations Can have a gap at splits Can Vary gap size down the hierarchy 12.8.4 Color good for rates in the subgroup displaying residual emphasizing particular subgroup 12.9 External resources Chapter 7 of Graphical data analysis with R by Anthony Unwin Link: A comprehensive overview of mosaic plot in ggplot check out the link below. "],
["parallelcoordinates.html", "13 Chart: Parallel Coordinate Plots 13.1 Overview 13.2 tl;dr 13.3 Simple examples 13.4 Theory 13.5 When to use 13.6 Considerations 13.7 Modifications 13.8 External Resources", " 13 Chart: Parallel Coordinate Plots This chapter originated as a community contribution created by aashnakanuga This page is a work in progress. We appreciate any input you may have. If you would like to help improve this page, consider contributing to our repo. 13.1 Overview This section covers how to create parallel coordinate plots 13.2 tl;dr I want a Fancy Example! Not tomorrow, not after breakfast, NOW! Here’s a look at the effect of different attributes on each Fair cut diamond from the “diamonds” dataset: And here’s the code: library(GGally) library(dplyr) #subset the data to get the first thousand cases diamonds_subset &lt;- subset(diamonds[1:1000,]) #rename the variables to understand what they signify names(diamonds_subset)&lt;-c(&quot;carat&quot;,&quot;cut&quot;,&quot;color&quot;,&quot;clarity&quot;,&quot;depth_percentage&quot;,&quot;table&quot;,&quot;price&quot;,&quot;length&quot;,&quot;width&quot;,&quot;depth&quot;) #Create a new column to highlight the fair cut diamonds ds_fair&lt;-within(diamonds_subset, diamond_cut&lt;-if_else(cut==&quot;Fair&quot;, &quot;Fair&quot;, &quot;Other&quot;)) #Create the graph ggparcoord(ds_fair[order(ds_fair$diamond_cut, decreasing=TRUE),], columns=c(1,5,7:10), groupColumn = &quot;diamond_cut&quot;, alphaLines = 0.8, order=c(5,1,8,9,10,7), title = &quot;Parallel Coordinate Plot showing trends for Fair cut diamonds&quot;, scale = &quot;uniminmax&quot;) + scale_color_manual(values=c(&quot;maroon&quot;,&quot;gray&quot;)) For more information about the dataset, type ?diamonds into the console. 13.3 Simple examples Woah woah woah! Too complicated! Much simpler, please. Let us use the popular “iris” dataset for this example: library(datasets) library(GGally) ggparcoord(iris, columns=1:4, title = &quot;Parallel coordinate plot for Iris flowers&quot;) For more information about the dataset, type ?iris into the console. 13.4 Theory For more info about parallel coordinate plots and multivariate continuous data, check out Chapter 6 of the textbook. 13.5 When to use Generally, parallel coordinate plots are used to infer relationships between multiple continuous variables - we mostly use them to detect a general trend that our data follows, and also the specific cases that are outliers. Please keep in mind that parallel coordinate plots are not the ideal graph to use when there are just categorical variables involved. We can include a few categorical variables in our axes or for the sake of clustering, but using a lot of categorical variables results in overlapping profiles, which makes it difficult to interpret. We can also use parallel coordinate plots to identify trends in specific clusters - just highlight each cluster in a different color using the groupColumn attribute of ggparcoord() to specify your column, and you are good to go! Sometimes, parallel coordinate plots are very helpful in graphing time series data - where we have information stored at regular time intervals. Each vertical axis will now become a time point and we need to pass that column in ggparcoord’s “column” attribute. 13.6 Considerations 13.6.1 When do I use clustering? Generally, you use clustering when you want to observe a pattern in a set of cases with some specific properties. This may include divvying up all variables into clusters based on their value for a specific categorical variable. But you can even use a continuous variable; for example, dividing all cases into two sections based on some continuous variable height: those who have a height greater than 150cm and those who do not. Let us look at an example using our iris dataset, clustering on the “Species” column: library(GGally) #highlight the clusters based on the Species column graph&lt;-ggparcoord(iris, columns=1:4, groupColumn = 5, title = &quot;Plot for Iris data, where each color represents a specific Species&quot;) graph 13.6.2 Deciding the value of alpha In practice, parallel coordinate plots are not going to be used for very small datasets. Your data will likely have thousands and thousands of cases, and sometimes it can get very difficult to observe anything when so many of your cases will overlap. So we set the aplhaLines parameter to a value between zero and one, and it reduces the opacity of all lines so that you can get a clearer view of what is going on if you have too many overlapping cases. Again we use our iris data, but reduce alpha to 0.5. Observe how much easier it is now to trace the course of every case: library(ggplot2) library(GGally) #set the value of alpha to 0.5 ggparcoord(iris, columns=1:4, groupColumn = 5, alphaLines = 0.5, title = &quot;Iris data with a lower alpha value&quot;) 13.6.3 Scales When we use ggparcoord(), we have an option to set the scale attribute, which will scale all variables so we can compare their values. The different types of scales are as follows: std: default value, where it subtracts mean and divides by SD robust: subtract median and divide by median absolute deviation uniminmax: scale all values so that the minimum is at 0 and maximum at 1 globalminmax: no scaling, original values taken center: centers each variable according to the value given in scaleSummary centerObs: centers each variable according to the value of the observation given in centerObsID Let us create a sample dataset and see how values on the y-axis change for different scales: library(ggplot2) library(GGally) library(gridExtra) #creating a sample dataset df1&lt;-data.frame(col1=c(11,4,7,4,3,8,5,7,9), col2=c(105,94,138,194,173,129,156,163,148)) #pay attention to the different values on the y-axis g1&lt;-ggparcoord(df1, columns=1:2, scale = &quot;std&quot;, title = &quot;Standard Scale&quot;) g2&lt;-ggparcoord(df1, columns=1:2, scale = &quot;robust&quot;, title = &quot;Robust Scale&quot;) g3&lt;-ggparcoord(df1, columns=1:2, scale = &quot;uniminmax&quot;, title = &quot;Uniminmax Scale&quot;) g4&lt;-ggparcoord(df1, columns=1:2, scale = &quot;globalminmax&quot;, title = &quot;Globalminmax Scale&quot;) g5&lt;-ggparcoord(df1, columns=1:2, scale = &quot;center&quot;, scaleSummary = &quot;mean&quot;, title = &quot;Center Scale&quot;) g6&lt;-ggparcoord(df1, columns=1:2, scale = &quot;centerObs&quot;, centerObsID = 4, title = &quot;CenterObs Scale&quot;) grid.arrange(g1, g2, g3, g4, g5, g6, nrow=2) 13.6.4 Order of the variables Deciding the order of the variables on the y-axis depends on your application. It can be specified using the order parameter. The different types of order are as follows: default: the order in which we add our variables to the column attribute given vector: providing a vector of the order we need (used most frequently) anyClass: order based on the separation of a variable from the rest (F-statistic - each variable v/s the rest) allClass: order based on the variation between classes (F-statistic - group column v/s the rest) skewness: order from most to least skewed Outlying: order based on the Outlying measure 13.7 Modifications 13.7.1 Flipping the coordinates A good idea if we have too many variables and their names are overlapping on the x-axis: library(ggplot2) library(GGally) #using the iris dataset graph + coord_flip() 13.7.2 Highlighting trends Let us see what trend the versicolor Species of the iris dataset follows over the other variables: library(ggplot2) library(GGally) library(dplyr) #get a new column that says &quot;Yes&quot; of the Species is versicolor. ds_versi&lt;-within(iris, versicolor&lt;-if_else(Species==&quot;versicolor&quot;, &quot;Yes&quot;, &quot;No&quot;)) ggparcoord(ds_versi[order(ds_versi$versicolor),], columns = 1:4, groupColumn = &quot;versicolor&quot;, title = &quot;Highlighting trends of Versicolor species&quot;) + scale_color_manual(values=c(&quot;gray&quot;,&quot;maroon&quot;)) 13.7.3 Using splines Generally, we use splines if we have a column where there are a lot of repeating values, which adds a lot of noise. The case lines become more and more curved when we set a higher spline factor, which removes noise and makes for easier observations of trends. It can be set using the splineFactor attribute: library(ggplot2) library(GGally) library(gridExtra) #create a sample dataset df2&lt;-data.frame(col1=c(1:9), col2=c(11,11,14,15,15,15,17,18,18), col3=c(4,4,4,7,7,7,8,9,9), col4=c(3,3,3,4,6,6,6,8,8)) #plot without spline g7&lt;-ggparcoord(df2, columns = 1:4, scale = &quot;globalminmax&quot;, title = &quot;No Spline factor&quot;) #plot with spline g8&lt;-ggparcoord(df2, columns = 1:4, scale = &quot;globalminmax&quot;, splineFactor=10, title = &quot;Spline factor set to 10&quot;) grid.arrange(g7,g8) 13.7.4 Adding boxplots to the graph You can add boxplots to your graph, which can be useful for observing the trend of median values. Generally, they are added to data with a lot of variables - for example, if we plot time series data. 13.8 External Resources Introduction to parallel coordinate plots: An excellent resource giving details of all attributes and possible values. Also has some good examples. How to create interactive parallel coordinate plots: a nice walkthrough on using plotly to create an interactive parallel coordinate plot. Different methods to create parallel coordinate plots: This is specifically when we have categorical variables. "],
["ridgeline.html", "14 Chart: Ridgeline Plots 14.1 Overview 14.2 tl;dr 14.3 Simple examples 14.4 Ridgeline Plots using ggridge 14.5 When to Use 14.6 Considerations 14.7 External Resources", " 14 Chart: Ridgeline Plots This chapter originated as a community contribution created by nehasaraf1994 This page is a work in progress. We appreciate any input you may have. If you would like to help improve this page, consider contributing to our repo. 14.1 Overview This section covers how to make ridgeline plots. 14.2 tl;dr I want a nice example and I want it NOW! Here’s a look at the dose of theophylline administered orally to the subject on which the concentration of theophylline is observed: Here is the code: library(&quot;ggridges&quot;) library(&quot;tidyverse&quot;) Theoph_data &lt;- Theoph ggplot(Theoph_data, aes(x=Dose,y=Subject,fill=Subject))+ geom_density_ridges_gradient(scale = 4, show.legend = FALSE) + theme_ridges() + scale_y_discrete(expand = c(0.01, 0)) + scale_x_continuous(expand = c(0.01, 0)) + labs(x = &quot;Dose of theophylline(mg/kg)&quot;,y = &quot;Subject #&quot;) + ggtitle(&quot;Density estimation of dosage given to various subjects&quot;) + theme(plot.title = element_text(hjust = 0.5)) For more info on this dataset, type ?datasets::Theoph into the console. 14.3 Simple examples Okay…much simpler please. Let’s use the Orange dataset from the datasets package: library(&quot;datasets&quot;) head(Orange, n=5) ## Grouped Data: circumference ~ age | Tree ## Tree age circumference ## 1 1 118 30 ## 2 1 484 58 ## 3 1 664 87 ## 4 1 1004 115 ## 5 1 1231 120 14.4 Ridgeline Plots using ggridge library(&quot;ggridges&quot;) library(&quot;tidyverse&quot;) ggplot(Orange, aes(x=circumference,y=Tree,fill = Tree))+ geom_density_ridges(scale = 2, alpha=0.5) + theme_ridges()+ scale_fill_brewer(palette = 4)+ scale_y_discrete(expand = c(0.8, 0)) + scale_x_continuous(expand = c(0.01, 0))+ labs(x=&quot;Circumference at Breast Height&quot;, y=&quot;Tree with ordering of max diameter&quot;)+ ggtitle(&quot;Density estimation of circumference of different types of Trees&quot;)+ theme(plot.title = element_text(hjust = 0.5)) ggridge uses two main geoms to plot the ridgeline density plots: “geom_density_ridges” and “geom_ridgeline”. They are used to plot the densities of categorical variable factors and see their distribution over a continuous scale. 14.5 When to Use Ridgeline plots can be used when a number of data segments have to be plotted on the same horizontal scale. It is presented with slight overlap. Ridgeline plots are very useful to visualize the distribution of a categorical variable over time or space. A good example using ridgeline plots will be a great example is visualizing the distribution of salary over different departments in a company. 14.6 Considerations The overlapping of the density plot can be controlled by adjusting the value of scale. Scale defines how much the peak of the lower curve touches the curve above. library(&quot;ggridges&quot;) library(&quot;tidyverse&quot;) OrchardSprays_data &lt;- OrchardSprays ggplot(OrchardSprays_data, aes(x=decrease,y=treatment,fill=treatment))+ geom_density_ridges_gradient(scale=3) + theme_ridges()+ scale_y_discrete(expand = c(0.3, 0)) + scale_x_continuous(expand = c(0.01, 0))+ labs(x=&quot;Response in repelling honeybees&quot;,y=&quot;Treatment&quot;)+ ggtitle(&quot;Density estimation of response by honeybees to a treatment for scale=3&quot;)+ theme(plot.title = element_text(hjust = 0.5)) ggplot(OrchardSprays_data, aes(x=decrease,y=treatment,fill=treatment))+ geom_density_ridges_gradient(scale=5) + theme_ridges()+ scale_y_discrete(expand = c(0.3, 0)) + scale_x_continuous(expand = c(0.01, 0))+ labs(x=&quot;Response in repelling honeybees&quot;,y=&quot;Treatment&quot;)+ ggtitle(&quot;Density estimation of response by honeybees to a treatment for scale=5&quot;)+ theme(plot.title = element_text(hjust = 0.5)) Ridgeline plots can also be used to plot histograms on the common horizontal axis rather than density plots. But doing that may not give us any valuable results. library(&quot;ggridges&quot;) library(&quot;tidyverse&quot;) ggplot(InsectSprays, aes(x = count, y = spray, height = ..density.., fill = spray)) + geom_density_ridges(stat = &quot;binline&quot;, bins = 20, scale = 0.7, draw_baseline = FALSE) If the same thing is done in ridgeline plots, it gives better results. library(&quot;ggridges&quot;) library(&quot;tidyverse&quot;) ggplot(InsectSprays, aes(x=count,y=spray,fill=spray))+ geom_density_ridges_gradient() + theme_ridges()+ labs(x=&quot;Count of Insects&quot;,y=&quot;Types of Spray&quot;)+ ggtitle(&quot;The counts of insects treated with different insecticides.&quot;)+ theme(plot.title = element_text(hjust = 0.5)) 14.7 External Resources Introduction to ggridges: An excellent collection of code examples on how to make ridgeline plots with ggplot2. Covers every parameter of ggridges and how to modify them for better visualization. If you want a ridgeline plot to look a certain way, this article will help. Article on ridgeline plots with ggplot2: Few examples using different examples. Great for starting with ridgeline plots. History of Ridgeline plots: To refer to the theory of ridgeline plots. "],
["scatter.html", "15 Chart: Scatterplot 15.1 Overview 15.2 tl;dr 15.3 Simple examples 15.4 Theory 15.5 When to use 15.6 Considerations 15.7 Modifications 15.8 External resources", " 15 Chart: Scatterplot 15.1 Overview This section covers how to make scatterplots 15.2 tl;dr Fancy Example NOW! Gimme Gimme GIMME! Here’s a look at the relationship between brain weight vs. body weight for 62 species of land mammals: And here’s the code: library(MASS) # data library(ggplot2) # plotting # ratio for color choices ratio &lt;- mammals$brain / (mammals$body*1000) ggplot(mammals, aes(x = body, y = brain)) + # plot points, group by color geom_point(aes(fill = ifelse(ratio &gt;= 0.02, &quot;#0000ff&quot;, ifelse(ratio &gt;= 0.01 &amp; ratio &lt; 0.02, &quot;#00ff00&quot;, ifelse(ratio &gt;= 0.005 &amp; ratio &lt; 0.01, &quot;#00ffff&quot;, ifelse(ratio &gt;= 0.001 &amp; ratio &lt; 0.005, &quot;#ffff00&quot;, &quot;#ffffff&quot;))))), col = &quot;#656565&quot;, alpha = 0.5, size = 4, shape = 21) + # add chosen text annotations geom_text(aes(label = ifelse(row.names(mammals) %in% c(&quot;Mouse&quot;, &quot;Human&quot;, &quot;Asian elephant&quot;, &quot;Chimpanzee&quot;, &quot;Owl monkey&quot;, &quot;Ground squirrel&quot;), paste(as.character(row.names(mammals)), &quot;→&quot;, sep = &quot; &quot;),&#39;&#39;)), hjust = 1.12, vjust = 0.3, col = &quot;grey35&quot;) + geom_text(aes(label = ifelse(row.names(mammals) %in% c(&quot;Golden hamster&quot;, &quot;Kangaroo&quot;, &quot;Water opossum&quot;, &quot;Cow&quot;), paste(&quot;←&quot;, as.character(row.names(mammals)), sep = &quot; &quot;),&#39;&#39;)), hjust = -0.12, vjust = 0.35, col = &quot;grey35&quot;) + # customize legend/color palette scale_fill_manual(name = &quot;Brain Weight, as the\\n% of Body Weight&quot;, values = c(&#39;#d7191c&#39;,&#39;#fdae61&#39;,&#39;#ffffbf&#39;,&#39;#abd9e9&#39;,&#39;#2c7bb6&#39;), breaks = c(&quot;#0000ff&quot;, &quot;#00ff00&quot;, &quot;#00ffff&quot;, &quot;#ffff00&quot;, &quot;#ffffff&quot;), labels = c(&quot;Greater than 2%&quot;, &quot;Between 1%-2%&quot;, &quot;Between 0.5%-1%&quot;, &quot;Between 0.1%-0.5%&quot;, &quot;Less than 0.1%&quot;)) + # formatting scale_x_log10(name = &quot;Body Weight&quot;, breaks = c(0.01, 1, 100, 10000), labels = c(&quot;10 g&quot;, &quot;1 kg&quot;, &quot;100 kg&quot;, &quot;10K kg&quot;)) + scale_y_log10(name = &quot;Brain Weight&quot;, breaks = c(1, 10, 100, 1000), labels = c(&quot;1 g&quot;, &quot;10 g&quot;, &quot;100 g&quot;, &quot;1 kg&quot;)) + ggtitle(&quot;An Elephant Never Forgets...How Big A Brain It Has&quot;, subtitle = &quot;Brain and Body Weights of Sixty-Two Species of Land Mammals&quot;) + labs(caption = &quot;Source: MASS::mammals&quot;) + theme(plot.title = element_text(face = &quot;bold&quot;)) + theme(plot.subtitle = element_text(face = &quot;bold&quot;, color = &quot;grey35&quot;)) + theme(plot.caption = element_text(color = &quot;grey68&quot;)) + theme(legend.position = c(0.832, 0.21)) For more info on this dataset, type ?MASS::mammals into the console. And if you are going crazy not knowing what species is in the top right corner, it’s another elephant. Specifically, it’s the African elephant. It also never forgets how big a brain it has. 15.3 Simple examples That was too fancy! Much simpler please! Let’s use the SpeedSki dataset from GDAdata to look at how the speed achieved by the participants related to their birth year: library(GDAdata) head(SpeedSki, n = 7) ## Rank Bib FIS.Code Name Year Nation Speed Sex Event ## 1 1 61 7039 ORIGONE Simone 1979 ITA 211.67 Male Speed One ## 2 2 59 7078 ORIGONE Ivan 1987 ITA 209.70 Male Speed One ## 3 3 66 190130 MONTES Bastien 1985 FRA 209.69 Male Speed One ## 4 4 57 7178 SCHROTTSHAMMER Klaus 1979 AUT 209.67 Male Speed One ## 5 5 69 510089 MAY Philippe 1970 SUI 209.19 Male Speed One ## 6 6 75 7204 BILLY Louis 1993 FRA 208.33 Male Speed One ## 7 7 67 7053 PERSSON Daniel 1975 SWE 208.03 Male Speed One ## no.of.runs ## 1 4 ## 2 4 ## 3 4 ## 4 4 ## 5 4 ## 6 4 ## 7 4 15.3.1 Scatterplot using base R x &lt;- SpeedSki$Year y &lt;- SpeedSki$Speed # plot data plot(x, y, main = &quot;Scatterplot of Speed vs. Birth Year&quot;) Base R scatterplots are easy to make. All you need are the two variables you want to plot. Although scatterplots can be made with categorical data, the variables you are plotting will usually be continuous. 15.3.2 Scatterplot using ggplot2 library(GDAdata) # data library(ggplot2) # plotting # main plot scatter &lt;- ggplot(SpeedSki, aes(Year, Speed)) + geom_point() # show with trimmings scatter + labs(x = &quot;Birth Year&quot;, y = &quot;Speed Achieved (km/hr)&quot;) + ggtitle(&quot;Ninety-One Skiers by Birth Year and Speed Achieved&quot;) ggplot2 makes it very easy to create scatterplots. Using geom_point(), you can easily plot two different aesthetics in one graph. It also is simple to add on extra formatting to make your plots look nice (All that is really necessary is the data, the aesthetics, and the geom). 15.4 Theory Scatterplots are very useful in understanding the correlation (or lack thereof) between variables. For example, in section 13.2 notice the positive relationship between brain and body weight in species of land mammals. The scatterplot gives a good idea of whether that relationship is positive or negative and if there’s a correlation. However, don’t mistake correlation in a scatterplot for causation! Below we show variations on the scatterplot which can be used to enhance interpretability. For more info about adding lines/contours, comparing groups, and plotting continuous variables check out Chapter 5 of the textbook. 15.5 When to use Scatterplots are great for exploring relationships between variables. Basically, if you are interested in how variables relate to each other, the scatterplot is a great place to start. 15.6 Considerations 15.6.1 Overlapping data Data with similar values will overlap in a scatterplot and may lead to problems. Consider exploring alpha blending or jittering as remedies (links from Overlapping Data section of Iris Walkthrough). 15.6.2 Scaling Consider how scaling can modify how your data will be perceived: library(ggplot2) num_points &lt;- 100 wide_x &lt;- c(rnorm(n = 50, mean = 100, sd = 2), rnorm(n = 50, mean = 10, sd = 2)) wide_y &lt;- rnorm(n = num_points, mean = 5, sd = 2) df &lt;- data.frame(wide_x, wide_y) ggplot(df, aes(wide_x, wide_y)) + geom_point() + ggtitle(&quot;Linear X-Axis&quot;) ggplot(df, aes(wide_x, wide_y)) + geom_point() + ggtitle(&quot;Log-10 X-Axis&quot;) + scale_x_log10() 15.7 Modifications 15.7.1 Contour lines Contour lines give a sense of the density of the data at a glance. For these contour maps, we will use the SpeedSki dataset. Contour lines can be added to the plot call using geom_density_2d(): ggplot(SpeedSki, aes(Year, Speed)) + geom_density_2d() Contour lines work best when combined with other layers: ggplot(SpeedSki, aes(Year, Speed)) + geom_point() + geom_density_2d(bins = 5) 15.7.2 Scatterplot matrices If you want to compare multiple parameters to each other, consider using a scatterplot matrix. This will allow you to show many comparisons in a compact and efficient manner. For these scatterplot matrices, we will use the movies dataset from the ggplot2movies package. As a default, the base R plot() function will create a scatterplot matrix when given multiple variables: library(ggplot2movies) # data library(dplyr) # manipulation index &lt;- sample(nrow(movies), 500) #sample data moviedf &lt;- movies[index,] # data frame splomvar &lt;- moviedf %&gt;% dplyr::select(length, budget, votes, rating, year) plot(splomvar) While this is quite useful for personal exploration of a datset, it is not recommended for presentation purposes. Something called the Hermann grid illusion makes this plot very difficult to examine. To remove this problem, consider using the splom() function from the lattice package: library(lattice) #sploms splom(splomvar) 15.8 External resources Quick-R article about scatterplots using Base R. Goes from the simple into the very fancy, with Matrices, High Density, and 3D versions. STHDA Base R: article on scatterplots in Base R. More examples of how to enhance the humble graph. STHDA ggplot2: article on scatterplots in ggplot2. Heavy on the formatting options available and facet warps. Stack Overflow on adding labels to points from geom_point() ggplot2 cheatsheet: Always good to have close by. "],
["qqplot.html", "16 Chart: QQ-Plot 16.1 Introduction 16.2 Interpreting qqplots 16.3 Normal or not (examples using qqnorm) 16.4 Different kinds of qqplots 16.5 qqplot using ggplot 16.6 References", " 16 Chart: QQ-Plot This chapter originated as a community contribution created by hao871563506 This page is a work in progress. We appreciate any input you may have. If you would like to help improve this page, consider contributing to our repo. 16.1 Introduction In statistics, a Q-Q (quantile-quantile) plot is a probability plot, which is a graphical method for comparing two probability distributions by plotting their quantiles against each other. A point (x, y) on the plot corresponds to one of the quantiles of the second distribution (y-coordinate) plotted against the same quantile of the first distribution (x-coordinate). Thus the line is a parametric curve with the parameter which is the number of the interval for the quantile. 16.2 Interpreting qqplots 16.3 Normal or not (examples using qqnorm) 16.3.1 Normal qqplot x &lt;- rnorm(1000, 50, 10) qqnorm(x) qqline(x, col = &quot;red&quot;) The points seem to fall along a straight line. Notice the x-axis plots the theoretical quantiles. Those are the quantiles from the standard Normal distribution with mean 0 and standard deviation 1. 16.3.2 Non-normal qqplot x &lt;- rexp(1000, 5) qqnorm(x) qqline(x, col = &quot;red&quot;) Notice the points form a curve instead of a straight line. Normal Q-Q plots that look like this usually mean your sample data are skewed. 16.4 Different kinds of qqplots The following graph is a conclusion of all the kinds of qqplot: via Stack Exchange Normal qqplot: The normal distribution is symmetric, so it has no skew (the mean is equal to the median). Right skewed qqplot: Right-skew is also known as positive skew. Left skewed qqplot: Left-skew is also known as negative skew. Light tailed qqplot: meaning that compared to the normal distribution there is little more data located at the extremes of the distribution and less data in the center of the distribution. Heavy tailed qqplot: meaning that compared to the normal distribution there is much more data located at the extremes of the distribution and less data in the center of the distribution. Biomodel qqplot: illustrate a bimodal distribution. 16.5 qqplot using ggplot In order to use ggplot2 to plot a qqplot, we must use a dataframe, so here we convert it to one. We can see that using ggplot to plot a qqplot has a similar outcome as using qqnorm library(ggplot2) x &lt;- rnorm(1000, 50, 10) x &lt;- data.frame(x) ggplot(x, aes(sample = x)) + stat_qq() + stat_qq_line() However, when we need to plot different groups, ggplot will be very helpful with its coloring by factor. library(ggplot2) ggplot(mtcars, aes(sample = mpg, colour = factor(cyl))) + stat_qq() + stat_qq_line() 16.6 References Understanding Q-Q Plots: A discussion from the University of Virginia Library on qqplots. How to interpret a QQ plot: Another resource for interpreting qqplots. A QQ Plot Dissection Kit: An excellent walkthrough on qqplots by Sean Kross. Probability plotting methods for the analysis of data: Paper on plotting techniques, which discusses qqplots. (Wilk, M.B.; Gnanadesikan, R. (1968)) QQ-Plot Wiki: Wikipedia entry on qqplots "],
["violin.html", "17 Chart: Violin Plot 17.1 Overview 17.2 Some Examples in R 17.3 Adding Statistics to the Violin Plot 17.4 Description 17.5 When to use 17.6 External Resources", " 17 Chart: Violin Plot This chapter originated as a community contribution created by AshwinJay101 This page is a work in progress. We appreciate any input you may have. If you would like to help improve this page, consider contributing to our repo. 17.1 Overview This section covers how to make violin plots. 17.2 Some Examples in R Let’s use the chickwts dataset from the datasets package to plot a violin plot using ggplot2. Here’s the code for that: # import ggplot and the Datasets Package library(datasets) library(ggplot2) supps &lt;- c(&quot;horsebean&quot;, &quot;linseed&quot;, &quot;soybean&quot;, &quot;meatmeal&quot;, &quot;sunflower&quot;, &quot;casein&quot;) # plot data ggplot(chickwts, aes(x = factor(feed, levels = supps), y = weight)) + # plotting geom_violin(fill = &quot;lightBlue&quot;, color = &quot;#473e2c&quot;) + labs(x = &quot;Feed Supplement&quot;, y = &quot;Chick Weight (g)&quot;) 17.3 Adding Statistics to the Violin Plot 17.3.1 Adding the median and the interquartile range We can add the median and the interquartile range to the violin plot ggplot(chickwts, aes(x = factor(feed, levels = supps), y = weight)) + # plotting geom_violin(fill = &quot;lightBlue&quot;, color = &quot;#473e2c&quot;) + labs(x = &quot;Feed Supplement&quot;, y = &quot;Chick Weight (g)&quot;) + geom_boxplot(width=0.1) To get the result, we just add a boxplot geom. 17.3.2 Displaying data as dots ggplot(chickwts, aes(x = factor(feed, levels = supps), y = weight)) + # plotting geom_violin(fill = &quot;lightBlue&quot;, color = &quot;#473e2c&quot;) + labs(x = &quot;Feed Supplement&quot;, y = &quot;Chick Weight (g)&quot;) + geom_dotplot(binaxis=&#39;y&#39;, dotsize=0.5, stackdir=&#39;center&#39;) 17.4 Description Violin plots are similar to box plots. The advantage they have over box plots is that they allow us to visualize the distribution of the data and the probability density. We can think of violin plots as a combination of boxplots and density plots. This plot type allows us to see whether the data is unimodal, bimodal or multimodal. These simple details will be hidden in the boxplot. The distribution can be seen through the width of the violin plot. 17.5 When to use Violin plots should be used to display continuous variables only. 17.6 External Resources ggplot2 Violin Plot: Excellent resource for showing the various customizations that can be added to the violin plot. "],
["dates.html", "18 Dates in R 18.1 Introduction 18.2 Converting to Date class 18.3 Working with Date Class 18.4 Plotting with a Date class variable", " 18 Dates in R 18.1 Introduction Working with dates and time can be very frustrating. In general, work with the least cumbersome class. That means if your variable is years, store it as an integer; there’s no reason to use a date or date-time class. If your variable does not involve time, use the Date class in R. 18.2 Converting to Date class You can convert character data to Date class with as.Date(): dchar &lt;- &quot;2018-10-12&quot; ddate &lt;- as.Date(dchar) Note that the two appear the same, although the class is different: dchar ## [1] &quot;2018-10-12&quot; ddate ## [1] &quot;2018-10-12&quot; class(dchar) ## [1] &quot;character&quot; class(ddate) ## [1] &quot;Date&quot; If the date is not in YYYY-MM-DD or YYYY/MM/DD form, you will need to specify the format to convert to Date class, using conversion specifications that begin with %, such as: as.Date(&quot;Thursday, January 6, 2005&quot;, format = &quot;%A, %B %d, %Y&quot;) ## [1] &quot;2005-01-06&quot; For a list of the conversion specifications available in R, see ?strptime. The tidyverse lubridate makes it easy to convert dates that are not in standard format with ymd(), ydm(), mdy(), myd(), dmy(), and dym() (among many other useful date-time functions): lubridate::mdy(&quot;April 13, 1907&quot;) ## [1] &quot;1907-04-13&quot; Try as.Date(&quot;April 13, 1907&quot;) and you will see the benefit of using a lubridate function. 18.3 Working with Date Class It is well worth the effort to convert to Date class, because there’s a lot you can do with dates in a Date class that you can’t do if you store the dates as character data. Number of days between dates: as.Date(&quot;2017-11-02&quot;) - as.Date(&quot;2017-01-01&quot;) ## Time difference of 305 days Compare dates: as.Date(&quot;2017-11-12&quot;) &gt; as.Date(&quot;2017-3-3&quot;) ## [1] TRUE Note that Sys.Date() returns today’s date as a Date class: Sys.Date() ## [1] &quot;2019-01-18&quot; class(Sys.Date()) ## [1] &quot;Date&quot; R has functions to pull particular pieces of information from a date: today &lt;- Sys.Date() weekdays(today) ## [1] &quot;Friday&quot; weekdays(today, abbreviate = TRUE) ## [1] &quot;Fri&quot; months(today) ## [1] &quot;January&quot; months(today, abbreviate = TRUE) ## [1] &quot;Jan&quot; quarters(today) ## [1] &quot;Q1&quot; The lubridate package provides additional functions to extract information from a date: today &lt;- Sys.Date() lubridate::year(today) ## [1] 2019 lubridate::yday(today) ## [1] 18 lubridate::month(today) ## [1] 1 lubridate::month(today, label = TRUE) ## [1] Jan ## 12 Levels: Jan &lt; Feb &lt; Mar &lt; Apr &lt; May &lt; Jun &lt; Jul &lt; Aug &lt; Sep &lt; ... &lt; Dec lubridate::mday(today) ## [1] 18 lubridate::week(today) ## [1] 3 lubridate::wday(today) ## [1] 6 18.4 Plotting with a Date class variable Both base R graphics and ggplot2 “know” how to work with a Date class variable, and label the axes properly: 18.4.1 base R df &lt;- read.csv(&quot;data/mortgage.csv&quot;) df$DATE &lt;- as.Date(df$DATE) plot(df$DATE, df$X5.1.ARM, type = &quot;l&quot;) # on the order of years plot(df$DATE[1:30], df$X5.1.ARM[1:30], type = &quot;l&quot;) # switch to months Note the the change in x-axis labels in the second graph. 18.4.2 ggplot2 # readr library(tidyverse) Note that unlike base Rread.csv(), readr::read_csv() automatically reads DATE in as a Date class since it’s in YYYY-MM-DD format: df &lt;- readr::read_csv(&quot;data/mortgage.csv&quot;) ## Parsed with column specification: ## cols( ## DATE = col_date(format = &quot;&quot;), ## `5/1 ARM` = col_double(), ## `15 YR FIXED` = col_double(), ## `30 YR FIXED` = col_double() ## ) g &lt;- ggplot(df, aes(DATE, `30 YR FIXED`)) + geom_line() + theme_grey(14) g ggplot(df %&gt;% filter(DATE &lt; as.Date(&quot;2006-01-01&quot;)), aes(DATE, `30 YR FIXED`)) + geom_line() + theme_grey(14) Again, when the data is filtered, the x-axis labels switch from years to months. 18.4.2.1 Breaks, limits, labels We can control the x-axis breaks, limits, and labels with scale_x_date(): library(lubridate) g + scale_x_date(limits = c(ymd(&quot;2008-01-01&quot;), ymd(&quot;2008-12-31&quot;))) + ggtitle(&quot;limits = c(ymd(\\&quot;2008-01-01\\&quot;), ymd(\\&quot;2008-12-31\\&quot;))&quot;) g + scale_x_date(date_breaks = &quot;4 years&quot;) + ggtitle(&quot;scale_x_date(date_breaks = \\&quot;4 years\\&quot;)&quot;) g + scale_x_date(date_labels = &quot;%Y-%m&quot;) + ggtitle(&quot;scale_x_date(date_labels = \\&quot;%Y-%m\\&quot;)&quot;) (Yes, even in the tidyverse we cannot completely escape the % conversion specification notation. Remember ?strptime for help.) 18.4.2.2 Annotations We can use geom_vline() with annotate() to mark specific events in a time series: ggplot(df, aes(DATE, `30 YR FIXED`)) + geom_line() + geom_vline(xintercept = ymd(&quot;2008-09-29&quot;), color = &quot;blue&quot;) + annotate(&quot;text&quot;, x = ymd(&quot;2008-09-29&quot;), y = 3.75, label = &quot; Market crash\\n 9/29/08&quot;, color = &quot;blue&quot;, hjust = 0) + scale_x_date(limits = c(ymd(&quot;2008-01-01&quot;), ymd(&quot;2009-12-31&quot;)), date_breaks = &quot;1 year&quot;, date_labels = &quot;%Y&quot;) + theme_grey(16) + ggtitle(&quot;`geom_vline()` with `annotate()`&quot;) "],
["import.html", "19 Importing Data 19.1 Overview 19.2 Import built-in dataset 19.3 Import local data 19.4 Import web data 19.5 Import data from database 19.6 More resources", " 19 Importing Data This chapter originated as a community contribution created by ZhangZhida This page is a work in progress. We appreciate any input you may have. If you would like to help improve this page, consider contributing to our repo. 19.1 Overview This section covers how to import data from built-in R sources, local files, web sources and databases. 19.2 Import built-in dataset R comes with quite a lot of built-in datasets, which R users can play around with. You are probably familiar with many of the built-in datasets like iris, mtcars, beavers, dataset, etc. Since datasets are preloaded, we can manipulate them directly. To see a full list of built-in R datasets and their descriptions, please refer to The R Datasets Package. We can also run data() to view the full list. After we find the dataset we want, we can use the function data(package_name) to load the built-in dataset into our workspace. For example, here we want to load the iris dataset: # load data data(&quot;iris&quot;) head(iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa Besides the datasets above, there are other datasets within specific R packages. To see a full list of data in various packages, run ??datasets in the RStudio console. For exmaple, say we want to load the diamonds dataset from the ggplot2 package. To load the diamonds data, first we need to load the ggplot2 library: library(ggplot2) data(&quot;diamonds&quot;) head(diamonds) ## # A tibble: 6 x 10 ## carat cut color clarity depth table price x y z ## &lt;dbl&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.23 Ideal E SI2 61.5 55 326 3.95 3.98 2.43 ## 2 0.21 Premium E SI1 59.8 61 326 3.89 3.84 2.31 ## 3 0.23 Good E VS1 56.9 65 327 4.05 4.07 2.31 ## 4 0.290 Premium I VS2 62.4 58 334 4.2 4.23 2.63 ## 5 0.31 Good J SI2 63.3 58 335 4.34 4.35 2.75 ## 6 0.24 Very Good J VVS2 62.8 57 336 3.94 3.96 2.48 19.3 Import local data 19.3.1 Import text file The function read.table() is the most general function for reading text files. To use this function, we need to specify how we read the file. In other words, we need to specify some basic parameters like sep, header, etc. sep represents the separator, and header is set to TRUE if we want to read the first line as the header information. Other parameters are also useful in different cases. For example, na.strings indicates strings should be regarded as NA values. df &lt;- read.table(&quot;data/MusicIcecream.csv&quot;, sep=&quot;,&quot;, header=TRUE) head(df) ## Age Favorite Music Freq ## 1 old bubble gum classical 1 ## 2 old bubble gum rock 1 ## 3 old coffee classical 3 ## 4 old coffee rock 1 ## 5 young bubble gum classical 2 ## 6 young bubble gum rock 5 19.3.2 Import CSV file A Comma-Separated Values file (CSV) is a delimited text file that uses a comma to separate values. We can easily read a CSV file with built-in R functions. The read.csv() function provides two useful parameters. One is header, which can be set to FALSE if there is no header. The other is sep, which specifies the separator. For example, we can specify the separator to be sep=&quot;\\t if the CSV file value is seperated by the tab character. The default value of header and sep are TRUE and &quot;,&quot;, respectively. read.csv2() is another function for reading CSV files. The difference between read.csv() and read.csv2 is that, the former uses the tab &quot;\\t&quot; as the separator, while the latter one uses the semicolon &quot;;&quot;. This serves as an easy shortcut for different CSV formats used in different regions. Let’s see an example on reading a standard CSV file: df &lt;- read.csv(&quot;data/MusicIcecream.csv&quot;) head(df) ## Age Favorite Music Freq ## 1 old bubble gum classical 1 ## 2 old bubble gum rock 1 ## 3 old coffee classical 3 ## 4 old coffee rock 1 ## 5 young bubble gum classical 2 ## 6 young bubble gum rock 5 A small note while reading multiple files: let R know your current directory by using setwd(). Then, you can read any file in this directory by directly using the name of the file, without specifying the location. 19.3.3 Import JSON file A JSON file is a file that stores simple data structures and objects in JavaScript Object Notation (JSON) format, which is a standard data interchange format. For example, {&quot;name&quot;:&quot;Vince&quot;, &quot;age&quot;:23, &quot;city&quot;:&quot;New York&quot;} is an object with JSON format. In recent years, JSON has become the mainstream format to transfer data on websites. To read a JSON file, we can use the jsonlite package. The jsonlite package is a JSON parser/generator optimized for the web. Its main strength is that it implements a bidirectional mapping between JSON data and the most important R data types. In the example below, the argument simplifyDataFrame = TRUE will directly transform a list of JSON objects into a dataframe. If you want to know more about the arguments simplifyVector and simplifyMatrix, which provide flexible control on other R data formats to transform to, please refer to Getting started with JSON and jsonlite. library(jsonlite) # read JSON data raw_json_data &lt;- fromJSON(txt = &quot;data/WaterConsumptionInNYC.json&quot;, simplifyDataFrame = TRUE) # transform JSON to Data Frame df &lt;- as.data.frame(raw_json_data) head(df) ## new_york_city_population nyc_consumption_million_gallons_per_day ## 1 7102100 1512 ## 2 7071639 1506 ## 3 7089241 1309 ## 4 7109105 1382 ## 5 7181224 1424 ## 6 7234514 1465 ## per_capita_gallons_per_person_per_day year ## 1 213 1979 ## 2 213 1980 ## 3 185 1981 ## 4 194 1982 ## 5 198 1983 ## 6 203 1984 19.4 Import web data We can import data from the web, as long as we have the link to the dataset. Basically, there are two ways to import web data. One way is to scrape the dataset with the link before we import that data into the R workspace. The other way is to import the dataset from the web to our workspace directly. Let’s take the example of Water Consumption In The New York City, which is on the NYC Open Data website. 19.4.1 Scrape web data to local space/memory before importing it To securely access the web data, we can use the RCurl package (for more resources, see The RCurl Package. To be more flexible, we can first download the data to local hard drive and then read them as local files. To download the file, we can use the commands like curl : Downloading files with curl, wget : Wget Command Examples. You can also download it with the browser. ## Year New.York.City.Population NYC.Consumption.Million.gallons.per.day. ## 1 1979 7102100 1512 ## 2 1980 7071639 1506 ## 3 1981 7089241 1309 ## 4 1982 7109105 1382 ## 5 1983 7181224 1424 ## 6 1984 7234514 1465 ## Per.Capita.Gallons.per.person.per.day. ## 1 213 ## 2 213 ## 3 185 ## 4 194 ## 5 198 ## 6 203 19.4.2 Directly read web source into the workspace We still use the water consumption data as an example. First, we specify the correct data URL, and then read the data just like reading the local dataset: # specify the URL link to the data source url &lt;- &quot;https://data.cityofnewyork.us/api/views/ia2d-e54m/rows.csv&quot; # read the URL df &lt;- read.table(url, sep = &quot;,&quot;, header = TRUE) head(df) ## Year New.York.City.Population NYC.Consumption.Million.gallons.per.day. ## 1 1979 7102100 1512 ## 2 1980 7071639 1506 ## 3 1981 7089241 1309 ## 4 1982 7109105 1382 ## 5 1983 7181224 1424 ## 6 1984 7234514 1465 ## Per.Capita.Gallons.per.person.per.day. ## 1 213 ## 2 213 ## 3 185 ## 4 194 ## 5 198 ## 6 203 19.5 Import data from database R provides packages to manipulate data from relational databases like PostgreSQL, MySQL, etc. One of those packages is odbc package, which is one database interface for communication between R and relational database management systems. More resources on package: odbc. Before we connect to a local database, we must satisfy the requirement of the ODBC driver, through which our R package can communicate with the database. To get help on how to install ODBC driver on systems like Windows, Linux, MacOS, please refer to this document: Install ODBC Driver. After we installed the ODBC driver, with odbc and DBI packages, we are able to manipulate the database. To read a table in the database, we usually take steps as follows. First, we build the connection to the database using dbConnect() function. Then, we can do some exploratory operations like listing all tables in the database. To query the data we want, we can send a SQL query into the database. Then we can retrieve the desired data and dfFetch() provides control on how many records to retrieve at a time. Finally, we finish reading and close the connection. library(odbc) library(DBI) # build connection with database con &lt;- dbConnect(odbc::odbc(), driver = &quot;PostgreSQL Driver&quot;, database = &quot;test_db&quot;, uid = &quot;postgres&quot;, pwd = &quot;password&quot;, host = &quot;localhost&quot;, port = 5432) # list all tables in the test_db database dbListTables(con) # read table test_table into Data Frame data &lt;- dbReadTable(con, &quot;test_table&quot;) # write an R Data Frame object to an SQL table # here we write the built-in data mtcars to a new_table in DB data &lt;- dbWriteTable(con, &quot;new_table&quot;, mtcars) # SQL query result &lt;- dbSendQuery(con, &quot;SELECT * FROM test_table&quot;) # Retrieve the first 10 results first_10 &lt;- dbFetch(result, n = 10) # Retrieve the rest of the results rest &lt;- dbFetch(result) # close the connection dbDisconnect(con) 19.6 More resources Import local file: This R Data Import Tutorial Is Everything You Need Import JSON file: Getting started with JSON and jsonlite Import web data: The RCurl Package Import database file Databases using R Documentation on odbc package odbc Install ODBC Driver On Your System Install ODBC Driver "],
["maps.html", "20 Spatial Data 20.1 Choropleth maps 20.2 Square bins 20.3 Longitude / Latitude data 20.4 Resources", " 20 Spatial Data This page is a work in progress. We appreciate any input you may have. If you would like to help improve this page, consider contributing to our repo. 20.1 Choropleth maps Cloropleth maps use color to indicate the value of a variable within a defined region, generally political boundaries. The choroplethr package makes it simple to draw choropleth maps of U.S. states, countries, and census tracts, as well as countries of the world; choroplethrZip provides data for zip code level choropleths; choroplethrAdmin1 draws choropleths for administrative regions of world countries. Note: You must install also install choroplethrMaps for choroplethr to work. In addition, choroplethr requires a number of other dependencies which should be installed automatically, but if they aren’t, you can manually install the missing packages that you are notified about when you call library(choroplethr): maptools, and rgdal, sp. We’ll use the state.x77 dataset for this example: library(tidyverse) library(choroplethr) # data frame must contain &quot;region&quot; and &quot;value&quot; columns df_illiteracy &lt;- state.x77 %&gt;% as.data.frame() %&gt;% rownames_to_column(&quot;state&quot;) %&gt;% transmute(region = tolower(`state`), value = Illiteracy) state_choropleth(df_illiteracy, title = &quot;State Illiteracy Rates, 1977&quot;, legend = &quot;Percent Illiterate&quot;) Note: the choroplethr “free course” that you may come across arrives one lesson at a time by email over an extended period so not the best option unless you have a few weeks to spare. 20.2 Square bins Packages such as statebins create choropleth style maps with equal size regions that roughly represent the location of the region, but not the size or shape. Important: Don’t install statebins from CRAN; use the dev version – it contains many improvements, which are detailed in “Statebins Reimagined”. # devtools::install_github(&quot;hrbrmstr/statebins&quot;) library(statebins) df_illit &lt;- state.x77 %&gt;% as.data.frame() %&gt;% rownames_to_column(&quot;state&quot;) %&gt;% select(state, Illiteracy) # Note: direction = 1 switches the order of the fill scale # so darker shades represent higher illiteracy rates # (The default is -1). statebins(df_illit, value_col=&quot;Illiteracy&quot;, name = &quot;%&quot;, direction = 1) + ggtitle(&quot;State Illiteracy Rates, 1977&quot;) + theme_statebins() 20.3 Longitude / Latitude data Note that the options above work with political boundaries, based on the names of the regions that you provide. If you have longitude / latitude data, ggmap is a good choice. It is straight-forward to plot lon/lat data on a Cartestian coordinate system, with the x-axis representation longitude and the y-axis latitude – just be careful not to mix them up. The ggmap package provides a variety of maps that can serve as the backdrop for the long/lat points. ggmap offers a number of different map sources. Google Maps API was the go-to, but they now require you to enable billing through Google Cloud Platorm. You get $300 in free credit, but if providing a credit card isn’t your thing, you may consider using Stamen Maps instead, with the get_stamenmap() function. Use the development version of the package; instructions and extensive examples are available on the package’s GitHub page. 20.4 Resources “Getting started Stamen maps with ggmap” – A short tutorial on using ggmap with Stamen maps using the Sacramento dataset in the caret package. "],
["leaflet.html", "21 Interactive Geographic Data 21.1 Overview 21.2 Brief Description about Dataset 21.3 Plotting Markers 21.4 Dynamic Heatmaps 21.5 Dynamic Clustering 21.6 Plotting Groups 21.7 Plotting Categorical Data 21.8 External Resources", " 21 Interactive Geographic Data This chapter originated as a community contribution created by AkhilPunia This page is a work in progress. We appreciate any input you may have. If you would like to help improve this page, consider contributing to our repo. 21.1 Overview You would have already seen different libraries that can help one in beautifully displaying geographic data like ggmap and choroplethr. Even though these libraries provide lots of interesting features to better express information through 2-dimensional graphs, they still lack one feature: interactivity. Here comes leaflet—a library written in javascript to handle interactive maps. Fun Fact: It’s actively used by a lot of leading newspapers like The New York Times and The Washington Post. Let’s dive in. 21.2 Brief Description about Dataset For our analysis, we are using NYC Open Data about schools in New York City in 2016. You can find more about it on the Kaggle page. We will be focusing on the the distribution of different variables as a factor of geographical positions. library(tidyverse) library(leaflet) library(htmltools) library(leaflet.extras) library(viridis) schools &lt;- read_csv(&#39;data/2016_school_explorer.csv&#39;) 21.3 Plotting Markers Here we can see that all the Private schools in NYC have been plotted on a map that allows one to zoom in and out. The markers are used to denote the location of each individual school. If we hover over a marker, it displays the name of the school. Isn’t that cool! Here’s the code for it: lat&lt;-median(schools$Latitude) lon&lt;-median(schools$Longitude) schools %&gt;% filter(`Community School?`==&quot;Yes&quot;) %&gt;% leaflet(options = leafletOptions(dragging = TRUE)) %&gt;% addTiles() %&gt;% addMarkers(label=~`School Name`) %&gt;% setView(lng=lon,lat=lat, zoom = 10) 21.4 Dynamic Heatmaps Heatmaps are really useful tools for visualizing the distribution of a particular variable over a certain region (they are so useful, we got a page on ’em). In this example, we see how leaflet is able to dynamically calculate the number of schools in a given region from just latitude and longitude data. You can experience this by zooming in and out of the graph. Here’s the code for it: lat&lt;-mean(schools$Latitude) lon&lt;-mean(schools$Longitude) leaflet(schools) %&gt;% addProviderTiles(providers$CartoDB.DarkMatterNoLabels) %&gt;% addWebGLHeatmap(size=15,units=&quot;px&quot;) %&gt;% setView(lng=lon,lat=lat, zoom = 10) 21.5 Dynamic Clustering Here we can see how leaflet allows one to dynamically cluster data based on its geographic distance at a given zoom level. Here’s the code for it: schools %&gt;% leaflet() %&gt;% addTiles() %&gt;% addCircleMarkers(radius = 2, label = ~htmlEscape(`School Name`), clusterOptions = markerClusterOptions()) 21.6 Plotting Groups Here’s the code for it: top&lt;- schools %&gt;% group_by(District)%&gt;% summarise(top=length(unique(`School Name`)),lon=mean(Longitude),lat=mean(Latitude))%&gt;% arrange(desc(top))%&gt;% head(10) pal &lt;- colorFactor(viridis(100),levels=top$District ) top %&gt;% leaflet(options = leafletOptions(dragging = TRUE)) %&gt;% addProviderTiles(providers$CartoDB.DarkMatterNoLabels) %&gt;% addCircleMarkers(radius=~top/10,label=~paste0(&quot;District &quot;, District,&quot; - &quot;, top,&quot; Schools&quot;),color=~pal(District),opacity = 1) %&gt;% setView(lng=lon,lat=lat, zoom = 10) %&gt;% addLegend(&quot;topright&quot;, pal = pal, values = ~District, title = &quot;District&quot;, opacity = 0.8) 21.7 Plotting Categorical Data We can visualize the distribution of a particular class over the common map. This is achieved through an interactive widget provided on the top right that allows one to choose a particular category or multiple categories. The example below explores how schools in different neighborhoods are racially segregated. Here’s the code for it: ss &lt;- schools %&gt;% dplyr::select(`School Name`,Latitude, Longitude,`Percent White`, `Percent Black`, `Percent Asian`, `Percent Hispanic`) segregation &lt;- function(x){ majority = c() w &lt;- gsub(&quot;%&quot;,&quot;&quot;,x$`Percent White`) b &lt;- gsub(&quot;%&quot;,&quot;&quot;,x$`Percent Black`) a &lt;- gsub(&quot;%&quot;,&quot;&quot;,x$`Percent Asian`) h &lt;- gsub(&quot;%&quot;,&quot;&quot;,x$`Percent Hispanic`) for (i in seq(1,nrow(ss))){ if (max(w[i],b[i],a[i],h[i]) == w[i]) {majority &lt;- c(majority,&#39;White&#39;)} else if (max(w[i],b[i],a[i],h[i]) == b[i]) {majority &lt;- c(majority,&#39;Black&#39;)} else if (max(w[i],b[i],a[i],h[i]) == a[i]) {majority &lt;- c(majority,&#39;Asian&#39;)} else if (max(w[i],b[i],a[i],h[i]) == h[i]) {majority &lt;- c(majority,&#39;Hispanic&#39;)} } return(majority) } ss$race &lt;- segregation(ss) white &lt;- ss %&gt;% filter(race == &quot;White&quot;) black &lt;- ss %&gt;% filter(race == &quot;Black&quot;) hispanic &lt;- ss %&gt;% filter(race == &quot;Hispanic&quot;) asian &lt;- ss %&gt;% filter(race ==&quot;Asian&quot;) lng &lt;- median(ss$Longitude) lat &lt;- median(ss$Latitude) pal_sector &lt;- colorFactor( viridis(4), levels = ss$race) m3 &lt;- leaflet() %&gt;% addProviderTiles(&quot;CartoDB&quot;) %&gt;% addCircleMarkers(data = white, radius = 2, label = ~htmlEscape(`School Name`), color = ~pal_sector(race), group = &quot;White&quot;) m3 %&gt;% addCircleMarkers(data = black, radius = 2, label = ~htmlEscape(`School Name`), color = ~pal_sector(race), group = &quot;Black&quot;) %&gt;% addCircleMarkers(data = hispanic, radius = 2, label = ~htmlEscape(`School Name`), color = ~pal_sector(race), group = &quot;Hispanic&quot;) %&gt;% addCircleMarkers(data = asian, radius = 2, label = ~htmlEscape(`School Name`), color = ~pal_sector(race), group = &quot;Asian&quot;) %&gt;% addLayersControl(overlayGroups = c(&quot;White&quot;, &quot;Black&quot;,&quot;Hispanic&quot;,&quot;Asian&quot;)) %&gt;% setView(lng=lng,lat=lat,zoom=10) These examples provide only a glimpse to what is truly possible with this library. If you want to explore more features and use-cases, check out the links listed below. 21.8 External Resources Leaflet in R Documentation: main documentation of the package. Basic leaflet maps in R: tutorial with examples. Interesting Kaggle Kernel visualizing earthquake data using leaflet in R: another use-case to explore. "],
["missing.html", "22 Missing Data 22.1 Overview 22.2 tl;dr 22.3 What are NAs? 22.4 Types of Missing Data 22.5 Missing Patterns 22.6 Handling Missing values 22.7 External Resources", " 22 Missing Data This chapter originated as a community contribution created by ujjwal95 This page is a work in progress. We appreciate any input you may have. If you would like to help improve this page, consider contributing to our repo. 22.1 Overview This section covers what kinds of missing values are encountered in data and how to handle them. 22.2 tl;dr It’s difficult to handle missing data! If your data has some missing values, which it most likely will, you can either remove such rows, such columns, or impute them. 22.3 What are NAs? Whenever data in some row or column in your data is missing, it comes up as NA. Let’s have a look at some data, shall we? Name Sex Age E_mail Education Income Melissa Female 27 NA NA 1.0e+04 Peter NA NA peter.parker@esu.edu NA 7.5e+03 Aang Male 110 aang@avatars.com NA 1.0e+03 Drake Male NA NA NA 5.0e+04 Bruce NA 45 bruce.wayne@wayne.org NA 1.0e+07 Gwen Female 28 gwen.stacy@esu.edu NA 2.3e+04 Ash Male NA ash.ketchum@pokemon.com NA NA NA NA NA NA NA NA We can see the number of NAs in each column and row: colSums(is.na(data)) ## Name Sex Age E_mail Education Income ## 1 3 4 3 8 2 rowSums(is.na(data)) ## [1] 2 3 1 3 2 1 3 6 We can also see the ratio of the number of NAs in each column and row: colMeans(is.na(data)) ## Name Sex Age E_mail Education Income ## 0.125 0.375 0.500 0.375 1.000 0.250 rowMeans(is.na(data)) ## [1] 0.3333333 0.5000000 0.1666667 0.5000000 0.3333333 0.1666667 0.5000000 ## [8] 1.0000000 22.4 Types of Missing Data Missing Completely at Random (MCAR): These are missing data values which are not related to any missing or non-missing values in other columns in the data. Missing at Random (MAR): These are missing data which are linked to one or more groups in the data. The great thing about MAR is that MAR values can be predicted using other features. For example, it may be observed that people older than 70 generally do not enter their income. Most of the data we encounter is MAR. Missing Not at Random (MNAR): Generally, data which is not MAR is MNAR. A big problem is that there is not a huge distinction between MAR and MNAR. We generally assume MAR, unless otherwise known by an outside source. 22.5 Missing Patterns 22.5.1 Missing Patterns by columns We can see some missing patterns in data by columns, ggplot(tidy_names, aes(x = key, y = fct_rev(Name), fill = missing)) + geom_tile(color = &quot;white&quot;) + ggtitle(&quot;Names dataset with NAs added&quot;) + scale_fill_viridis_d() + theme_bw() And we can also add a scale to check the numerical values available in the dataset and look for any trends: library(scales) # for legend # Select columns having numeric values numeric_col_names &lt;- colnames(select_if(data, is.numeric)) filtered_for_numeric &lt;- tidy_names[tidy_names$key %in% numeric_col_names,] filtered_for_numeric$value &lt;- as.integer(filtered_for_numeric$value) # Use label=comma to remove scientific notation ggplot(data = filtered_for_numeric, aes(x = key, y = fct_rev(Name), fill = value)) + geom_tile(color = &quot;white&quot;) + scale_fill_gradient(low = &quot;grey80&quot;, high = &quot;red&quot;, na.value = &quot;black&quot;, label=comma) + theme_bw() Can you see the problem with the above graph? Notice that the scale is for all the variables, hence it cannot show the variable level differences! To solve this problem, we can standardize the variables: filtered_for_numeric &lt;- filtered_for_numeric %&gt;% group_by(key) %&gt;% mutate(Std = (value-mean(value, na.rm = TRUE))/sd(value, na.rm = TRUE)) %&gt;% ungroup() ggplot(filtered_for_numeric, aes(x = key, y = fct_rev(Name), fill = Std)) + geom_tile(color = &quot;white&quot;) + scale_fill_gradient2(low = &quot;blue&quot;, mid = &quot;white&quot;, high =&quot;yellow&quot;, na.value = &quot;black&quot;) + theme_bw() Now, we can see the missing trends better! Let us sort them by the number missing by each row and column: # convert missing to numeric so it can be summed up filtered_for_numeric &lt;- filtered_for_numeric %&gt;% mutate(missing2 = ifelse(missing == &quot;yes&quot;, 1, 0)) ggplot(filtered_for_numeric, aes(x = fct_reorder(key, -missing2, sum), y = fct_reorder(Name, -missing2, sum), fill = Std)) + geom_tile(color = &quot;white&quot;) + scale_fill_gradient2(low = &quot;blue&quot;, mid = &quot;white&quot;, high =&quot;yellow&quot;, na.value = &quot;black&quot;) + theme_bw() 22.5.2 Missing Patterns by rows We can also see missing patterns in data by rows using the mi package: library(mi) x &lt;- missing_data.frame(data) ## Warning in .guess_type(y, favor_ordered, favor_positive, threshold, ## variable_name): Education : cannot infer variable type when all values are ## NA, guessing &#39;irrelevant&#39; ## NOTE: In the following pairs of variables, the missingness pattern of the second is a subset of the first. ## Please verify whether they are in fact logically distinct variables. ## [,1] [,2] ## [1,] &quot;Age&quot; &quot;Income&quot; ## [2,] &quot;Education&quot; &quot;Income&quot; ## Warning in .local(.Object, ...): Some observations are missing on all included variables. ## Often, this indicates a more complicated model is needed for this missingness mechanism image(x) Did you notice that the Education variable has been skipped? That is because the whole column is missing. Let us try to see some patterns in the missing data: x@patterns ## [1] E_mail Sex, Age ## [3] nothing Age, E_mail ## [5] Sex nothing ## [7] Age, Income Name, Sex, Age, E_mail, Income ## 7 Levels: nothing E_mail Sex Sex, Age Age, E_mail ... Name, Sex, Age, E_mail, Income levels(x@patterns) ## [1] &quot;nothing&quot; &quot;E_mail&quot; ## [3] &quot;Sex&quot; &quot;Sex, Age&quot; ## [5] &quot;Age, E_mail&quot; &quot;Age, Income&quot; ## [7] &quot;Name, Sex, Age, E_mail, Income&quot; summary(x@patterns) ## nothing E_mail ## 2 1 ## Sex Sex, Age ## 1 1 ## Age, E_mail Age, Income ## 1 1 ## Name, Sex, Age, E_mail, Income ## 1 We can visualize missing patterns using the visna (VISualize NA) function in the extracat package: extracat::visna(data) Here, the rows represent a missing pattern and the columns represent the column level missing values. The advantage of this graph is that it shows you only the missing patterns available in the data, not all the possible combinations of data (which will be 2^6 = 64), so that you can focus on the pattern in the data itself. We can sort the graph by most to least common missing pattern (i.e., by row): extracat::visna(data, sort = &quot;r&quot;) Or, by most to least missing values (i.e., by column): extracat::visna(data, sort = &quot;c&quot;) Or, by both row and column sort: extracat::visna(data, sort = &quot;b&quot;) 22.6 Handling Missing values There are multiple methods to deal with missing values. 22.6.1 Deletion of rows containing NAs Often we would delete rows that contain NAs when we are handling Missing Completely at Random data. We can delete the rows having NAs as below: na.omit(data) ## [1] Name Sex Age E_mail Education Income ## &lt;0 rows&gt; (or 0-length row.names) This method is called list-wise deletion. It removes all the rows having NAs. But we can see that the Education column is only NAs, so we can remove that column itself: edu_data &lt;- data[, !(colnames(data) %in% c(&quot;Education&quot;))] na.omit(edu_data) ## Name Sex Age E_mail Income ## 3 Aang Male 110 aang@avatars.com 1000 ## 6 Gwen Female 28 gwen.stacy@esu.edu 23000 Another method is pair-wise deletion, in which only the rows having missing values in the variable of interest are removed. 22.6.2 Imputation Techniques Imputation means to replace missing data with substituted values. These techniques are generally used with MAR data. 22.6.2.1 Mean/Median/Mode Imputation We can replace missing data in continuous variables with their mean/median and missing data in discrete/categorical variables with their mode. Either we can replace all the values in the missing variable directly, for example, if “Income” has a median of 15000, we can replace all the missing values in “Income” with 15000, in a technique known as Generalized Imputation. Or, we can replace all values on a similar case basis. For example, we notice that the income of people with Age &gt; 60 is much less than those with Age &lt; 60, on average, and hence we calculate the median income of each Age group separately, and impute values separately for each group. The problem with these methods is that they disturb the underlying distribution of the data. 22.6.3 Model Imputation There are several model based approaches for imputation of data, and several packages, like mice, Hmisc, and Amelia II, which deal with this. For more info, checkout this blog on DataScience+ about imputing missing data with the R mice package. 22.7 External Resources Missing Data Imputation - A PDF by the Stats Department at Columbia University regarding Missing-data Imputation How to deal with missing data in R - A 2 min read blogpost in missing data handling in R Imputing Missing Data in R; MICE package - A 9 min read on how to use the mice package to impute missing values in R How to Handle Missing Data - A great blogpost on how to handle missing data. "],
["outliers.html", "23 Outliers 23.1 Overview 23.2 tl;dr 23.3 What are outliers? 23.4 Types of Outliers 23.5 Handling Outliers 23.6 External Resources", " 23 Outliers This chapter originated as a community contribution created by kiransaini This page is a work in progress. We appreciate any input you may have. If you would like to help improve this page, consider contributing to our repo. 23.1 Overview This section covers what types of outliers are encountered in data and how to handle them. 23.2 tl;dr I want to see my outliers! Outliers are difficult to spot because judging a datapoint as an outlier depends on the data or model with which they are compared. It is important to detect outliers because they can distort predictions and affect the accuracy of the model. 23.3 What are outliers? Outliers are noticeably far from the bulk of the data. They can be errors, genuine extreme values, rare values, unusual values, cases of special interest, or data from another source. Outliers on individual variables can be spotted using boxplots and bivariate outliers can be spotted using scatterplots. There can also be higher dimensional outliers that are not outliers in lower dimensions. It is worth identifying outliers for a number of reasons. Bad outliers should always be corrected and many statistical methods may work poorly in presence of outliers, but genuine outlying values can be interesting in their own right. Let’s have a look at the outliers of the ‘carat’ variable in the diamonds dataset: 23.4 Types of Outliers 23.4.1 Univariate Outliers Univariate outliers are outlying along one dimension. The best-known approach for an initial look at the data is to use boxplots. Tukey suggests marking individual cases as outliers if they are more than 1.5 IQR (the interquartile range) outside the hinges (basically the quartiles). Outliers may change if they are grouped by another variable. Let’s have a look at outliers on the Sepal Width variable in the iris dataset, both when the data is grouped by Species and when it is not. The outliers are clearly different: p &lt;- ggplot(iris, aes(x=Species, y=Sepal.Width)) + geom_boxplot(color=&quot;black&quot;, fill=&quot;lightblue&quot;) + ggtitle(&quot;Boxplot for Sepal Width grouped by Species in iris dataset&quot;) p p &lt;- ggplot(iris, aes(y=Sepal.Width)) + geom_boxplot(color=&quot;black&quot;, fill=&quot;lightblue&quot;) + ggtitle(&quot;Boxplot for Sepal Width in iris dataset&quot;) p 23.4.2 Multivariate Outliers Multivariate outliers are outlying along more than one dimension. Scatterplots and parallel coordinate plots are useful for visualizing multivariate outliers. You could regard points as outliers that are far from the mass of the data, or you could regard points as outliers that do not fit the smooth model well. Some points are outliers on both criteria. Let’s have a look at outliers on the Petal Length and Sepal Width variables in the iris dataset. We can clearly see an outlier which is far from the mass of the data (lower left): ggplot(iris, aes(x=Sepal.Width, y=Petal.Length)) + geom_point() + ggtitle(&quot;Scatterplot for Petal Length vs Sepal Width in iris dataset&quot;) Let’s have a look at outliers on the Petal Length and Petal Width variables in the iris dataset by fitting a smooth model. Here the outliers are the points that do not fit the smooth model: ggplot(iris, aes(x=Petal.Width, y=Petal.Length)) + geom_point() + geom_smooth() + geom_density2d(col=&quot;red&quot;,bins=4) + ggtitle(&quot;Scatterplot for Petal Length vs Petal Width in iris dataset&quot;) ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; Lets have a look at outliers in the diamond dataset using a parallel coordinate plot. We can see an outlier on the carat, cut, color, and clarity variables that is not an outlier on individual variables: library(GGally) ggparcoord(diamonds[1:1000,], columns=1:5, scale=&quot;uniminmax&quot;, alpha=0.8) + ggtitle(&quot;Parallel coordinate plot of diamonds dataset&quot;) 23.4.3 Categorical Outliers Outliers can be rare on a categorical scale. Certain combinations of categories are rare or should not occur at all. Fluctuation diagrams can be used to find such outliers. We can see rare cases in the HairEyeColor dataset: library(datasets) library(extracat) fluctile(HairEyeColor) 23.5 Handling Outliers Identifying outliers using plots and fitting models is relatively easy compared to what to do after identifying the outliers. Outliers can be rare cases, unusual values, or genuine errors. Genuine errors must be corrected if possible or else they must be removed. Imputation of outliers is complicated and appropriate background knowledge is required. A strategy for dealing with outliers is as follows Plot the one-dimensional distributions of the variables using boxplots. Examine any extreme outliers to see if they are rare values or errors and decide if they should be removed or imputed. For outliers which are extreme on one dimension, examine their values on other dimensions to decide whether they should be discarded or not. Discard values that are outliers on more than one dimension. Consider cases which are outliers in a higher dimensions but not in lower dimensions. Decide whether they are errors or not and consider discarding or imputing the errors. Plot boxplots and parallel coordinate plots by using grouping on a variable to find outliers in subsets of the data. 23.5.1 Not informative Consider the diamonds dataset. Let’s have a look at the width (y) and depth (z) variables: ggplot(diamonds, aes(y=y)) + geom_boxplot(color=&quot;black&quot;, fill=&quot;#9B3535&quot;) + ggtitle(&quot;Ouliers on width variable in diamonds dataset&quot;) ggplot(diamonds, aes(y=z)) + geom_boxplot(color=&quot;black&quot;, fill=&quot;#9B3535&quot;) + ggtitle(&quot;Ouliers on depth variable in diamonds dataset&quot;) ggplot(diamonds, aes(y, z)) + geom_point(col = &quot;#9B3535&quot;) + xlab(&quot;width&quot;) + ylab(&quot;depth&quot;) 23.5.2 More informative The plots are not very informative due to the outliers. The same plots after filtering the outliers are much more informative: d2 &lt;- filter(diamonds, y &gt; 2 &amp; y &lt; 11 &amp; z &gt; 1 &amp; z &lt; 8) ggplot(d2, aes(y=y)) + geom_boxplot(color=&quot;black&quot;, fill=&quot;lightblue&quot;) + ggtitle(&quot;Ouliers on width variable in diamonds dataset&quot;) d2 &lt;- filter(diamonds, y &gt; 2 &amp; y &lt; 11 &amp; z &gt; 1 &amp; z &lt; 8) ggplot(d2, aes(y=z)) + geom_boxplot(color=&quot;black&quot;, fill=&quot;lightblue&quot;) + ggtitle(&quot;Ouliers on depth variable in diamonds dataset&quot;) d2 &lt;- filter(diamonds, y &gt; 2 &amp; y &lt; 11 &amp; z &gt; 1 &amp; z &lt; 8) ggplot(d2, aes(y, z)) + geom_point(shape = 21, color = &quot;darkGrey&quot;, fill = &quot;lightBlue&quot;, stroke = 0.1) + xlab(&quot;width&quot;) + ylab(&quot;depth&quot;) 23.6 External Resources Identify, describe, plot, and remove the outliers from the dataset: Plotting and removing outliers from a dataset A Brief Overview of Outlier Detection Techniques: Discussion of the theoretical aspect of outlier detection "],
["network.html", "24 Interactive Networks 24.1 visNetwork (interactive)", " 24 Interactive Networks 24.1 visNetwork (interactive) visNetwork is a powerful R implementation of the interactive JavaScript vis.js library; it uses tidyverse piping: VisNetwork Docs. –&gt; The Vignette has clear worked-out examples: https://cran.r-project.org/web/packages/visNetwork/vignettes/Introduction-to-visNetwork.html The visNetwork documentation doesn’t provide the same level of explanation as the original, so it’s worth checking out the vis.js documentation as well: http://visjs.org/index.html In particular, the interactive examples are particularly useful for trying out different options. For example, you can test out physics options with this network configurator. 24.1.1 Minimum working example Create a node data frame with a minimum one of column (must be called id) with node names: # nodes boroughs &lt;- data.frame(id = c(&quot;The Bronx&quot;, &quot;Manhattan&quot;, &quot;Queens&quot;, &quot;Brooklyn&quot;, &quot;Staten Island&quot;)) Create a separate data frame of edges with from and to columns. # edges connections &lt;- data.frame(from = c(&quot;The Bronx&quot;, &quot;The Bronx&quot;, &quot;Queens&quot;, &quot;Queens&quot;, &quot;Manhattan&quot;, &quot;Brooklyn&quot;), to = c(&quot;Manhattan&quot;, &quot;Queens&quot;, &quot;Brooklyn&quot;, &quot;Manhattan&quot;, &quot;Brooklyn&quot;, &quot;Staten Island&quot;)) Draw the network with visNetwork(nodes, edges) library(visNetwork) visNetwork(boroughs, connections) Add labels by adding a label column to nodes: library(dplyr) boroughs &lt;- boroughs %&gt;% mutate(label = id) visNetwork(boroughs, connections) 24.1.2 Performance visNetwork can be very slow. %&gt;% visPhysics(stabilization = FALSE) starts rendering before the stabilization is complete, which does actually speed things up but allows you to see what’s happening, which makes a big difference in user experience. (It’s also fun to watch the network stabilize). Other performance tips are described here. 24.1.3 Helpful configuration tools %&gt;% visConfigure(enabled = TRUE) is a useful tool for configuring options interactively. Upon completion, click “generate options” for the code to reproduce the settings. More here (Note that changing options and then viewing them requires a lot of vertical scrolling in the browser. I’m not sure if anything can be done about this. If you have a solution, let me know!) 24.1.4 Coloring nodes Add a column of actual color names to the nodes data frame: boroughs &lt;- boroughs %&gt;% mutate(is.island = c(FALSE, TRUE, FALSE, FALSE, TRUE)) %&gt;% mutate(color = ifelse(is.island, &quot;blue&quot;, &quot;yellow&quot;)) visNetwork(boroughs, connections) 24.1.5 Directed nodes (arrows) visNetwork(boroughs, connections) %&gt;% visEdges(arrows = &quot;to;from&quot;, color = &quot;green&quot;) 24.1.6 Turn off the physics simulation It’s much faster without the simulation. The nodes are randomly placed and can be moved around without affecting the rest of the network, at least in the case of small networks. visNetwork(boroughs, connections) %&gt;% visEdges(physics = FALSE) 24.1.7 Grey out nodes far from selected (defined by “degree”) (Click a node to see effect.) # defaults to 1 degree visNetwork(boroughs, connections) %&gt;% visOptions(highlightNearest = TRUE) # set degree to 2 visNetwork(boroughs, connections) %&gt;% visOptions(highlightNearest = list(enabled = TRUE, degree = 2)) "],
["tidyquant.html", "25 Timeseries with tidyquant 25.1 Overview 25.2 What is tidyquant? 25.3 Installing tidyquant 25.4 Single timeseries 25.5 Multiple timeseries 25.6 External Resources", " 25 Timeseries with tidyquant This chapter originated as a community contribution created by naotominakawa This page is a work in progress. We appreciate any input you may have. If you would like to help improve this page, consider contributing to our repo. 25.1 Overview This section covers how to use the tidyquant package to conduct timeseries analysis. 25.2 What is tidyquant? tidyquant is an one-stop shop for financial analysis. It is suitable for analyzing timeseries data, such as financial and economic data. tidyquant connects to various data sources such as Yahoo! Finance, Morning Star, Bloomberg market data, etc. It also behaves well with other Tidyverse packages. 25.3 Installing tidyquant To install tidyquant, you can run the following code: # install.packages(&quot;tidyquant&quot;) library(tidyquant) If you want to see which functions are available, you can run the following: # to see which functions are available tq_transmute_fun_options() ## $zoo ## [1] &quot;rollapply&quot; &quot;rollapplyr&quot; &quot;rollmax&quot; ## [4] &quot;rollmax.default&quot; &quot;rollmaxr&quot; &quot;rollmean&quot; ## [7] &quot;rollmean.default&quot; &quot;rollmeanr&quot; &quot;rollmedian&quot; ## [10] &quot;rollmedian.default&quot; &quot;rollmedianr&quot; &quot;rollsum&quot; ## [13] &quot;rollsum.default&quot; &quot;rollsumr&quot; ## ## $xts ## [1] &quot;apply.daily&quot; &quot;apply.monthly&quot; &quot;apply.quarterly&quot; ## [4] &quot;apply.weekly&quot; &quot;apply.yearly&quot; &quot;diff.xts&quot; ## [7] &quot;lag.xts&quot; &quot;period.apply&quot; &quot;period.max&quot; ## [10] &quot;period.min&quot; &quot;period.prod&quot; &quot;period.sum&quot; ## [13] &quot;periodicity&quot; &quot;to_period&quot; &quot;to.daily&quot; ## [16] &quot;to.hourly&quot; &quot;to.minutes&quot; &quot;to.minutes10&quot; ## [19] &quot;to.minutes15&quot; &quot;to.minutes3&quot; &quot;to.minutes30&quot; ## [22] &quot;to.minutes5&quot; &quot;to.monthly&quot; &quot;to.period&quot; ## [25] &quot;to.quarterly&quot; &quot;to.weekly&quot; &quot;to.yearly&quot; ## ## $quantmod ## [1] &quot;allReturns&quot; &quot;annualReturn&quot; &quot;ClCl&quot; ## [4] &quot;dailyReturn&quot; &quot;Delt&quot; &quot;HiCl&quot; ## [7] &quot;Lag&quot; &quot;LoCl&quot; &quot;LoHi&quot; ## [10] &quot;monthlyReturn&quot; &quot;Next&quot; &quot;OpCl&quot; ## [13] &quot;OpHi&quot; &quot;OpLo&quot; &quot;OpOp&quot; ## [16] &quot;periodReturn&quot; &quot;quarterlyReturn&quot; &quot;seriesAccel&quot; ## [19] &quot;seriesDecel&quot; &quot;seriesDecr&quot; &quot;seriesHi&quot; ## [22] &quot;seriesIncr&quot; &quot;seriesLo&quot; &quot;weeklyReturn&quot; ## [25] &quot;yearlyReturn&quot; ## ## $TTR ## [1] &quot;adjRatios&quot; &quot;ADX&quot; &quot;ALMA&quot; ## [4] &quot;aroon&quot; &quot;ATR&quot; &quot;BBands&quot; ## [7] &quot;CCI&quot; &quot;chaikinAD&quot; &quot;chaikinVolatility&quot; ## [10] &quot;CLV&quot; &quot;CMF&quot; &quot;CMO&quot; ## [13] &quot;DEMA&quot; &quot;DonchianChannel&quot; &quot;DPO&quot; ## [16] &quot;DVI&quot; &quot;EMA&quot; &quot;EMV&quot; ## [19] &quot;EVWMA&quot; &quot;GMMA&quot; &quot;growth&quot; ## [22] &quot;HMA&quot; &quot;KST&quot; &quot;lags&quot; ## [25] &quot;MACD&quot; &quot;MFI&quot; &quot;momentum&quot; ## [28] &quot;OBV&quot; &quot;PBands&quot; &quot;ROC&quot; ## [31] &quot;rollSFM&quot; &quot;RSI&quot; &quot;runCor&quot; ## [34] &quot;runCov&quot; &quot;runMAD&quot; &quot;runMax&quot; ## [37] &quot;runMean&quot; &quot;runMedian&quot; &quot;runMin&quot; ## [40] &quot;runPercentRank&quot; &quot;runSD&quot; &quot;runSum&quot; ## [43] &quot;runVar&quot; &quot;SAR&quot; &quot;SMA&quot; ## [46] &quot;SMI&quot; &quot;SNR&quot; &quot;stoch&quot; ## [49] &quot;TDI&quot; &quot;TRIX&quot; &quot;ultimateOscillator&quot; ## [52] &quot;VHF&quot; &quot;VMA&quot; &quot;volatility&quot; ## [55] &quot;VWAP&quot; &quot;VWMA&quot; &quot;wilderSum&quot; ## [58] &quot;williamsAD&quot; &quot;WMA&quot; &quot;WPR&quot; ## [61] &quot;ZigZag&quot; &quot;ZLEMA&quot; ## ## $PerformanceAnalytics ## [1] &quot;Return.annualized&quot; &quot;Return.annualized.excess&quot; ## [3] &quot;Return.clean&quot; &quot;Return.cumulative&quot; ## [5] &quot;Return.excess&quot; &quot;Return.Geltner&quot; ## [7] &quot;zerofill&quot; 25.4 Single timeseries Obtain historical data for single stock (for example, Google): # get historical data for single stock. e.g. google tq_get(&quot;GOOGL&quot;, get=&quot;stock.prices&quot;) ## # A tibble: 2,528 x 7 ## date open high low close volume adjusted ## &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2009-01-02 154. 161. 153. 161. 7213700 161. ## 2 2009-01-05 161. 166. 158. 164. 9768200 164. ## 3 2009-01-06 167. 171. 163. 167. 12837500 167. ## 4 2009-01-07 164. 166. 160. 161. 8980000 161. ## 5 2009-01-08 159. 163. 159. 163. 7194100 163. ## 6 2009-01-09 164. 164. 157. 158. 8672300 158. ## 7 2009-01-12 158. 160. 155. 157. 6601900 157. ## 8 2009-01-13 156. 160. 155. 157. 8856100 157. ## 9 2009-01-14 155. 157. 149. 151. 10924800 151. ## 10 2009-01-15 149. 152. 144. 150. 11857100 150. ## # … with 2,518 more rows Calculate monthly return of single stock: # calculate monthly return of single stock tq_get(c(&quot;GOOGL&quot;), get=&quot;stock.prices&quot;) %&gt;% tq_transmute(select=adjusted, mutate_fun=periodReturn, period=&quot;monthly&quot;, col_rename = &quot;monthly_return&quot;) ## # A tibble: 121 x 2 ## date monthly_return ## &lt;date&gt; &lt;dbl&gt; ## 1 2009-01-30 0.0536 ## 2 2009-02-27 -0.00160 ## 3 2009-03-31 0.0298 ## 4 2009-04-30 0.138 ## 5 2009-05-29 0.0537 ## 6 2009-06-30 0.0104 ## 7 2009-07-31 0.0509 ## 8 2009-08-31 0.0420 ## 9 2009-09-30 0.0740 ## 10 2009-10-30 0.0812 ## # … with 111 more rows Create a line chart of the closing price for single stock: # showing closing price for single stock tq_get(c(&quot;GOOGL&quot;), get=&quot;stock.prices&quot;) %&gt;% ggplot(aes(date, close)) + geom_line() Create a line chart of the monthly return for single stock: # showing monthly return for single stock tq_get(c(&quot;GOOGL&quot;), get=&quot;stock.prices&quot;) %&gt;% tq_transmute(select=adjusted, mutate_fun=periodReturn, period=&quot;monthly&quot;, col_rename = &quot;monthly_return&quot;) %&gt;% ggplot(aes(date, monthly_return)) + geom_line() 25.5 Multiple timeseries Obtain historical data for multiple stocks (for example, GAFA): # get historical data for multiple stocks. e.g. GAFA tq_get(c(&quot;GOOGL&quot;,&quot;AMZN&quot;,&quot;FB&quot;,&quot;AAPL&quot;), get=&quot;stock.prices&quot;) ## # A tibble: 9,261 x 8 ## symbol date open high low close volume adjusted ## &lt;chr&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 GOOGL 2009-01-02 154. 161. 153. 161. 7213700 161. ## 2 GOOGL 2009-01-05 161. 166. 158. 164. 9768200 164. ## 3 GOOGL 2009-01-06 167. 171. 163. 167. 12837500 167. ## 4 GOOGL 2009-01-07 164. 166. 160. 161. 8980000 161. ## 5 GOOGL 2009-01-08 159. 163. 159. 163. 7194100 163. ## 6 GOOGL 2009-01-09 164. 164. 157. 158. 8672300 158. ## 7 GOOGL 2009-01-12 158. 160. 155. 157. 6601900 157. ## 8 GOOGL 2009-01-13 156. 160. 155. 157. 8856100 157. ## 9 GOOGL 2009-01-14 155. 157. 149. 151. 10924800 151. ## 10 GOOGL 2009-01-15 149. 152. 144. 150. 11857100 150. ## # … with 9,251 more rows Create a multiple line chart of the closing prices of multiple stocks (again, GAFA). We can show each stock in a different color on the same graph: # Create a multiple line chart of the closing prices of the four stocks, # showing each stock in a different color on the same graph. tq_get(c(&quot;GOOGL&quot;,&quot;AMZN&quot;,&quot;FB&quot;,&quot;AAPL&quot;), get=&quot;stock.prices&quot;) %&gt;% ggplot(aes(date, close, color=symbol)) + geom_line() Transform the data so each stock begins at 100 and replot (Standardize the data so that we can compare timeseries): # Create a multiple line chart of the closing prices of the four stocks, # showing each stock in a different color on the same graph. # Transform the data so each stock begins at 100 and replot. tq_get(c(&quot;GOOGL&quot;,&quot;AMZN&quot;,&quot;FB&quot;,&quot;AAPL&quot;), get=&quot;stock.prices&quot;) %&gt;% group_by(symbol) %&gt;% mutate(close = 100*close/first(close)) %&gt;% ggplot(aes(date, close, color=symbol)) + geom_line() Calculate monthly return of multiple stocks (again, GAFA): # calculate monthly return of multiple stocks tq_get(c(&quot;GOOGL&quot;,&quot;AMZN&quot;,&quot;FB&quot;,&quot;AAPL&quot;), get=&quot;stock.prices&quot;) %&gt;% group_by(symbol) %&gt;% tq_transmute(select=adjusted, mutate_fun=periodReturn, period=&quot;monthly&quot;, col_rename = &quot;monthly_return&quot;) ## # A tibble: 444 x 3 ## # Groups: symbol [4] ## symbol date monthly_return ## &lt;chr&gt; &lt;date&gt; &lt;dbl&gt; ## 1 GOOGL 2009-01-30 0.0536 ## 2 GOOGL 2009-02-27 -0.00160 ## 3 GOOGL 2009-03-31 0.0298 ## 4 GOOGL 2009-04-30 0.138 ## 5 GOOGL 2009-05-29 0.0537 ## 6 GOOGL 2009-06-30 0.0104 ## 7 GOOGL 2009-07-31 0.0509 ## 8 GOOGL 2009-08-31 0.0420 ## 9 GOOGL 2009-09-30 0.0740 ## 10 GOOGL 2009-10-30 0.0812 ## # … with 434 more rows Create a multiple line chart of monthly return of the four stocks. Again, we can show each stock in a different color on the same graph: # Create a multiple line chart of monthly return of the four stocks, # showing each stock in a different color on the same graph tq_get(c(&quot;GOOGL&quot;,&quot;AMZN&quot;,&quot;FB&quot;,&quot;AAPL&quot;), get=&quot;stock.prices&quot;) %&gt;% group_by(symbol) %&gt;% tq_transmute(select=adjusted, mutate_fun=periodReturn, period=&quot;monthly&quot;, col_rename = &quot;monthly_return&quot;) %&gt;% ggplot(aes(date, monthly_return, color=symbol)) + geom_line() 25.6 External Resources tidyquant CRAN doc: formal documentation on the package tidyquant Github repo: Github repository for the tidyquant package with a great README "],
["themes.html", "26 Themes and Palettes 26.1 Overview 26.2 ggplot2 themes 26.3 RColorBrewer 26.4 ggthemes 26.5 ggthemr 26.6 ggsci 26.7 External Resources", " 26 Themes and Palettes This chapter originated as a community contribution created by ar3879 This page is a work in progress. We appreciate any input you may have. If you would like to help improve this page, consider contributing to our repo. 26.1 Overview Our graphs have to be informative and attractive to the audience to get their attention. Themes and colors play an important role in making the graphs attractive. This section covers how we can set different palettes and themes to suit the context and to make them look cool. 26.2 ggplot2 themes In ggplot2, we do have a set of themes, which we can set. A brief description of them is as follows: theme_gray(): signature ggplot2 theme theme_bw(): dark on light ggplot2 theme theme_linedraw(): uses black lines on white backgrounds only theme_light(): similar to linedraw() but with grey lines as well theme_dark(): lines on a dark background instead of light theme_minimal(): no background annotations, minimal feel theme_classic(): theme with no grid lines theme_void(): empty theme with no elements 26.2.1 ggplot2 theme example q &lt;- ggplot(subset, aes(x = clarity, y = carat, color = cut)) + geom_point(size = 2.5,alpha = 0.75) q + theme_minimal() There are several other packages available that set the themes and colors in many ways. We will discuss 4 of them. RColorBrewer ggthemes ggthemr ggsci 26.3 RColorBrewer Often, we find ourselves looking for the colors which make our graph look clear and cool. RColorBrewer offers a number of palettes, which we can use based on the context of our graph. There are three categories of these palettes: Sequential, Diverging, and Qualitative q &lt;- ggplot(subset, aes(x = clarity, y = carat, color = cut)) + geom_point(size = 2.5, alpha = 0.75) Sequential Palette: It represents the shade of the color from light to dark. It is usually used to represent interval data where low values can be shown with a light color and high values can be shown with a dark color. For instance –Blues, BuPu, YlGn, Reds, OrRd q + scale_colour_brewer(palette = &quot;Blues&quot;) Diverging Palette: It has darker colors of contrasting hues on both the ends, and lighter color in the middle. For instance –Spectral, RdGy, PuOr q + scale_colour_brewer(palette = &quot;PuOr&quot;) Qualitative Palette: It is usually used when we want to highlight the differences in the classes (categorial variables). For instance –set1, set2, set3, pastel1, pastel2 , dark2 q + scale_colour_brewer(palette = &quot;Pastel1&quot;) 26.4 ggthemes ggthemes provides additional geoms, scales, and themes to ggplot2. Some of them are really cool! We can change the theme and color of a graph based on the context. g1 &lt;- ggplot(subset, aes(x = clarity, y = carat, color = cut)) + geom_point(size = 2.5,alpha = 0.75) 26.4.1 ggthemes examples g1 + theme_economist() + scale_colour_economist() g1 + theme_igray() + scale_colour_tableau() g1 + theme_wsj() + scale_color_wsj() g1 + theme_igray() + scale_colour_colorblind() If we would like to use these colors in the graphs, which may not support using ggthemes, we can use the scales package to know what colors were used for a given palette. For example: show_col(colorblind_pal()(6)) 26.5 ggthemr ggthemr is used to set the theme of ggplot graphs. It has 17 different themes to change the way ggplot graphs look. Use of ggthemr is different from other other packages. We set the theme before using it. 26.5.1 ggthemr examples ggthemr(&quot;sky&quot;) ## Warning: New theme missing the following elements: panel.grid, plot.tag, ## plot.tag.position ggplot(subset, aes(x = clarity, y = carat, color = cut)) + geom_point(size = 2.5, alpha = 0.75) ggthemr(&quot;flat&quot;) ## Warning: New theme missing the following elements: panel.grid, plot.tag, ## plot.tag.position ggplot(subset, aes(x = clarity, y = carat, color = cut)) + geom_point(size = 2.5, alpha = 0.75) Interestingly, we can set more parameters to change the themes: ggthemr(&quot;lilac&quot;, type = &quot;outer&quot;, layout = &quot;scientific&quot;, spacing = 2) ## Warning: New theme missing the following elements: panel.grid, plot.tag, ## plot.tag.position ggplot(subset, aes(x = clarity, y = carat, color = cut)) + geom_point(size = 2.5, alpha = 0.75) 26.6 ggsci ggsci offers a number of palettes inspired by colors used in scientific journals, science fiction movies, and TV shows. For continous data, scale_fill_material(colname) is used, and for discrete data, scale_color_palname() or scale_fill_palname() are used. 26.6.1 ggsci for discrete data # we need to remove the theme set previously if we don&#39;t want to use it anymore ggthemr_reset() g1 &lt;- ggplot(subset, aes(x = clarity, y = carat, color = cut)) + geom_point(size = 2.5, alpha = 0.75) g1 + scale_color_startrek() g1 + scale_color_jama() g1 + scale_color_locuszoom() 26.6.2 ggsci for continuous data ggplot(diamonds, aes(carat, price)) + geom_hex(bins = 20, color = &quot;red&quot;) + scale_fill_material(&quot;orange&quot;) We can also find out the color used, so that we can use them in some other graphs created in base R: palette = pal_lancet(&quot;lanonc&quot;, alpha = 0.7)(9) show_col(palette) 26.7 External Resources RColorBrewer: Setting up Color Palettes in R ggthemes: Github page containing more examples ggthemr: Github Repository of the package ggsci: Scientific Journal and Sci-Fi Themed Color Palettes for ggplot2 "],
["general.html", "27 General Resources 27.1 Books 27.2 Cheatsheets 27.3 Articles 27.4 Meetups 27.5 Twitter", " 27 General Resources This is a long list of helpful general resources related to EDAV. If you have come across a good resource you don’t see here, consider adding it with a pull request (see the contribute page for more info). 27.1 Books A lot of these are available for students through Columbia Libraries, in both physical and e-book formats. Graphical Data Analysis with R: This book systematically goes through the different types of data, including categorical variables, continuous variables, and time series. The author shows different examples of plotting techniques using ggplot and promoting the “grammar of graphics” model. Code snippets included and available at the book’s website. R for Data Science: The classic. Everything from data types, programming, modeling, communicating, and those keyboard shortcuts you keep forgetting. To quote the book, “this book will teach you how to do data science with R.” Nuff said. 27.2 Cheatsheets Cheatsheet of cheatsheets: Paul van der Laken has put together a large collection of R resource links, including cheat sheets, style guides, package info, blogs, and other helpful resources. RStudio Cheatsheet Collection: Collection of downloadable cheatsheets from RStudio. Includes ones on R Markdown, Data Transformation (dplyr), and Data Visualization (ggplot2). They also have a R Markdown Reference Guide, which is great for remembering that one chunk option that’s on the tip of your tongue. R Base Graphics Cheatsheet: Oddly enough, despite the length of time it’s been around, it’s hard to find a base graphics cheatsheet. Joyce put this one together to help you out if you’re using base graphics. 27.3 Articles Ten Simple Rules for Better Figures: A helpful article discussing how to make the best figures possible by following ten basic rules such as “Avoid ‘chartjunk’” and “Know Your Audience”. Good to keep these rules in mind. The Simpsons by the Data: Nice example of telling a story with data (histograms, scatterplots, etc.). Also, it’s subject is everybody’s favorite TV family. 27.4 Meetups New York Open Statistical Programming Meetup: Meetups hosted by Jared Lander and Wes McKinney on a variety of topics in statistical programming, but with a focus on the R language. Past speakers have included J.J. Allaire (founder of RStudio) and Hadley Wickham (core tidyverse developer). Other attendees are generally eager to welcome newcomers and all of their talks are available on the Lander Analytics Youtube channel. 27.5 Twitter R likes Twitter. Here are some cool people doing work with #rstats: Hadley Wickham David Robinson Julia Silge "],
["percept.html", "28 Perception/Color Resources 28.1 Overview 28.2 Perception 28.3 Color 28.4 Quick tips on using color with ggplot2", " 28 Perception/Color Resources 28.1 Overview This section has resources for learning about graphical perception and how to use colors effectively. 28.2 Perception Here are some links to some key books/articles on perception: Graphical Perception: Theory, Experimentation, and Application to the Development of Graphical Methods: Classic article from William Cleveland and Robert McGill The Elements of Graphing Data: Textbook by William Cleveland Visualizing Data: Textbook by William Cleveland Creating More Effective Graphs: Textbook by Naomi Robbins 28.3 Color Color is very subjective. It is important to choose the right ones so that your work is easy to understand. Color Brewer: Excellent resource for getting effective color palettes for different projects. Its main focus is on cartography, but it is super useful for any project involving color. You can choose between different types of data (sequential, diverging, qualitative), ensure your chosen palette is effective for colorblind users (or print friendly or photocopy safe), and easily export the color palette to different formats (Adobe, GIMP/Inkscape, JS, CSS). The best go-to for effective color palettes. Color Blindness Simulator: Not sure how effective your project will be to a colorblind user? This tool can help. You can upload an image to see how it will look with different color vision handicaps. ColorPick Eyedropper: This Chrome extension allows you to copy hex color values from webpages. Simple and intuitive, it will make creating your awesome color palettes a lot easier. 28.4 Quick tips on using color with ggplot2 One of the most common problems is confusing color and fill. geom_point() and geom_line use color, many of the other geoms use fill. Some use both, such as geom_tile() in which case color is the border color and fill is the fill color. 28.4.1 Continuous data 28.4.1.1 ColorBrewer scale_color_distiller(palette = &quot;PuBu&quot;) or scale_fill_distiller(palette = &quot;PuBu&quot;) (What doesn’t work: scale_color_brewer(palette = &quot;PuBu&quot;)) 28.4.1.2 Viridis scale_color_viridis_c() or scale_fill... (the c stands for continuous) 28.4.1.3 Create your own + scale_color_gradient(low = &quot;white&quot;, high = &quot;red&quot;) or + scale_fill... + scale_color_gradient2(low = &quot;red&quot;, mid = &quot;white&quot;, high = &quot;blue&quot;, midpoint = 50) or + scale_fill... + scale_color_gradientn(colours = c(&quot;red&quot;, &quot;pink&quot;, &quot;lightblue&quot;, &quot;blue&quot;)) or scale_fill... 28.4.2 Discrete data 28.4.2.1 ColorBrewer scale_color_brewer(palette = &quot;PuBu&quot;) or scale_fill... 28.4.2.2 Viridis scale_color_viridis_d() or scale_fill... (the d stands for discrete) 28.4.2.3 Create your own + scale_color_manual(values = c(&quot;red&quot;, &quot;yellow&quot;, &quot;blue&quot;)) or scale_fill... + scale_fill_manual(values = c(&quot;red&quot;, &quot;yellow&quot;, &quot;blue&quot;)) or scale_fill... "],
["publish.html", "29 Publishing Resources 29.1 Overview 29.2 tl;dr 29.3 Bookdown 29.4 Essentials 29.5 Adding a custom domain name 29.6 Make a custom 404 page 29.7 Hooking up Travis 29.8 Other resources", " 29 Publishing Resources 29.1 Overview This section discusses how we built edav.info/ and includes references for building sites and books of your own using R. 29.2 tl;dr Want to get started making a site complete with Travis CI like this one? Zach Bogart has created a bookdown-template you can clone and build off of to create your own site. For instructions, consult the README file. 29.3 Bookdown edav.info/ is built using Bookdown, “a free and open-source R package built on top of R Markdown to make it really easy to write books and long-form articles/reports.” The biggest selling-point for bookdown is that it allows you to make content that is both professional and adaptable. If you want to update a regular book, you need to issue another edition and go through a lot of hassle. With bookdown, you can publish it in different formats (including print, if desired) and be able to change things easily when needed. We chose bookdown for edav.info/ because it allows us to present a lot of content in a compact, searchable manner, while also letting students suggest updates and contribute to its structure. Again, it is professional and adaptable (The default bookdown output is essentially just an online book, but we tried to liven it up by adding a lot of helpful icons, logos, and banners to improve navigation). Below are some helpful references we used in creating edav.info/, which may be helpful if you are interested in creating your own website or online resource with R. 29.4 Essentials How to Start a Bookdown Book: The hardest part about bookdown is getting it up and running. Sean Kross has the best template instructions we found. We started this project by cloning his template repo and building off of it. Excellent descriptions on what all the files do and what is essential to start your project. bookdown: Authoring Books and Technical Documents with R Markdown: This textbook by Yihui Xie, author of the bookdown package, explains everything bookdown is able to accomplish (published using bookdown…because of course it is). An incredible informative reference which we always kept close by. Author’s blurb: A guide to authoring books with R Markdown, including how to generate figures and tables, and insert cross-references, citations, HTML widgets, and Shiny apps in R Markdown. RStudio Bookdown Talk: Yihui Xie (author of the bookdown package) discusses his package and what it can do in a one-hour talk. Good for seeing finished examples. bookdown.org: Site for the bookdown package. Has a bunch of popular books published using bookdown and some info about how to get started using the package. Creating Websites in R: This tutorial, written by Emily Zabor (a Columbia alum), provides a thorough walkthrough for creating websites using different R tools. She discusses how to make different kinds of sites (personal, package, project, blog) as well as GitHub integration and step-by-step instructions for getting setup with templates and hosting. Very detailed and worth perusing if interested in making your own site. 29.5 Adding a custom domain name There are several parts to adding a custom domain name. Buy a domain name and edit DNS settings We used Google domains. In the registrar page, click the DNS icon and add the following to Custom resource records: NAME TYPE TTL DATA @ A 1h 185.199.108.153 www CNAME 1h @ Note that some tutorials list older IP addresses. Check here for the recommended ones. Change settings in your repo In Settings, add your custom domain name in the GitHub Pages section. Add a CNAME file to the gh-pages branch This is a very simple text file named CNAME (all caps). The contents should be one line with the custom domain name. For more detail on steps 2 and 3, see: Emily Zabor’s Tutorial on Custom Domains 29.6 Make a custom 404 page Your site may be lovely, but a default 404 page is always a let down. Not if but when someone types part of your URL incorrectly or a link gets broken, you should make sure there is something to see other than a boring backend page you had no input in designing. This article explains the process, but all you have to do is make a file called 404.html in your root directory and GitHub will use it rather than the default. Because of this, there is really no excuse for not having one. Here’s a look at our 404 page. Hopefully you aren’t seeing it that often. Some considerations: Always include a link back to the site: Throw the user a life-saver. Make it clear that something went wrong: Don’t hide the fact that this page is because of some error. Use absolute paths: The URL that throws the 404 error may be nested within unexpected folders. Make sure if you have any images or links, they work regardless of the file path (use “/images/…” rather than “images/…”, maybe link directly to css/homepage, etc.) Other than that, have fun with it!: There are plenty of examples of people making excellent 404 pages. It should make a frustrating experience just a little bit more bearable. 29.7 Hooking up Travis This tutorial is designed to help you add Travis to your GitHub Pages bookdown web site. It assumes you already have a working web site, with pages stored in a gh-pages branch. We’re not necessarily recommending the gh-pages route; we chose it since we found examples that worked for us using this method. Since the /docs folder is a newer and cleaner approach, it is certainly possible that it provides a better way to organize the repo. That said, there are various tutorials for how to set up the gh-pages branch; it appears that the best way to do so is to create an orphan branch, as explained here. We should note that this makes it all seem very easy to add Travis, which actually was not the case at all for us. I guess everything looks easy in retrospect. If you run into trouble, let us know by filing an issue or submitting a pull request. More info on all the contribution stuff can be found on our contribute page. 29.7.1 Add Travis files to GitHub repo Add these files to your repo: https://github.com/rstudio/bookdown-demo/blob/master/.travis.yml No changes https://github.com/rstudio/bookdown-demo/blob/master/_build.sh Remove the last two lines if you’re only interested in a GitHub Pages book. https://github.com/rstudio/bookdown-demo/blob/master/_deploy.sh The only changes you need to make are to the git config lines. You need to use your GitHub email, but the username can be anything. 29.7.2 Add Travis service Create a Travis account on www.travis-ci.org by clicking on “Sign in with GitHub” on the top right. Click Authorize to allow Travis to have proper access to GitHub. Go back to GitHub and create a personal access token (PAT) if you don’t have one already. You can do so here. Note that you must save your PAT somewhere because you can’t access it once it’s created. Also note that the PAT provides a means to access your GitHub repo through an API, an alternative means to logging in with your username/password (There is an API Token in Travis but this is not the one to use). Return to your Travis profile (travis-ci.org/profile/[GITHUB username]) and click the button next to the appropriate repo to toggle it on. Click on Settings next to the button and add your saved GITHUB_PAT under Environmental Variables: set “Name” to “GITHUB_PAT” and “Value” to the value of the token. If all goes well, you can sit back, relax, and watch Travis do the work for you. via GIPHY 29.8 Other resources blogdown: Creating Websites with R Markdown: Textbook on the blogdownpackage, another option for generating websites with R. Getting Started with GitHub Pages: Short article from GitHub Guides on creating/hosting a website using GitHub Pages. A Beginner’s Guide to Travis CI for R: Fantastic blog post by Julia Silge, includes debugging advice that helped us solve a problem involving installing packages with system requirements. "],
["github.html", "30 GitHub Resources 30.1 Overview 30.2 tl;dr 30.3 On GitHub 30.4 Getting started 30.5 Getting help 30.6 Branching out 30.7 Going deeper", " 30 GitHub Resources 30.1 Overview This section includes links for working with GitHub and advice on how to collaborate in teams on large coding projects. 30.2 tl;dr I don’t wanna just read about GitHub; I wanna learn by doing! We love your enthusiasm. To hit the ground running, checkout GitHub Learning Lab. This application will teach you how to use GitHub with hands-on courses using actual repos. Its the perfect way to understand what using GitHub looks like. Want a little reading as well?: Resources to learn Git is a simple site split into two main sections: Learn by reading and Learn by doing. Take your pick. 30.3 On GitHub In this course, you will be working on a project in teams. Because of this, you probably want to be able to share code and work on different parts of the project simultaneously. This is where Git and GitHub comes in. GitHub is a way to work on projects and keep track of their status easily and efficiently. It is built off of Git, a type of version control software. It is super useful and powerful, but people also find it quite annoying and difficult to understand. So, in an effort to help you, we have collected some resources to learn about GitHub and how you can use it to work on projects. 30.4 Getting started What’s GitHub? Start here. Hello World: GitHub’s take on the “Hello World” program. Great starting point to learn how GitHub works. Github Training &amp; Guides: This YouTube Channel has a lot of info about what GitHub can do. The first line of the opening video is, “Okay. You signed up for GitHub. What do you do now?”. If you are asking that very question, this channel will serve you well. 30.5 Getting help If you’re lost, these might help. GitHub Guides: This is a phenomenal collection of short articles from GitHub to help you learn about the fundamentals around their product. They are so great, we have already listed their Hello World article. Here are some other important ones: Understanding the GitHub Flow: Explains how working with GitHub generally goes. Git Handbook: Explains what version control is. GitHub Help: This is the yellow-pages of GitHub. Ask a question and it will try to push you in the right direction. Get it? 30.6 Branching out GitHub is super social. Learn how to git involved! Open Source Guide: Info on how to contribute to open source projects. Great links to the GitHub skills involved as well as good GitHub etiquette to adopt. Forking Projects: Quick read from GitHub on how to fork a repository so you can contribute to it. Mastering Issues: On what Issues are in GitHub and how they can help get things done. Our Page on Contributing: You can contribute to edav.info/ with your new-found GitHub skills! Checkout our page on how to contribute through pull requests and/or issues. 30.7 Going deeper For the nerds in the room… Git For Ages 4 And Up: There’s a lot going on under the hood. This talk will help explain how it all works…with kids toys! Make pretty git logs: Always remember (A DOG). Also, this alias command is nice to have around: git config --global alias.adog &quot;log --all --decorate --oneline --graph&quot; add and commit with one command: Another (even more) helpful alias command: git config --global alias.add-commit '!git add -A &amp;&amp; git commit' Git Aware Prompt: An excellent add-on to the Terminal that informs you which branch you have checked out. Someone also made an even spiffier version where it will inform you of your git status using helpful emojis. "],
["chapter-index.html", "31 Chapter Index 31.1 Overview 31.2 Index", " 31 Chapter Index 31.1 Overview This page includes links to every chapter in edav.info/ Click on a banner to go to the desired page. If you’re wondering, here’s an explanation of what the banner colors mean. 31.2 Index "]
]
